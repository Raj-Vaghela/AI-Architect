# Hugging Face Model Card Analysis Report

**Date:** December 27, 2025  
**Total Models Analyzed:** 30,403  
**Purpose:** Inform chunking strategy for RAG implementation

---

## 1. Token Count Distribution

### Statistical Summary

| Metric | Token Count |
|--------|-------------|
| **Minimum** | 1 |
| **Maximum** | 278,027 |
| **Average** | 2,095 |
| **Median (p50)** | 829 |
| **p75** | 1,747 |
| **p90** | 3,799 |
| **p95** | 6,366 |
| **p99** | 23,061 |

### Distribution Breakdown

| Token Range | Card Count | Percentage | Cumulative % |
|-------------|-----------|------------|--------------|
| **0-100** | 2,907 | 9.56% | 9.56% |
| **100-500** | 7,609 | 25.03% | 34.59% |
| **500-1000** | 6,688 | 22.00% | 56.59% |
| **1000-2000** | 6,508 | 21.41% | 78.00% |
| **2000-5000** | 4,497 | 14.79% | 92.79% |
| **5000-10000** | 1,415 | 4.65% | 97.44% |
| **10000+** | 779 | 2.56% | 100.00% |

### Key Insights

✅ **~57% of cards are under 1000 tokens** - These can likely be stored as single chunks  
✅ **~78% are under 2000 tokens** - Moderate chunking needed  
⚠️ **~22% are over 2000 tokens** - Require strategic chunking  
⚠️ **~2.6% are extreme outliers (>10k tokens)** - Need special handling

### Visual Distribution

```
0-100      ████▌                        (9.56%)
100-500    ████████████▌                (25.03%)
500-1000   ███████████                  (22.00%)
1000-2000  ██████████▋                  (21.41%)
2000-5000  ███████▍                     (14.79%)
5000-10k   ██▎                          (4.65%)
10000+     █▎                           (2.56%)
```

---

## 2. Duplicate Card Analysis

### Summary Statistics

| Metric | Count |
|--------|-------|
| **Total Cards** | 30,403 |
| **Unique Hashes** | 27,107 |
| **Duplicate Cards** | 3,296 (10.84%) |
| **Deduplication Savings** | ~10.8% storage reduction |

### Top 10 Most Duplicated Cards

| Rank | Hash (truncated) | Duplicate Count | Token Count | Pattern |
|------|------------------|-----------------|-------------|---------|
| 1 | 0cbb01bc... | 1,045 | 5 | "No model card found." fallback |
| 2 | 0336368f... | 605 | 1,068 | Generic auto-generated cards |
| 3 | 1d5838fa... | 56 | 260 | Multi-view diffusion template |
| 4 | 54f0e20b... | 56 | 30 | Chinese voice model template |
| 5 | 98b45ea8... | 55 | 10 | Minimal/placeholder cards |
| 6 | 3e0e15fa... | 50 | 1,068 | Standard training card template |
| 7 | 6936b52b... | 50 | 1,068 | Another generic template |
| 8 | 7c2ae962... | 43 | 86 | Facebook textless translation series |
| 9 | e6878a0d... | 39 | 142 | LGM model card template |
| 10 | a093c3ee... | 24 | 185 | Gemma fine-tune series |

### Duplicate Patterns

1. **Fallback Text (1,045 cards):**  
   - "No model card found." - Models with missing READMEs
   - **Recommendation:** Consider excluding from embeddings or flagging as low-quality

2. **Template Cards (605+ cards):**  
   - Auto-generated by training frameworks (Trainer, AutoTrain)
   - Minimal customization by model creators
   - **Recommendation:** These may have low semantic value for RAG

3. **Model Series (100+ cards):**  
   - Organizations uploading variants with identical READMEs
   - Examples: hmrvc voice models, gemma fine-tunes
   - **Recommendation:** Deduplicate by hash before embedding

---

## 3. Card Content Analysis

### Sample Card Structures

#### **Short Card (251 tokens)**
**Model:** `0-hero/led-large-legal-summary`

**Structure:**
```markdown
---
tags: [...]
language: [...]
datasets: [...]
---
# Model Trained Using AutoTrain
- Problem type
- Model ID
- CO2 Emissions
## Validation Metrics
## Usage
```

**Characteristics:**
- YAML frontmatter with metadata
- Minimal sections (2-3 headings)
- Focus on metrics and usage
- ~250-500 tokens typical

---

#### **Medium Card (725 tokens)**
**Model:** `josesantorcuato/videomae-base-constanza-aumentado`

**Structure:**
```markdown
---
library_name: transformers
license: cc-by-nc-4.0
base_model: [...]
---
# Model Name
- Description
- Results summary
## Model description
## Intended uses & limitations
## Training and evaluation data
## Training procedure
### Training hyperparameters
### Training results
### Framework versions
```

**Characteristics:**
- Standard HF Trainer template
- 6-8 main sections
- Training tables (~300 tokens alone)
- Structured, predictable layout
- ~500-1500 tokens typical

---

#### **Long Card (2,028 tokens)**
**Model:** `GottBERT/GottBERT_base_last`

**Structure:**
```markdown
---
license: mit
language: de
---
# Model Name: Description
- Model Type
- Language
- Parameters
- Link to paper
## Pretraining Details
- Corpus details
- Data size
- Preprocessing
### Filtering Metrics
## Training Configuration
- Framework
- Hardware
- Training time
- Hyperparameters
## Evaluation and Results
- Task descriptions
- Metrics
- Large comparison table (15+ rows)
## Model Architecture
## Tokenizer
## Limitations
## Fairseq Checkpoints
## Citations
```

**Characteristics:**
- Comprehensive documentation
- Multiple nested sections (10+ headings)
- Large comparison tables
- Code blocks (citations, usage)
- ~1500-5000 tokens typical

---

### Content Pattern Analysis

| Element | Frequency | Typical Token Count | Notes |
|---------|-----------|---------------------|-------|
| **YAML Frontmatter** | ~95% | 50-150 | Tags, license, language |
| **Title/Description** | ~90% | 20-100 | Model name + 1-2 sentences |
| **Validation Metrics** | ~75% | 50-200 | Tables or bullet lists |
| **Training Hyperparameters** | ~70% | 100-300 | Generated by Trainer |
| **Training Results Table** | ~60% | 100-500 | Can be very large |
| **Usage/Inference Code** | ~55% | 50-300 | Python snippets |
| **Model Description** | ~50% | 100-500 | Purpose, architecture |
| **Limitations** | ~40% | 50-200 | Often brief or missing |
| **Citations** | ~30% | 50-150 | BibTeX blocks |
| **Comparison Tables** | ~15% | 200-5000 | Large benchmark comparisons |
| **Dataset Details** | ~40% | 50-300 | Training data info |

### Redundant/Boilerplate Content

1. **Auto-generated disclaimers:**  
   - "This model card has been generated automatically..."
   - "You should probably proofread and complete it..."
   - **Estimate:** ~50 tokens per card (where present)

2. **Empty placeholder sections:**  
   - "More information needed"
   - "TODO: Add more details"
   - **Recommendation:** Low value for RAG

3. **Repeated training tables:**  
   - Identical hyperparameter listings across model series
   - **Recommendation:** Consider stripping or summarizing

4. **License boilerplate:**  
   - Full license text embedded (rare but occurs)
   - **Estimate:** 500-2000 tokens
   - **Recommendation:** Extract to metadata, don't embed

---

## 4. Structural Analysis

### Markdown Heading Usage

Approximately **70-80%** of cards with >500 tokens use markdown headings (`#`, `##`, `###`).

**Common heading patterns:**

| Heading Level | Usage | Examples |
|---------------|-------|----------|
| `#` (H1) | Model name | "# GottBERT", "# Model Trained Using AutoTrain" |
| `##` (H2) | Major sections | "## Training", "## Evaluation", "## Usage" |
| `###` (H3) | Subsections | "### Hyperparameters", "### Results" |
| `####` (H4+) | Rare | Dataset details, nested configs |

### Section Size Estimates

| Section Type | Typical Token Range | Chunking Strategy |
|--------------|---------------------|-------------------|
| **Title + Description** | 50-200 | Keep together |
| **Metrics/Results** | 100-500 | Keep together if <800 tokens |
| **Training Hyperparameters** | 100-300 | Keep together |
| **Large Comparison Tables** | 500-5000 | **Split** by row groups |
| **Usage/Code Examples** | 50-300 | Keep together |
| **Limitations/Biases** | 50-200 | Keep together |

---

## 5. Extreme Outlier Analysis

**Top 2 Longest Cards:**

1. **s3nh/SegFormer-b4-person-segmentation** - 278,027 tokens  
2. **s3nh/SegFormer-b0-person-segmentation** - 277,999 tokens

**Cause:** Likely embedded base64-encoded images or extremely large embedded datasets/tables.

**Recommendation:**  
- Investigate and potentially exclude from embeddings
- Strip non-textual content before chunking
- Set a maximum card size threshold (e.g., 20,000 tokens)

---

## 6. Recommendations for Chunking Strategy

### ✅ **Recommended Approach: Section-Based Chunking**

Based on the analysis, implement a **hierarchical section-based chunking** strategy:

#### **Tier 1: Small Cards (<800 tokens) → Single Chunk**
- **Count:** ~17,000 cards (56%)
- **Action:** Store as-is, no chunking needed
- **Benefit:** Preserves context, reduces complexity

#### **Tier 2: Medium Cards (800-2000 tokens) → Section-Based**
- **Count:** ~6,500 cards (21%)
- **Strategy:**  
  - Split by H2 (`##`) headings
  - Target chunk size: 400-800 tokens
  - Include heading in chunk text for context
  - Overlap: ~100 tokens between adjacent sections

#### **Tier 3: Large Cards (2000-10000 tokens) → Aggressive Chunking**
- **Count:** ~6,100 cards (20%)
- **Strategy:**  
  - Split by H2/H3 headings
  - Break large tables into row groups
  - Target chunk size: 600-1000 tokens
  - Overlap: ~150 tokens

#### **Tier 4: Extreme Cards (>10000 tokens) → Special Handling**
- **Count:** ~779 cards (2.6%)
- **Strategy:**  
  - Validate content (exclude if non-textual)
  - Apply aggressive chunking
  - Consider excluding from initial RAG implementation

### Chunking Parameters

| Parameter | Recommended Value | Rationale |
|-----------|-------------------|-----------|
| **Target Chunk Size** | 600-1000 tokens | Balances context vs retrieval granularity |
| **Max Chunk Size** | 1200 tokens | Prevents oversized chunks |
| **Min Chunk Size** | 200 tokens | Avoids tiny, low-value chunks |
| **Overlap** | 100-150 tokens | Preserves context across boundaries |
| **Section Preservation** | Yes | Keep logical units intact |

### Metadata to Attach to Each Chunk

```json
{
  "model_id": "string",
  "chunk_index": "int",
  "section_title": "string or null",
  "total_chunks": "int",
  "token_count": "int",
  "source_card_tokens": "int",
  "task_ids": ["array"],
  "license": "string",
  "downloads": "int",
  "pipeline_tag": "string",
  "tags": ["array"]
}
```

### Deduplication Strategy

1. **Before Chunking:**  
   - Identify duplicate cards by `card_hash`
   - Choose single representative from each group (e.g., highest downloads)
   - **Savings:** ~3,300 cards (10.8%)

2. **After Chunking:**  
   - Check for near-duplicate chunks across different models
   - Use semantic similarity threshold (e.g., >0.95 cosine similarity)

### Content Filtering

**Exclude from embedding:**
- Cards with "No model card found." (1,045 cards)
- Cards <50 tokens (likely empty/minimal)
- Non-English cards if not multilingual embedding model
- Cards with excessive boilerplate (auto-disclaimers >30% of content)

**Estimated usable corpus:** ~26,000-27,000 cards after filtering

---

## 7. Expected Chunking Output

### Estimated Chunk Statistics

| Metric | Estimated Value |
|--------|-----------------|
| **Total Cards** | 30,403 |
| **After Deduplication** | ~27,100 |
| **After Filtering** | ~26,000 |
| **Avg Chunks per Card** | ~2.5 |
| **Total Chunks** | ~65,000-70,000 |

### Chunk Size Distribution (Estimated)

```
200-400 tokens   ████████░░░░░░░░░░░░  (20%)
400-600 tokens   ██████████████░░░░░░  (35%)
600-800 tokens   ████████████░░░░░░░░  (25%)
800-1000 tokens  ██████░░░░░░░░░░░░░░  (15%)
1000-1200 tokens ██░░░░░░░░░░░░░░░░░░  ( 5%)
```

---

## 8. Next Steps

### Immediate Actions

1. ✅ **Review this analysis** with stakeholders
2. ⏳ **Decide on embedding model** (OpenAI vs local, multilingual needs)
3. ⏳ **Implement section-based chunker** script
4. ⏳ **Create `hf.model_card_chunks` table** with metadata
5. ⏳ **Run chunking on sample (1000 cards)** for validation
6. ⏳ **Generate chunk quality report** before full run

### Implementation Checklist

- [ ] Choose embedding model
- [ ] Decide on vector storage (pgvector vs external)
- [ ] Implement chunking script with section detection
- [ ] Create chunks table with indexes
- [ ] Run deduplication by card_hash
- [ ] Apply content filters
- [ ] Chunk 1000-card sample
- [ ] Validate chunk quality manually
- [ ] Generate embeddings for sample
- [ ] Test retrieval queries
- [ ] Full chunking run
- [ ] Full embedding generation

---

## 9. Technical Considerations

### Embedding Model Selection

**Option A: OpenAI `text-embedding-3-large`**
- **Pros:** High quality, 3072 dimensions, multilingual
- **Cons:** Cost (~$0.13 per 1M tokens), API dependency
- **Est. Cost:** ~$9-12 for 70k chunks

**Option B: `sentence-transformers/all-mpnet-base-v2`**
- **Pros:** Free, 768 dimensions, good quality
- **Cons:** English-only, self-host compute
- **Est. Time:** ~2-3 hours on GPU

**Option C: `intfloat/e5-large-v2`**
- **Pros:** Strong performance, 1024 dimensions
- **Cons:** Larger model, slower inference

### Vector Storage

**pgvector in Supabase:**
- Native integration with existing schema
- HNSW index for fast similarity search
- SQL-based hybrid queries (filter + vector)

---

## 10. Conclusion

The HF model card corpus is **well-suited for section-based chunking** with the following characteristics:

✅ **~56% can be stored as single chunks** (no chunking overhead)  
✅ **~70-80% have structured markdown** (headings for splitting)  
✅ **Predictable templates** (training, eval, usage sections)  
⚠️ **~11% are duplicates** (deduplication recommended)  
⚠️ **~3% are extreme outliers** (need special handling)

**Recommended chunk size: 600-1000 tokens** with section-based splitting will provide optimal balance between context preservation and retrieval granularity for RAG applications.

---

**Report Generated:** December 27, 2025  
**Analyst:** AI Senior Data Engineer  
**Next Review:** After chunking implementation

