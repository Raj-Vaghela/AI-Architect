# Test Scenarios - Stack8s AI Architect

## Purpose
This document contains test scenarios to verify that the AI Architect bot correctly uses tools, provides accurate recommendations, and meets acceptance criteria.

---

## 1. Kubernetes Tool Testing

### Test Case K8S-001: Basic Package Search
**Objective:** Verify K8s tool returns relevant results for common queries

**Test Scenarios:**

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| K8S-001a | "I need a monitoring solution for Kubernetes" | Should call `search_k8s_packages("monitoring")` and recommend Prometheus, Grafana | ‚úÖ Tool called<br>‚úÖ Results include official packages<br>‚úÖ Explains what each does |
| K8S-001b | "Help me set up a database in Kubernetes" | Should call `search_k8s_packages("database")` and suggest PostgreSQL, MySQL, MongoDB | ‚úÖ Tool called<br>‚úÖ Multiple database options shown<br>‚úÖ Mentions Bitnami charts |
| K8S-001c | "I want to deploy Redis" | Should call `search_k8s_packages("redis")` | ‚úÖ Tool called<br>‚úÖ Redis chart found<br>‚úÖ Provides deployment guidance |

---

### Test Case K8S-002: Specific Infrastructure Components
**Objective:** Test tool accuracy for specific infrastructure needs

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| K8S-002a | "I need an ingress controller" | Should search for "ingress" and recommend NGINX, Traefik | ‚úÖ NGINX ingress mentioned<br>‚úÖ Explains ingress role |
| K8S-002b | "How do I handle secrets in K8s?" | Should search for "secrets" or "vault" and suggest External Secrets Operator, Sealed Secrets | ‚úÖ Security-focused recommendations<br>‚úÖ Multiple options |
| K8S-002c | "I need a service mesh" | Should search "service mesh" and recommend Istio, Linkerd | ‚úÖ CNCF packages prioritized<br>‚úÖ Explains service mesh benefits |

---

### Test Case K8S-003: Official vs Community Packages
**Objective:** Verify preference for official/CNCF packages

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| K8S-003a | "Show me Kubernetes packages for logging" | Should prioritize official packages (Fluentd, Elasticsearch) | ‚úÖ Official packages ranked higher<br>‚úÖ Mentions package source |
| K8S-003b | "I need cert management" | Should recommend cert-manager (CNCF) | ‚úÖ CNCF badge highlighted<br>‚úÖ Explains why it's trusted |

---

### Test Case K8S-004: Edge Cases
**Objective:** Test handling of ambiguous or no-result queries

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| K8S-004a | "xyz123 kubernetes package" | Should search but likely find no results, inform user politely | ‚úÖ Tool called<br>‚úÖ "No packages found" message<br>‚úÖ Suggests alternatives |
| K8S-004b | "What's in your K8s catalog?" | Should explain what's available without calling tool unnecessarily | ‚úÖ No tool call (informational)<br>‚úÖ Describes Bitnami/Helm charts |

---

## 2. Compute Tool Testing

### Test Case COMPUTE-001: CPU vs GPU Selection (Post-Bias Fix)
**Objective:** Verify tool is no longer GPU-biased

**Test Scenarios:**

| Scenario | User Query | Expected Behavior | Tool Call | Acceptance Criteria |
|----------|-----------|-------------------|-----------|---------------------|
| COMPUTE-001a | "I need to deploy a REST API for my scikit-learn model" | Should recommend CPU instances | `search_compute_instances(gpu_needed=False, ...)` | ‚úÖ `gpu_needed=False`<br>‚úÖ CPU instances returned<br>‚úÖ Cost-effective options |
| COMPUTE-001b | "I want to train a YOLOv8 model" | Should recommend GPU instances | `search_compute_instances(gpu_needed=True, min_vram_gb=16)` | ‚úÖ `gpu_needed=True`<br>‚úÖ GPU with sufficient VRAM<br>‚úÖ Mentions T4/A100 |
| COMPUTE-001c | "I need compute for my ML pipeline, not sure what" | Should search without GPU filter to show both | `search_compute_instances()` or `gpu_needed=None` | ‚úÖ Both CPU and GPU shown<br>‚úÖ Explains when to use each |
| COMPUTE-001d | "Run a PostgreSQL database" | Should recommend CPU instances | `search_compute_instances(gpu_needed=False, ...)` | ‚úÖ CPU instances only<br>‚úÖ Focuses on RAM/storage |
| COMPUTE-001e | "Fine-tune Llama 3 70B" | Should recommend high-VRAM GPUs | `search_compute_instances(gpu_needed=True, min_vram_gb=140)` | ‚úÖ A100 80GB or H100<br>‚úÖ Multi-GPU recommendation<br>‚úÖ Mentions interconnect |

---

### Test Case COMPUTE-002: Price Filtering
**Objective:** Verify budget constraints are respected

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| COMPUTE-002a | "I need a GPU for inference under $200/month" | Should call tool with `max_price_monthly=200` | ‚úÖ Price filter applied<br>‚úÖ Results under budget<br>‚úÖ Recommends T4/L4 |
| COMPUTE-002b | "Cheapest CPU instance for testing" | Should search CPUs, sort by price | ‚úÖ Small CPU instances<br>‚úÖ Mentions it's for testing |

---

### Test Case COMPUTE-003: Provider Preferences
**Objective:** Test provider filtering

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| COMPUTE-003a | "I need an AWS GPU" | Should filter by `provider="aws"` | ‚úÖ Provider filter applied<br>‚úÖ Only AWS results |
| COMPUTE-003b | "GCP or Azure options for my model" | Should make 2 separate calls or show both | ‚úÖ Multiple providers shown<br>‚úÖ Compares options |

---

### Test Case COMPUTE-004: VRAM Estimation
**Objective:** Test LLM VRAM guidance

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| COMPUTE-004a | "How much VRAM for Llama 2 13B?" | Should estimate ~26GB (2GB/B param) and search accordingly | ‚úÖ Rough estimate given<br>‚úÖ Labeled as estimate<br>‚úÖ Suggests verification |
| COMPUTE-004b | "Can I run Mistral 7B on a T4?" | Should explain T4 has 16GB, 7B needs ~14GB, yes | ‚úÖ Factual VRAM check<br>‚úÖ Margin for inference overhead<br>‚úÖ Clear answer |

---

## 3. HuggingFace Tool Testing

### Test Case HF-001: Model Search
**Objective:** Verify model search returns relevant results

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| HF-001a | "Find me a text generation model" | Should search with `pipeline_tag="text-generation"` | ‚úÖ Tool called correctly<br>‚úÖ Llama/Mistral/Phi in results<br>‚úÖ Shows downloads/likes |
| HF-001b | "I need an image classifier" | Should search `pipeline_tag="image-classification"` | ‚úÖ Vision models returned<br>‚úÖ Explains use cases |
| HF-001c | "Open source LLM for chatbot" | Should search and prioritize permissive licenses | ‚úÖ License filter considered<br>‚úÖ Apache-2.0/MIT highlighted |

---

### Test Case HF-002: License Awareness
**Objective:** Verify license checking for commercial use

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| HF-002a | "Commercial LLM for my startup" | Should filter by commercial-friendly licenses | ‚úÖ License filter applied<br>‚úÖ Warns about restrictive licenses<br>‚úÖ Suggests Llama 3, Mistral |
| HF-002b | "Can I use model X commercially?" | Should check license field from tool results | ‚úÖ States actual license<br>‚úÖ Explains implications<br>‚úÖ No guessing |

---

### Test Case HF-003: Popularity Metrics
**Objective:** Test grounding in downloads/likes data

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| HF-003a | "Most popular text generation model" | Should use tool results, cite downloads/likes | ‚úÖ Shows actual numbers<br>‚úÖ Doesn't hallucinate rankings |
| HF-003b | "Is model X well-maintained?" | Should check downloads/likes as proxy | ‚úÖ Uses tool data<br>‚úÖ Qualifies assessment |

---

## 4. Multi-Tool Orchestration

### Test Case MULTI-001: End-to-End Workflow
**Objective:** Verify agent can chain tools for complete solutions

| Scenario | User Query | Expected Tools Called | Acceptance Criteria |
|----------|-----------|----------------------|---------------------|
| MULTI-001a | "I want to deploy Llama 2 70B for inference on GCP" | 1. `search_hf_models("llama 2 70b")`<br>2. `search_compute_instances(gpu_needed=True, min_vram_gb=140, provider="gcp")`<br>3. `search_k8s_packages("llm serving")` | ‚úÖ All 3 tools called<br>‚úÖ Recommendations aligned<br>‚úÖ Coherent deployment plan |
| MULTI-001b | "Set up a monitoring stack for my ML cluster" | 1. `search_k8s_packages("prometheus")`<br>2. `search_k8s_packages("grafana")`<br>3. Maybe `search_compute_instances` for resources | ‚úÖ K8s tools called<br>‚úÖ Stack components explained<br>‚úÖ Integration guidance |
| MULTI-001c | "I need a complete MLOps pipeline" | Multiple calls to K8s tool (Kubeflow, MLflow, Airflow, monitoring) | ‚úÖ Comprehensive search<br>‚úÖ Explains each component<br>‚úÖ Architecture diagram (text) |

---

### Test Case MULTI-002: Tool Selection Logic
**Objective:** Verify agent picks the RIGHT tool for the job

| Scenario | User Query | Expected Tool(s) | Should NOT Call | Acceptance Criteria |
|----------|-----------|-----------------|----------------|---------------------|
| MULTI-002a | "What's Kubernetes?" | None (informational) | Any tool | ‚úÖ Direct answer<br>‚úÖ No unnecessary tool call |
| MULTI-002b | "Compare AWS and GCP pricing" | `search_compute_instances` (2 calls) | HF/K8s tools | ‚úÖ Only compute tool<br>‚úÖ Both providers queried |
| MULTI-002c | "Install kubectl" | None or K8s tool | Compute/HF tools | ‚úÖ Installation instructions<br>‚úÖ K8s context provided |

---

## 5. Truthfulness & Grounding

### Test Case TRUTH-001: No Hallucination
**Objective:** Verify agent doesn't invent data

| Scenario | User Query | Expected Behavior | Red Flags (FAIL) |
|----------|-----------|-------------------|------------------|
| TRUTH-001a | "What's the price of GCP A100?" | Must use tool, cite actual result | ‚ùå Makes up a price<br>‚ùå Says "around $X" without tool |
| TRUTH-001b | "How many downloads does model X have?" | Must use HF tool and cite exact number | ‚ùå Guesses<br>‚ùå Says "popular" without data |
| TRUTH-001c | "Is model X available?" | Must search HF tool | ‚ùå Says yes/no without checking |

---

### Test Case TRUTH-002: Uncertainty Handling
**Objective:** Verify agent admits when it doesn't know

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| TRUTH-002a | "Does provider X have GPU Y?" | Search tool, if not found ‚Üí "Not found in results" | ‚úÖ Honest about limitations<br>‚úÖ Suggests alternatives |
| TRUTH-002b | "What's the best model for task X?" | Qualifies recommendation ("popular choices include...") | ‚úÖ Doesn't claim absolute "best"<br>‚úÖ Shows multiple options |

---

## 6. Conversational Quality

### Test Case CONV-001: Natural Interaction
**Objective:** Test post-fix for "templatey" system prompt

| Scenario | User Query | Expected Style | Red Flags (FAIL) |
|----------|-----------|---------------|------------------|
| CONV-001a | "Hey, I need help deploying a model" | Friendly, conversational response | ‚ùå Rigid "RECOMMENDATION:" format<br>‚ùå Too formal |
| CONV-001b | "Thanks!" | Natural acknowledgment | ‚ùå Ignores<br>‚ùå Launches into recommendations |
| CONV-001c | "Not sure what I need" | Asks clarifying questions naturally | ‚ùå "CLARIFICATION NEEDED:" header<br>‚ùå Bullet-point interrogation |

---

### Test Case CONV-002: Context Retention
**Objective:** Verify multi-turn conversations work

| Turn | Message | Expected Behavior | Acceptance Criteria |
|------|---------|-------------------|---------------------|
| 1 | "I need to deploy Llama 2 7B" | Searches HF, compute, provides plan | ‚úÖ Initial recommendation |
| 2 | "What about on Azure?" | Remembers Llama 2 7B, searches Azure compute | ‚úÖ Doesn't re-ask model<br>‚úÖ Focuses on Azure |
| 3 | "Under $500/month" | Applies budget to previous context | ‚úÖ Filters previous results<br>‚úÖ Updates recommendation |

---

## 7. Tool Logging Verification

### Test Case LOG-001: Tool Visibility
**Objective:** Verify new logging feature works (post-enhancement)

**Test Method:** Check terminal output during tool calls

**Acceptance Criteria:**
- ‚úÖ Tool name clearly logged: `üõ†Ô∏è [TOOL CALL] search_compute_instances`
- ‚úÖ Arguments shown:
  ```
  [ARGUMENTS]
    ‚Ä¢ gpu_needed: True
    ‚Ä¢ min_vram_gb: 16
  ```
- ‚úÖ Output preview shown (first 3 results + metadata)
- ‚úÖ Success/failure status logged
- ‚úÖ Delimiter lines make output readable

**Test Queries:**
1. "Find me an A100 GPU" ‚Üí Check compute tool logs
2. "Search for Prometheus" ‚Üí Check K8s tool logs
3. "Find Llama models" ‚Üí Check HF tool logs

---

## 8. Performance & Efficiency

### Test Case PERF-001: Minimal Tool Calls
**Objective:** Verify agent doesn't spam tools

| Scenario | User Query | Max Tool Calls | Acceptance Criteria |
|----------|-----------|---------------|---------------------|
| PERF-001a | "Find me a GPU" | 1 | ‚úÖ Single compute call<br>‚úÖ Doesn't re-query |
| PERF-001b | "Compare 3 cloud providers" | 3 | ‚úÖ One call per provider<br>‚úÖ No redundant calls |
| PERF-001c | "What's Kubernetes?" | 0 | ‚úÖ No tool call needed<br>‚úÖ Direct answer |

---

## 9. Edge Cases & Error Handling

### Test Case EDGE-001: Empty Results
**Objective:** Handle no-result scenarios gracefully

| Scenario | Tool Call | Expected Results | Expected Response |
|----------|-----------|-----------------|-------------------|
| EDGE-001a | `search_compute_instances(min_vram_gb=9999)` | 0 results | "No instances found with 9999GB VRAM. Consider multi-GPU..." |
| EDGE-001b | `search_k8s_packages("xyznonexistent")` | 0 results | "No K8s packages found. Did you mean...?" |
| EDGE-001c | `search_hf_models("nonsense12345")` | 0 results | "No models found. Try broader terms..." |

---

### Test Case EDGE-002: Ambiguous Queries
**Objective:** Handle vague requests well

| Scenario | User Query | Expected Behavior | Acceptance Criteria |
|----------|-----------|-------------------|---------------------|
| EDGE-002a | "I need compute" | Asks clarifying questions OR shows both CPU/GPU | ‚úÖ Doesn't assume GPU<br>‚úÖ Helpful follow-up |
| EDGE-002b | "Help me deploy" | Asks what they want to deploy | ‚úÖ Polite clarification<br>‚úÖ No guessing |

---

## 10. System Prompt Compliance

### Test Case PROMPT-001: Emoji Usage
**Objective:** Verify "sparingly" means sparingly

| Scenario | Response | Acceptance Criteria |
|----------|----------|---------------------|
| PROMPT-001a | Any response | ‚úÖ Max 2-3 emojis total<br>‚úÖ Not every line<br>‚úÖ Adds to readability |

---

### Test Case PROMPT-002: Conciseness
**Objective:** Verify responses aren't walls of text

| Scenario | Response | Acceptance Criteria |
|----------|----------|---------------------|
| PROMPT-002a | Any recommendation | ‚úÖ Short paragraphs<br>‚úÖ Max 5 bullets per section<br>‚úÖ Scannable format |

---

## Quick Test Script

### Priority 1 Tests (Must Pass)
```
1. K8S-001a: "I need monitoring for Kubernetes"
2. COMPUTE-001a: "Deploy a REST API for scikit-learn" (CPU test)
3. COMPUTE-001b: "Train YOLOv8" (GPU test)
4. HF-001a: "Find a text generation model"
5. MULTI-001a: "Deploy Llama 2 70B on GCP" (full workflow)
6. TRUTH-001a: "What's the price of GCP A100?" (no hallucination)
7. CONV-001a: "Hey, I need help" (natural style)
8. LOG-001: Check terminal for tool logs
```

### Priority 2 Tests (Should Pass)
```
9. COMPUTE-001c: "Need compute, not sure" (show both)
10. COMPUTE-001d: "PostgreSQL database" (CPU bias fix)
11. K8S-003a: Logging packages (official preference)
12. MULTI-002a: "What's Kubernetes?" (no tool spam)
13. TRUTH-002a: Handle "not found" gracefully
14. CONV-002: Multi-turn context retention
```

---

## Test Execution Checklist

**Before Testing:**
- [ ] Backend server restarted after latest changes
- [ ] Terminal visible to check tool logs
- [ ] Test user logged in (if testing chat persistence)

**During Testing:**
- [ ] Note which tool(s) are called
- [ ] Check tool arguments match query intent
- [ ] Verify response accuracy against tool results
- [ ] Check terminal logs for detailed tool output
- [ ] Assess conversational quality

**After Testing:**
- [ ] Document any failures with screenshots
- [ ] Note unexpected behaviors
- [ ] Suggest improvements

---

## Success Metrics

**Tool Accuracy:**
- ‚úÖ 95%+ tool selection correctness (right tool for the query)
- ‚úÖ 100% grounding (no made-up prices/stats/availability)

**CPU/GPU Balance:**
- ‚úÖ CPU recommendations for appropriate workloads
- ‚úÖ GPU recommendations for deep learning
- ‚úÖ Both shown when ambiguous

**Conversational Quality:**
- ‚úÖ Natural, human-like responses
- ‚úÖ Not overly formal or templated
- ‚úÖ Context retained across turns

**Logging:**
- ‚úÖ Tool calls visible in terminal
- ‚úÖ Arguments and output preview shown
- ‚úÖ Readable format

---

## Notes
- These tests should be run after any changes to tools or system prompt
- Tests can be automated with scripts (see `test_ai_architect.py`)
- Document edge cases discovered during user testing
- Update scenarios as new features are added


