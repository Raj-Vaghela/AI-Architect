{
  "results": [
    {
      "model_id": "google/gemma-3-27b-it",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3-27b-pt\n---\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-27b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single/multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-27b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "c41e39b345afbed23425d72d2f0675363531f0dfac5eb3515db939fc2340bac6",
      "token_count": 6438,
      "success": true
    },
    {
      "model_id": "google/gemma-3-12b-it",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3-12b-pt\n---\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-12b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single / multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-12b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "bcd00f0d73a59772e5cac6a98519e8f5eca15f1354cd3889c916e45ba16d8c2e",
      "token_count": 6439,
      "success": true
    },
    {
      "model_id": "google/gemma-3-4b-it",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3-4b-pt\n---\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single/multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-4b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "eb6df732fd79f239d46a2bbc75302795cf293acd5b75fcc01995344da02aa0bf",
      "token_count": 6439,
      "success": true
    },
    {
      "model_id": "microsoft/Florence-2-large",
      "card_text": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE\npipeline_tag: image-text-to-text\ntags:\n- vision\n---\n\n# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n\n## Model Summary\n\n**This is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator ('\\n'). COCO OD AP 39.8**\n\nThis Hub repository contains a HuggingFace's `transformers` implementation of Florence-2 model from Microsoft.\n\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. \n\nResources and Technical Documentation:\n+ [Florence-2 technical report](https://arxiv.org/abs/2311.06242). \n+ [Jupyter Notebook for inference and visualization of Florence-2-large](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n\n| Model   | Model size | Model Description | \n| ------- | ------------- |   ------------- |  \n| Florence-2-base[[HF]](https://huggingface.co/microsoft/Florence-2-base) | 0.23B | Pretrained model with FLD-5B  \n| Florence-2-large[[HF]](https://huggingface.co/microsoft/Florence-2-large) | 0.77B  | Pretrained model with FLD-5B  \n| Florence-2-base-ft[[HF]](https://huggingface.co/microsoft/Florence-2-base-ft) | 0.23B  | Finetuned model on a colletion of downstream tasks\n| Florence-2-large-ft[[HF]](https://huggingface.co/microsoft/Florence-2-large-ft) | 0.77B | Finetuned model on a colletion of downstream tasks\n \n## How to Get Started with the Model\n\nUse the code below to get started with the model. All models are trained with float16. \n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n\nprompt = \"<OD>\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=4096,\n    num_beams=3,\n    do_sample=False\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n\nprint(parsed_answer)\n\n```\n\n\n## Tasks\n\nThis model is capable of performing different tasks through changing the prompts.\n\nFirst, let's define a function to run a prompt.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ndef run_example(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n\n    print(parsed_answer)\n```\n</details>\n\nHere are the tasks `Florence-2` could perform:\n\n<details>\n<summary> Click to expand </summary>\n\n\n\n### Caption\n```python\nprompt = \"<CAPTION>\"\nrun_example(prompt)\n```\n\n### Detailed Caption\n```python\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### More Detailed Caption\n```python\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### Caption to Phrase Grounding \ncaption to phrase grounding task requires additional text input, i.e. caption. \n\nCaption to phrase grounding results format: \n{'\\<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n```python\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n```\n\n### Object Detection\n\nOD results format: \n{'\\<OD>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n\n```python\nprompt = \"<OD>\"\nrun_example(prompt)\n```\n\n### Dense Region Caption\nDense region caption results format: \n{'\\<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n```python\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\n```\n\n### Region proposal\nDense region caption results format: \n{'\\<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['', '', ...]}}\n```python\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\n```\n\n### OCR \n\n```python\nprompt = \"<OCR>\"\nrun_example(prompt)\n```\n\n### OCR with Region\nOCR with region output format:\n{'\\<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\n```python\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\n```\n\n### Output confidence score with Object Detection\n```python\n\ndef run_example_with_score(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      num_beams=3,\n      return_dict_in_generate=True,\n      output_scores=True,\n    )\n    generated_text = processor.batch_decode(generated_ids.sequences, skip_special_tokens=False)[0]\n\n    prediction, scores, beam_indices = generated_ids.sequences, generated_ids.scores, generated_ids.beam_indices\n    transition_beam_scores = model.compute_transition_scores(\n        sequences=prediction,\n        scores=scores,\n        beam_indices=beam_indices,\n    )\n\n    parsed_answer = processor.post_process_generation(sequence=generated_ids.sequences[0], \n        transition_beam_score=transition_beam_scores[0],\n        task=task_prompt, image_size=(image.width, image.height)\n    )\n\n    print(parsed_answer)\n\nprompt = \"<OD>\"\nrun_example_with_score(prompt)\n\n```\n\nfor More detailed examples, please refer to [notebook](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n</details>\n\n# Benchmarks\n\n## Florence-2 Zero-shot performance\n  \nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.  \n  \n| Method | #params | COCO Cap. test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | COCO Det. val2017 mAP |  \n|--------|---------|----------------------|------------------|--------------------|-----------------------|\n| Flamingo | 80B | 84.3 | - | - | - | \n| Florence-2-base| 0.23B | 133.0 | 118.7 | 70.1 | 34.7 | \n| Florence-2-large| 0.77B | 135.6 | 120.8 | 72.8 | 37.5 |\n\n  \nThe following table continues the comparison with performance on other vision-language evaluation tasks.  \n  \n| Method | Flickr30k test R@1 | Refcoco val Accuracy | Refcoco test-A Accuracy | Refcoco test-B Accuracy | Refcoco+ val Accuracy | Refcoco+ test-A Accuracy | Refcoco+ test-B Accuracy | Refcocog val Accuracy | Refcocog test Accuracy | Refcoco RES val mIoU |  \n|--------|----------------------|----------------------|-------------------------|-------------------------|-----------------------|--------------------------|--------------------------|-----------------------|------------------------|----------------------|  \n| Kosmos-2 | 78.7 | 52.3 | 57.4 | 47.3 | 45.5 | 50.7 | 42.2 | 60.6 | 61.7 | - |  \n| Florence-2-base | 83.6 | 53.9 | 58.4 | 49.7 | 51.5 | 56.4 | 47.9 | 66.3 | 65.1 | 34.6 |  \n| Florence-2-large | 84.4 | 56.3 | 61.6 | 51.4 | 53.6 | 57.9 | 49.9 | 68.0 | 67.0 | 35.8 |  \n\n\n\n## Florence-2 finetuned performance \n\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models *Florence-2-base-ft* and *Florence-2-large-ft* that can conduct a wide range of downstream tasks. \n  \nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"\u25b2\" indicates the usage of external OCR as input.  \n  \n| Method         | # Params | COCO Caption Karpathy test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | VQAv2 test-dev Acc | TextVQA test-dev Acc | VizWiz VQA test-dev Acc |  \n|----------------|----------|-----------------------------------|------------------|--------------------|--------------------|----------------------|-------------------------|  \n| **Specialist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| CoCa           | 2.1B     | 143.6                              | 122.4            | -                  | 82.3               | -                    | -                       |  \n| BLIP-2         | 7.8B     | 144.5                              | 121.6            | -                  | 82.2               | -                    | -                       |  \n| GIT2           | 5.1B     | 145.0                              | 126.9            | 148.6              | 81.7               | 67.3                 | 71.0                    |  \n| Flamingo       | 80B      | 138.1                              | -                | -                  | 82.0               | 54.1                 | 65.7                    |  \n| PaLI           | 17B      | 149.1                              | 127.0            | 160.0\u25b2             | 84.3               | 58.8 / 73.1\u25b2         | 71.6 / 74.4\u25b2            |  \n| PaLI-X         | 55B      | 149.2                              | 126.3            | 147.0 / 163.7\u25b2     | 86.0               | 71.4 / 80.8\u25b2         | 70.9 / 74.6\u25b2            |  \n| **Generalist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| Unified-IO     | 2.9B     | -                                  | 100.0            | -                  | 77.9               | -                    | 57.4                    |  \n| Florence-2-base-ft | 0.23B  | 140.0                              | 116.7            | 143.9              | 79.7               | 63.6                 | 63.6                    |  \n| Florence-2-large-ft | 0.77B  | 143.3                              | 124.9            | 151.1              | 81.7               | 73.5                 | 72.6                    |  \n  \n  \n| Method               | # Params | COCO Det. val2017 mAP | Flickr30k test R@1 | RefCOCO val Accuracy | RefCOCO test-A Accuracy | RefCOCO test-B Accuracy | RefCOCO+ val Accuracy | RefCOCO+ test-A Accuracy | RefCOCO+ test-B Accuracy | RefCOCOg val Accuracy | RefCOCOg test Accuracy | RefCOCO RES val mIoU |  \n|----------------------|----------|-----------------------|--------------------|----------------------|-------------------------|-------------------------|------------------------|---------------------------|---------------------------|------------------------|-----------------------|------------------------|  \n| **Specialist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| SeqTR                | -        | -                     | -                  | 83.7                 | 86.5                    | 81.2                    | 71.5                   | 76.3                      | 64.9                      | 74.9                   | 74.2                  | -                      |  \n| PolyFormer           | -        | -                     | -                  | 90.4                 | 92.9                    | 87.2                    | 85.0                   | 89.8                      | 78.0                      | 85.8                   | 85.9                  | 76.9                   |  \n| UNINEXT              | 0.74B    | 60.6                  | -                  | 92.6                 | 94.3                    | 91.5                    | 85.2                   | 89.6                      | 79.8                      | 88.7                   | 89.4                  | -                      |  \n| Ferret               | 13B      | -                     | -                  | 89.5                 | 92.4                    | 84.4                    | 82.8                   | 88.1                      | 75.2                      | 85.8                   | 86.3                  | -                      |  \n| **Generalist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| UniTAB               | -        | -                     | -                  | 88.6                 | 91.1                    | 83.8                    | 81.0                   | 85.4                      | 71.6                      | 84.6                   | 84.7                  | -                      |  \n| Florence-2-base-ft | 0.23B    | 41.4                  | 84.0                | 92.6                 | 94.8                    | 91.5                   | 86.8                   | 91.7                      | 82.2                      | 89.8                   | 82.2                  | 78.0                  |  \n| Florence-2-large-ft| 0.77B    | 43.4                  | 85.2               | 93.4                 | 95.3                    | 92.0                    | 88.3                   | 92.9                      | 83.6                      | 91.2                   | 91.7                  | 80.5                   |  \n  \n\n## BibTex and citation info\n\n```\n@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}\n```",
      "card_hash": "16212f1929e4992649d18b812e0ebf7ebbbaffd90a5ba23dfdc5c78df2f92ac2",
      "token_count": 3998,
      "success": true
    },
    {
      "model_id": "OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF",
      "card_text": "---\nlicense: apache-2.0\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n  - OpenGVLab/InternViT-300M-448px-V2_5\n  - openai/gpt-oss-20b\nbase_model_relation: merge\ndatasets:\n  - OpenGVLab/MMPR-v1.2\n  - OpenGVLab/MMPR-Tiny\nlanguage:\n  - multilingual\ntags:\n  - internvl\n  - custom_code\n---\n\n# InternVL3_5-GPT-OSS-20B-A4B-Preview\n\n[\\[\ud83d\udcc2 GitHub\\]](https://github.com/OpenGVLab/InternVL)  [\\[\ud83d\udcdc InternVL 1.0\\]](https://huggingface.co/papers/2312.14238)  [\\[\ud83d\udcdc InternVL 1.5\\]](https://huggingface.co/papers/2404.16821)  [\\[\ud83d\udcdc InternVL 2.5\\]](https://huggingface.co/papers/2412.05271)  [\\[\ud83d\udcdc InternVL2.5-MPO\\]](https://huggingface.co/papers/2411.10442)  [\\[\ud83d\udcdc InternVL3\\]](https://huggingface.co/papers/2504.10479) [\\[\ud83d\udcdc InternVL3.5\\]](https://huggingface.co/papers/2508.18265)\n\n[\\[\ud83c\udd95 Blog\\]](https://internvl.github.io/blog/)  [\\[\ud83d\udde8\ufe0f Chat Demo\\]](https://chat.intern-ai.org.cn/)  [\\[\ud83d\ude80 Quick Start\\]](#quick-start)  [\\[\ud83d\udcd6 Documents\\]](https://internvl.readthedocs.io/en/latest/)\n\n<div align=\"center\">\n  <img width=\"500\" alt=\"image\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/zJsd2hqd3EevgXo6fNgC-.png\">\n</div>\n\n## Introduction\n\nWe introduce *InternVL3.5*, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the *Cascade Reinforcement Learning (Cascade RL)* framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a *Visual Resolution Router (ViR)* that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled *Vision-Language Deployment (DvD)* strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05 \\\\(\\times\\\\) inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e.,  InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks\u2014narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance.jpg)\n\n> Hatched bars represent closed-source commercial models. We report average scores on a set of multimodal general, reasoning, text, and agentic benchmarks: MMBench v1.1 (en), MMStar,BLINK, HallusionBench, AI2D, OCRBench, MMVet, MME-RealWorld (en), MVBench, VideoMME, MMMU, MathVista, MathVision, MathVerse, DynaMath, WeMath, LogicVista, MATH500, AIME24, AIME25, GPQA, MMLU-Pro, GAOKAO, IFEval, SGP-Bench, VSI-Bench, ERQA, SpaCE-10, and OmniSpatial.\n\nSee [quick start](#quick-start) for how to use our model.\n\n## InternVL3.5 Family\n\nIn the following table, we provide an overview of the InternVL3.5 series.\nTo maintain consistency with earlier generations, we provide two model formats: [the GitHub format](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B), consistent with prior releases, and [the HF format](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF), aligned with the official Transformers standard.\n\n> If you want to convert the checkpoint between these two formats, please refer to the scripts about [custom2hf](https://github.com/OpenGVLab/InternVL/blob/main/internvl_chat/tools/internvl_custom2hf.py) and [hf2custom](https://github.com/OpenGVLab/InternVL/blob/main/internvl_chat/tools/internvl_hf2custom.py).\n\n\n### Github Format\n\n\n| Model                 | #Vision Param | #Language Param | #Total Param | HF Link                                                                        | ModelScope Link                                                                          |\n| --------------------- | ------------- | --------------- | ------------ | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- |\n| InternVL3.5-1B        | 0.3B          | 0.8B            | 1.1B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B)                      |\n| InternVL3.5-2B        | 0.3B          | 2.0B            | 2.3B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B)                      |\n| InternVL3.5-4B        | 0.3B          | 4.4B            | 4.7B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B)                      |\n| InternVL3.5-8B        | 0.3B          | 8.2B            | 8.5B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B)                      |\n| InternVL3.5-14B       | 0.3B          | 14.8B           | 15.1B        | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B)                     | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B)                     |\n| InternVL3.5-38B       | 5.5B          | 32.8B           | 38.4B        | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B)                     | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B)                     |\n| InternVL3.5-20B-A4B   | 0.3B          | 20.9B           | 21.2B-A4B    | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview) | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview) |\n| InternVL3.5-30B-A3B   | 0.3B          | 30.5B           | 30.8B-A3B    | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B)                 | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B)                 |\n| InternVL3.5-241B-A28B | 5.5B          | 235.1B          | 240.7B-A28B  | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B)               |\n\n\n### HuggingFace Format\n\n\n| Model                    | #Vision Param | #Language Param | #Total Param | HF Link                                                                           | ModelScope Link                                                                             |\n| ------------------------ | ------------- | --------------- | ------------ | --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n| InternVL3.5-1B-HF        | 0.3B          | 0.8B            | 1.1B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B-HF)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B-HF)                      |\n| InternVL3.5-2B-HF        | 0.3B          | 2.0B            | 2.3B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B-HF)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B-HF)                      |\n| InternVL3.5-4B-HF        | 0.3B          | 4.4B            | 4.7B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B-HF)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B-HF)                      |\n| InternVL3.5-8B-HF        | 0.3B          | 8.2B            | 8.5B         | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B-HF)                      | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B-HF)                      |\n| InternVL3.5-14B-HF       | 0.3B          | 14.8B           | 15.1B        | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B-HF)                     | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B-HF)                     |\n| InternVL3.5-38B-HF       | 5.5B          | 32.8B           | 38.4B        | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B-HF)                     | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B-HF)                     |\n| InternVL3.5-20B-A4B-HF   | 0.3B          | 20.9B           | 21.2B-A4B    | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF) | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-GPT-OSS-20B-A4B-Preview-HF) |\n| InternVL3.5-30B-A3B-HF   | 0.3B          | 30.5B           | 30.8B-A3B    | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-HF)                 | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B-HF)                 |\n| InternVL3.5-241B-A28B-HF | 5.5B          | 235.1B          | 240.7B-A28B  | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-HF)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B-HF)               |\n\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_overall.jpg)\n\n> We conduct the evaluation with [VLMEvalkit](https://github.com/open-compass/VLMEvalKit). ***To enable the Thinking mode of our model, please set the system prompt to [R1_SYSTEM_PROMPT](https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/vlm/internvl/internvl_chat.py#L38).*** When enabling Thinking mode, we recommend setting `do_sample=True` and `temperature=0.6` to mitigate undesired repetition.\n\nOur training pipeline comprises four stages: Multimodal Continual Pre-Training (**CPT**), Supervised Fine-Tuning (**SFT**), and Cascade Reinforcement Learning (**CascadeRL**). In CascadeRL, we first fine-tune the model using Mixed Preference Optimization (**MPO**) under an offline RL setting, followed by **GSPO** under an oneline RL setting.\nFor the Flash version of InternVL3.5, we additionally introduce a lightweight training stage, termed Visual Consistency Learning (**ViCO**), which reduces the token cost required to represent an image patch.\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/training_pipeline.jpg)\n\nHere, we also open-source the model weights after different training stages for potential research usage.\n***If you're unsure which version to use, please select the one without any suffix, as it has completed the full training pipeline.***\n\n\n| Model                            | Training Pipeline     | HF Link                                                                     | ModelScope Link                                                                       |\n| -------------------------------- | --------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n| InternVL3.5-1B-Pretrained        | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B-Pretrained)        | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B-Pretrained)        |\n| InternVL3.5-1B-Instruct          | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B-Instruct)          | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B-Instruct)          |\n| InternVL3.5-1B-MPO               | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B-MPO)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B-MPO)               |\n| InternVL3.5-1B                   | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-1B)                   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-1B)                   |\n| InternVL3.5-2B-Pretrained        | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B-Pretrained)        | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B-Pretrained)        |\n| InternVL3.5-2B-Instruct          | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B-Instruct)          | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B-Instruct)          |\n| InternVL3.5-2B-MPO               | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B-MPO)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B-MPO)               |\n| InternVL3.5-2B                   | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-2B)                   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-2B)                   |\n| InternVL3.5-4B-Pretrained        | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B-Pretrained)        | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B-Pretrained)        |\n| InternVL3.5-4B-Instruct          | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B-Instruct)          | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B-Instruct)          |\n| InternVL3.5-4B-MPO               | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B-MPO)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B-MPO)               |\n| InternVL3.5-4B                   | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-4B)                   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-4B)                   |\n| InternVL3.5-8B-Pretrained        | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B-Pretrained)        | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B-Pretrained)        |\n| InternVL3.5-8B-Instruct          | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B-Instruct)          | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B-Instruct)          |\n| InternVL3.5-8B-MPO               | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B-MPO)               | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B-MPO)               |\n| InternVL3.5-8B                   | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-8B)                   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-8B)                   |\n| InternVL3.5-14B-Pretrained       | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B-Pretrained)       | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B-Pretrained)       |\n| InternVL3.5-14B-Instruct         | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B-Instruct)         | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B-Instruct)         |\n| InternVL3.5-14B-MPO              | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B-MPO)              | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B-MPO)              |\n| InternVL3.5-14B                  | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-14B)                  | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-14B)                  |\n| InternVL3.5-30B-A3B-Pretrained   | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Pretrained)   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B-Pretrained)   |\n| InternVL3.5-30B-A3B-Instruct     | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-Instruct)     | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B-Instruct)     |\n| InternVL3.5-30B-A3B-MPO          | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B-MPO)          | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B-MPO)          |\n| InternVL3.5-30B-A3B              | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-30B-A3B)              | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-30B-A3B)              |\n| InternVL3.5-38B-Pretrained       | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B-Pretrained)       | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B-Pretrained)       |\n| InternVL3.5-38B-Instruct         | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B-Instruct)         | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B-Instruct)         |\n| InternVL3.5-38B-MPO              | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B-MPO)              | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B-MPO)              |\n| InternVL3.5-38B                  | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-38B)                  | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-38B)                  |\n| InternVL3.5-241B-A28B-Pretrained | CPT                   | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Pretrained) | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B-Pretrained) |\n| InternVL3.5-241B-A28B-Instruct   | CPT + SFT             | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-Instruct)   | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B-Instruct)   |\n| InternVL3.5-241B-A28B-MPO        | CPT + SFT + MPO       | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B-MPO)        | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B-MPO)        |\n| InternVL3.5-241B-A28B            | CPT + SFT + CascadeRL | [\ud83e\udd17 link](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B)            | [\ud83e\udd16 link](https://www.modelscope.cn/models/OpenGVLab/InternVL3_5-241B-A28B)            |\n\n\nThe Flash version of our model will be released as soon as possible.\n\n\n\n## Model Architecture\n\n`InternVL3.5`:\nThis series of models follow the \"ViT\u2013MLP\u2013LLM\" paradigm adopted in previous versions of InternVL.\nWe initialize the language model using the Qwen3 series and GPT-OSS, and the vision encoder using InternViT-300M and InternViT-6B.\nThe Dynamic High Resolution strategy introduced in InternVL1.5 is also retained in our design.\n\n\n`InternVL3.5-Flash`:\nCompared to InternVL3.5, InternVL3.5-Flash further integrates the *Visual Resolution Router (ViR)*, thus yielding a series of  efficient variants friendly  suitable for  resource-constrained scenarios. \nSpecifically, in InternVL3.5, each image patch is initially represented as 1024 visual tokens for the vision encoder, which are then compressed into 256 tokens via a pixel shuffle module before being passed to the Large Language Model (LLM).\nIn InternVL3.5-Flash, as shown in the Figure below, an additional pixel shuffle module with a higher compression rate is included, enabling the compression of visual tokens down to 64 tokens.\nFor each patch, the patch router determines the appropriate compression rate by assessing its semantic richness, and routes it to the corresponding pixel shuffle module accordingly.\nBenefiting from this patch-aware compression mechanism, InternVL3.5-Flash is able to reduce the number of visual tokens by 50\\% while maintaining nearly 100\\% of the performance of InternVL3.5.\n\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/architecture.jpg)\n\n## Training and Deployment Strategy\n\n### Pre-Training\n\nDuring the pre-training stage, we update all model parameters jointly using the combination of large-scale text and multimodal corpora. Specifically, given an arbitrary training sample consisting of a multimodal token sequence \\\\(\\mathbf{x}=\\left(x_1, x_2, \\ldots, x_L\\right)\\\\), the next token prediction (NTP) loss is calculated on each text token as follows:\n\n$$\n    \\mathcal{L}_{i}=-\\log p_\\theta\\left(x_i \\mid x_1, \\ldots, x_{i-1}\\right),\n$$\n\nwhere \\\\(x_i\\\\) is the predicted token and  prefix tokens in \\\\(\\{x_1, x_2, \\ldots, x_{i-1}\\}\\\\) can be either  text tokens or  image tokens. Notably, for conversation samples, only response tokens  are included for the calculation of the loss.\nAdditionally, to mitigate bias toward either longer or shorter responses during training, we adopt the square averaging to re-weight the NTP loss  as follows:\n\n$$\n\\mathcal{L}_{i}^{'} = \\frac{w_i}{\\sum_j w_j} \\cdot \\mathcal{L}_i, \\quad w_i = \\frac{1}{N^{0.5}},\n$$\n\nwhere \\\\(N\\\\) denotes the number of tokens in the training sample on which the loss needs to be calculated. The random JPEG compression is also included to enhance the model's real-world performance.\n\n### Supervised Fine-Tuning\n\nDuring the SFT phase, we adopt the same objective as in the pre-training stage and use the  square-root averaging strategy to calculate the final loss.  In this stage, the context window is set to 32K tokens to adapt long-context information.\nCompared to InternVL3, the SFT stage of InternVL3.5 contains  more high-quality and  diverse training data derived from three sources: \n\n(1) Instruction-following data from InternVL3, which are reused to preserve broad coverage of vision\u2013language tasks. \n\n(2) Multimodal reasoning data in the \"Thinking\" mode, which are included to instill long-thinking capabilities in the model. To construct such data, we first use InternVL3-78B to describe the image and then input the description into DeepSeek-R1 to sample rollouts with detailed reasoning processes. Rollouts with an incorrect final answer are filtered out. The questions in these datasets cover various expert domains, such as mathematics and scientific disciplines, thereby strengthening performance on different reasoning tasks. \n\n(3) Capability-expansion datasets, which endow InternVL3.5 with new skills, including GUI-based interaction, embodied interaction, and scalable vect\n\n### Cascade Reinforcement Learning\n\nCascade RL aims to combine the benefits of offline RL and online RL to progressively facilitate the post-training of MLLMs in an efficient manner.\nSpecifically, we first fine-tune the model using an offline RL algorithm as an efficient warm-up stage to reach a satisfied results, which can guarantee the high-quality rollouts for the latter stage. \nSubsequently, we employ an online RL algorithm to further refine the output distribution based on rollouts generated by the model itself.  Compared to the single offline or online RL stage, our cascaded RL achieves significant performance improvements at a fraction of the GPU time cost.\n\n\n\nDuring the offline RL stage, we employ mixed preference optimization (MPO) to fine-tune the model. Specifically, the training objective of MPO is a combination of preference loss \\\\(\\mathcal{L}_{p}\\\\), quality loss \\\\(\\mathcal{L}_{q}\\\\), and generation loss \\\\(\\mathcal{L}_{g}\\\\), which can be formulated as follows:\n\n$$\n    \\mathcal{L}_{\\text{MPO}}=\n    w_{p} \\mathcal{L}_{p}\n    +\n    w_{q} \\mathcal{L}_{q}\n    +\n    w_{g} \\mathcal{L}_{g}\n    ,\n$$\n\nwhere \\\\(w_{*}\\\\) represents the weight assigned to each loss component.\nThe DPO loss, BCO loss, and LM loss serve as the preference loss, quality loss, and generation loss, respectively.\n\n\nDuring the online RL stage, we employ GSPO, without reference model constraints, as our online RL algorithm, which we find more effective in training both dense and mixture-of-experts (MoE) models. Similar to GRPO, the advantage is defined as the normalized reward across responses sampled from the same query.\nThe training objective of GSPO is given by:\n\n$$\n    \\mathcal{L}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta \\text { old }}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\min \\left(s_i(\\theta) \\widehat{A}_i, \\operatorname{clip}\\left(s_i(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_i\\right)\\right],\n$$\n\nwhere the importance sampling ratio is defined as the geometric mean of the per-token ratios.\n\n> Please see [our paper](https://huggingface.co/papers/2508.18265) for more technical and experimental details.\n\n\n### Visual Consistency Learning\n\n\nWe further include ViCO as an additional training stage to integrate the *visual resolution router (ViR)* into InternVL3.5, thereby reducing the inference cost of InternVL3.5. The obtained efficient version of InternVL3.5 are termed as *InternVL3.5-Flash*. In particular, ViCO comprises two stages:\n\n`Consistency training`:\nIn this stage, the entire model is trained to minimize the divergence between response distributions conditioned on visual tokens with different compression rates.\nIn practice, we introduce an extra reference model, which is frozen and initialized with InternVL3.5.\nGiven a sample, each image patch is represented as either 256 or 64 tokens, and the training objective is defined as follows:\n\n\n$$\n\\mathcal{L}_\\text{ViCO} =\n\\mathbb{E}_{\\xi \\sim \\mathcal{R}} \\Bigg[\n\\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{KL} \\Big(\n\\pi_{\\theta_{ref}}\\left(y_i \\mid y_{<i}, I\\right) \\;\\Big\\|\\;\n\\pi_{\\theta_{policy}}\\left(y_i \\mid y_{<i}, I_\\xi\\right)\n\\Big)\n\\Bigg],\n$$\n\nwhere \\\\(\\mathrm{KL}\\) denotes the KL divergence and \\(\\xi\\) denotes the compression rate, which is uniformly sampled from \\(\\{\\frac{1}{4},\\frac{1}{16}\\}\\). The image \\(I_\\xi\\) is represented as 256 tokens when \\(\\xi=\\frac{1}{4}\\) and 64 tokens when \\(\\xi=\\frac{1}{16}\\). Notably, the reference model always performs inference with \\(\\xi=\\frac{1}{4}\\).\n\n\n`Router training`:\nThis stage aims to train the ViR to select an appropriate trade-off resolution for different inputs.\nViR is formulated as a binary classifier and trained using standard cross-entropy loss.\nTo construct the route targets, we first compute the KL divergence between the model outputs conditioned on uncompressed visual tokens (i.e., 256 tokens per patch) and those conditioned on compressed visual tokens (i.e., 64 tokens per patch).\nDuring this stage, the main MLLM (ViT, MLP and LLM) is kept frozen, and only the ViR is trained.\nSpecifically, we first compute the loss ratio for each patch:\n\n$$\nr_i = \\frac{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{16}}\\big)}{\\mathcal{L}_\\text{ViCO}\\big(y_i \\mid I_{\\frac{1}{4}}\\big)},\n$$\n\nwhich quantifies the relative increase in loss caused by compressing the visual tokens. Based on this ratio, the binary ground-truth label for the patch router is defined as:\n\n$$\ny_i^\\text{router} =\n\\begin{cases}\n0, & r_i < \\tau \\; \\text{(compression has negligible impact)} \\\\\n1, & r_i \\ge \\tau \\; \\text{(compression has significant impact)},\n\\end{cases}\n$$\n\nwhere \\(y_i^{\\text{router}}=0\\) and \\(y_i^{\\text{router}}=1\\)  indicate that the compression rate \\(\\xi\\) is set to \\(\\tfrac{1}{16}\\) and \\(\\tfrac{1}{4}\\), respectively.\n\n> Please see [our paper](https://huggingface.co/papers/2508.18265) for more technical and experimental details.\n\n\n### Test-Time Scaling\n\n\nTest-time scaling (TTS) has been empirically demonstrated as an effective approach to enhance the reasoning capabilities of LLMs and MLLMs, particularly for complex tasks necessitating multi-step inference.\nIn this work, we implement a comprehensive test-time scaling approach that simultaneously improves reasoning depth (i.e., deep thinking) and breadth (i.e., parallel thinking).\n\n`Deep Thinking`: By activating the Thinking mode, we guide the model to deliberately engage in step-by-step reasoning (i.e., decomposing complex problems into logical steps and validating intermediate conclusions) prior to generating the final answer. This approach systematically improves the logical structure of solutions for complex problems, particularly those requiring multi-step inference, and enhances reasoning depth.\n\n`Parallel Thinking`: Following InternVL3, for reasoning tasks, we adopt the Best-of-N (BoN) strategy by employing [VisualPRM-v1.1](https://huggingface.co/OpenGVLab/VisualPRM-8B-v1_1) as the critic model to select the optimal response from multiple reasoning candidates.\nThis approach improves reasoning breadth.\n\n> Notably, unless otherwise specified, the experimental results reported in our paper are obtained without applying TTS. Thus far, we have only applied TTS to reasoning benchmarks, since we found that the model already exhibits strong perception and understanding capabilities, and initiating TTS yields no significant improvement.\n\n\n### Decoupled Vision-Language Deployment\n\nIn multimodal inference, the vision encoder and language model have distinct computational characteristics. The vision encoder that transforms images into semantic features is highly parallelizable and does not rely on long-term history state.  In contrast,  the language model adopts the inference in an autoregressive manner, which requires previous states to compute the next one. This sequential property makes the language part more sensitive to memory bandwidth and latency. \nWhen MLLMs are deployed online at scale, the vision and language models often block each other, thus incurring additional inference cost. This effect becomes more pronounced with larger vision models or higher-resolution images.\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/DvD.jpg)\n\nAs shown in the Figure above, we propose decoupled vision-language deployment (DvD) to address this issue by separating vision and language processing, with a particular focus on optimizing the prefilling stage. The vision subsystem batches and processes images to produce compact feature embeddings, which are then transmitted to the language subsystem for fusion with the text context prior to decoding. This separation alleviates blocking and brings multimodal prefilling performance closer to that of pure language models.\nIn our system implementation, the ViT and MLP (and ViR for InternVL3.5-Flash) are deployed on the vision server, while the language server executes only the LLM. The communication is unidirectional, transmitting BF16 visual features over TCP, with RDMA optionally employed to achieve higher transmission speed. Vision processing, feature transmission, and language processing are organized into an asynchronous three-stage pipeline, enabling overlapped execution and minimizing pipeline stalls.\n\n\nDvD increases GPU utilization and processing efficiency on the vision side, while enabling the language server to focus exclusively on the LLM\u2019s prefilling and decoding without being blocked by vision computation. This design leads to improved throughput and responsiveness. Moreover, the architecture supports independent hardware cost optimization for the vision and language modules, and facilitates the seamless integration of new modules without requiring modifications to the language server deployment.\n\n\n## Evaluation on Multimodal Capability\n\n### Multimodal Reasoning and Mathematics\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_reasoning.jpg)\n\n### OCR, Chart, and Document Understanding\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_ocr.jpg)\n\n### Multi-Image Understanding & Real-World Comprehension\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_multi_images.jpg)\n\n### Comprehensive Multimodal Understanding & Multimodal Hallucination Evaluation\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_comprehensive.jpg)\n\n### Visual Grounding\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_grounding.jpg)\n\n### Multimodal Multilingual Understanding\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_multilingual.jpg)\n\n### Video Understanding\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_video.jpg)\n\n### GUI Tasks\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_gui.jpg)\n\n### Embodied Tasks\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_embody.jpg)\n\n### SVG Tasks\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_svg.jpg)\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_svg_gen.jpg)\n\n## Evaluation on Language Capability\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/performance_text.jpg)\n\n## Ablation Study\n\n### Cascade Reinforcement Learning\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/ablation_cascade_rl.jpg)\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/ablation_cascade_rl_table.jpg)\n\n### Decoupled Vision-Language Deployment\n\n\n![image/jpg](https://huggingface.co/OpenGVLab/InternVL3_5-241B-A28B/resolve/main/images/ablation_dvd.jpg)\n\n## Quick Start\n\nWe provide an example code to run `InternVL3.5-8B-HF` using `transformers`. Please note that our models with up to 30B parameters can be deployed on a single A100 GPU, while the 38B model requires two A100 GPUs and the 235B model requires eight A100 GPUs.\n\n> In most cases, both [LMDeploy](https://github.com/InternLM/lmdeploy) and [vLLM](https://github.com/vllm-project/vllm) can be used for model deployment. However, for InternVL3.5-20B-A4B, we recommend using vLLM since lmdeploy has not yet supported GPT-OSS.\n\n> Please use transformers>=4.52.1 to ensure the model works normally. For the 20B version of our model, transformers>=4.55.0 is required.\n\n### Model Loading\n\n#### 16-bit (bf16 / fp16)\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForImageTextToText\npath = \"OpenGVLab/InternVL3_5-8B-HF\"\nmodel = AutoModelForImageTextToText.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=True,\n    trust_remote_code=True).eval().cuda()\n```\n\n#### BNB 8-bit Quantization\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForImageTextToText\npath = \"OpenGVLab/InternVL3_5-8B-HF\"\nmodel = AutoModelForImageTextToText.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    load_in_8bit=True,\n    low_cpu_mem_usage=True,\n    use_flash_attn=True,\n    trust_remote_code=True).eval()\n```\n\n#### Multiple GPUs\n\n```python\nimport math\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForImageTextToText\n\npath = \"OpenGVLab/InternVL3_5-8B-HF\"\nmodel = AutoModelForImageTextToText.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=True,\n    trust_remote_code=True,\n    device_map=\"auto\").eval()\n```\n\n### Thinking Mode\n\nTo enable thinking mode, please set the system prompt to our Thinking System Prompt. When enabling Thinking mode, we recommend setting `do_sample=True` and `temperature=0.6` to mitigate undesired repetition.\n\n```python\nR1_SYSTEM_PROMPT = \"\"\"\nYou are an AI assistant that rigorously follows this response protocol:\n\n1. First, conduct a detailed analysis of the question. Consider different angles, potential solutions, and reason through the problem step-by-step. Enclose this entire thinking process within <think> and </think> tags.\n\n2. After the thinking section, provide a clear, concise, and direct answer to the user's question. Separate the answer from the think section with a newline.\n\nEnsure that the thinking process is thorough but remains focused on the query. The final answer should be standalone and not reference the thinking section.\n\"\"\".strip()\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": R1_SYSTEM_PROMPT},\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"xxx\"},\n        ],\n    },\n]\n```\n\n### Inference with Transformers\n\nThe HuggingFace format checkpoints of our models are fully consistent with the APIs of the official HuggingFace models. For details, please refer to the official [documentation](https://huggingface.co/docs/transformers/v4.55.4/en/model_doc/internvl).\n\n## Finetune\n\nMany repositories now support fine-tuning of the InternVL series models, including [InternVL](https://github.com/OpenGVLab/InternVL), [SWIFT](https://github.com/modelscope/ms-swift), [XTuner](https://github.com/InternLM/xtuner), and others. Please refer to their documentation for more details on fine-tuning.\n\n## Deployment\n\n### LMDeploy\n\nLMDeploy is a toolkit for compressing, deploying, and serving LLMs & VLMs.\n\n```sh\npip install lmdeploy>=0.9.1\n```\n\nLMDeploy abstracts the complex inference process of multi-modal Vision-Language Models (VLM) into an easy-to-use pipeline, similar to the Large Language Model (LLM) inference pipeline.\n\n#### A 'Hello, world' Example\n\n```python\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\n\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/tests/data/tiger.jpeg')\n\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\n\nresponse = pipe(('describe this image', image))\nprint(response.text)\n```\n\n#### Multi-images Inference\n\nWhen dealing with multiple images, you can put them all in one list. Keep in mind that multiple images will lead to a higher number of input tokens, and as a result, the size of the context window typically needs to be increased.\n\n```python\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\nfrom lmdeploy.vl.constants import IMAGE_TOKEN\n\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\n\nimage_urls=[\n    'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg',\n    'https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg'\n]\n\nimages = [load_image(img_url) for img_url in image_urls]\n# Numbering images improves multi-image conversations\nresponse = pipe((f'Image-1: {IMAGE_TOKEN}\\nImage-2: {IMAGE_TOKEN}\\ndescribe these two images', images))\nprint(response.text)\n```\n\n#### Batch Prompts Inference\n\nConducting inference with batch prompts is quite straightforward; just place them within a list structure:\n\n```python\nfrom lmdeploy import pipeline, PytorchEngineConfig\nfrom lmdeploy.vl import load_image\n\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\n\nimage_urls=[\n    \"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg\",\n    \"https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/det.jpg\"\n]\nprompts = [('describe this image', load_image(img_url)) for img_url in image_urls]\nresponse = pipe(prompts)\nprint(response)\n```\n\n#### Multi-turn Conversation\n\nThere are two ways to do the multi-turn conversations with the pipeline. One is to construct messages according to the format of OpenAI and use above introduced method, the other is to use the `pipeline.chat` interface.\n\n```python\nfrom lmdeploy import pipeline, PytorchEngineConfig, GenerationConfig\nfrom lmdeploy.vl import load_image\n\n# Please set tp=2 for the 38B version and tp=8 for the 241B-A28B version.\nmodel = 'OpenGVLab/InternVL3_5-8B'\npipe = pipeline(model, backend_config=PytorchEngineConfig(session_len=32768, tp=1))\n\nimage = load_image('https://raw.githubusercontent.com/open-mmlab/mmdeploy/main/demo/resources/human-pose.jpg')\ngen_config = GenerationConfig(top_k=50, top_p=0.95, temperature=0.6, max_new_tokens=8192)\nsess = pipe.chat(('describe this image', image), gen_config=gen_config)\nprint(sess.response.text)\nsess = pipe.chat('What is the woman doing?', session=sess, gen_config=gen_config)\nprint(sess.response.text)\n```\n\n#### Service\n\nLMDeploy's `api_server` enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup:\n\n```shell\nlmdeploy serve api_server OpenGVLab/InternVL3_5-8B --server-port 23333 --tp 1 --backend pytorch\n```\n\nTo use the OpenAI-style interface, you need to install OpenAI:\n\n```shell\npip install openai\n```\n\nThen, use the code below to make the API call:\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')\nmodel_name = client.models.list().data[0].id\nresponse = client.chat.completions.create(\n    model=model_name,\n    messages=[{\n        'role':\n        'user',\n        'content': [{\n            'type': 'text',\n            'text': 'describe this image',\n        }, {\n            'type': 'image_url',\n            'image_url': {\n                'url':\n                'https://modelscope.oss-cn-beijing.aliyuncs.com/resource/tiger.jpeg',\n            },\n        }],\n    }],\n    temperature=0.8,\n    top_p=0.8)\nprint(response)\n```\n\n## License\n\nThis project is released under the apache-2.0 License. This project uses the pre-trained Qwen3 as a component, which is licensed under the apache-2.0 License.\n\n## Citation\n\nIf you find this project useful in your research, please consider citing:\n\n```BibTeX\n@article{wang2025internvl3_5,\n  title={InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency},\n  author={Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others},\n  journal={arXiv preprint arXiv:2508.18265},\n  year={2025}\n}\n```\n",
      "card_hash": "df85730e66c214fbb36542054c64c93837128b8963b1d987e2ac11af6b6426ec",
      "token_count": 11885,
      "success": true
    },
    {
      "model_id": "microsoft/Florence-2-base",
      "card_text": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Florence-2-base/resolve/main/LICENSE\npipeline_tag: image-text-to-text\ntags:\n- vision\n---\n\n# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n\n## Model Summary\n\nThis Hub repository contains a HuggingFace's `transformers` implementation of Florence-2 model from Microsoft.\n\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. \n\nResources and Technical Documentation:\n+ [Florence-2 technical report](https://arxiv.org/abs/2311.06242). \n+ [Jupyter Notebook for inference and visualization of Florence-2-large model](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n\n| Model   | Model size | Model Description | \n| ------- | ------------- |   ------------- |  \n| Florence-2-base[[HF]](https://huggingface.co/microsoft/Florence-2-base) | 0.23B  | Pretrained model with FLD-5B  \n| Florence-2-large[[HF]](https://huggingface.co/microsoft/Florence-2-large) | 0.77B  | Pretrained model with FLD-5B  \n| Florence-2-base-ft[[HF]](https://huggingface.co/microsoft/Florence-2-base-ft) | 0.23B  | Finetuned model on a colletion of downstream tasks\n| Florence-2-large-ft[[HF]](https://huggingface.co/microsoft/Florence-2-large-ft) | 0.77B | Finetuned model on a colletion of downstream tasks\n \n## How to Get Started with the Model\n\nUse the code below to get started with the model.  All models are trained with float16. \n\n```python\nimport requests\n\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n\nprompt = \"<OD>\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n\ngenerated_ids = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    pixel_values=inputs[\"pixel_values\"],\n    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nparsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\n\nprint(parsed_answer)\n\n```\n\n\n## Tasks\n\nThis model is capable of performing different tasks through changing the prompts.\n\nFirst, let's define a function to run a prompt.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\n\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ndef run_example(task_prompt, text_input=None):\n    if text_input is None:\n        prompt = task_prompt\n    else:\n        prompt = task_prompt + text_input\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n    generated_ids = model.generate(\n      input_ids=inputs[\"input_ids\"],\n      pixel_values=inputs[\"pixel_values\"],\n      max_new_tokens=1024,\n      num_beams=3\n    )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n\n    print(parsed_answer)\n```\n</details>\n\nHere are the tasks `Florence-2` could perform:\n\n<details>\n<summary> Click to expand </summary>\n\n\n\n### Caption\n```python\nprompt = \"<CAPTION>\"\nrun_example(prompt)\n```\n\n### Detailed Caption\n```python\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### More Detailed Caption\n```python\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### Caption to Phrase Grounding \ncaption to phrase grounding task requires additional text input, i.e. caption. \n\nCaption to phrase grounding results format: \n{'\\<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n```python\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n```\n\n### Object Detection\n\nOD results format: \n{'\\<OD>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n\n```python\nprompt = \"<OD>\"\nrun_example(prompt)\n```\n\n### Dense Region Caption\nDense region caption results format: \n{'\\<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n```python\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\n```\n\n### Region proposal\nDense region caption results format: \n{'\\<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['', '', ...]}}\n```python\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\n```\n\n### OCR \n\n```python\nprompt = \"<OCR>\"\nrun_example(prompt)\n```\n\n### OCR with Region\nOCR with region output format:\n{'\\<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\n```python\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\n```\n\nfor More detailed examples, please refer to [notebook](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n</details>\n\n# Benchmarks\n\n## Florence-2 Zero-shot performance\n  \nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.  \n  \n| Method | #params | COCO Cap. test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | COCO Det. val2017 mAP |  \n|--------|---------|----------------------|------------------|--------------------|-----------------------|\n| Flamingo | 80B | 84.3 | - | - | - | \n| Florence-2-base| 0.23B | 133.0 | 118.7 | 70.1 | 34.7 | \n| Florence-2-large| 0.77B | 135.6 | 120.8 | 72.8 | 37.5 |\n\n  \nThe following table continues the comparison with performance on other vision-language evaluation tasks.  \n  \n| Method | Flickr30k test R@1 | Refcoco val Accuracy | Refcoco test-A Accuracy | Refcoco test-B Accuracy | Refcoco+ val Accuracy | Refcoco+ test-A Accuracy | Refcoco+ test-B Accuracy | Refcocog val Accuracy | Refcocog test Accuracy | Refcoco RES val mIoU |  \n|--------|----------------------|----------------------|-------------------------|-------------------------|-----------------------|--------------------------|--------------------------|-----------------------|------------------------|----------------------|  \n| Kosmos-2 | 78.7 | 52.3 | 57.4 | 47.3 | 45.5 | 50.7 | 42.2 | 60.6 | 61.7 | - |  \n| Florence-2-base | 83.6 | 53.9 | 58.4 | 49.7 | 51.5 | 56.4 | 47.9 | 66.3 | 65.1 | 34.6 |  \n| Florence-2-large | 84.4 | 56.3 | 61.6 | 51.4 | 53.6 | 57.9 | 49.9 | 68.0 | 67.0 | 35.8 |  \n\n\n\n## Florence-2 finetuned performance \n\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models *Florence-2-base-ft* and *Florence-2-large-ft* that can conduct a wide range of downstream tasks. \n  \nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"\u25b2\" indicates the usage of external OCR as input.  \n  \n| Method         | # Params | COCO Caption Karpathy test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | VQAv2 test-dev Acc | TextVQA test-dev Acc | VizWiz VQA test-dev Acc |  \n|----------------|----------|-----------------------------------|------------------|--------------------|--------------------|----------------------|-------------------------|  \n| **Specialist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| CoCa           | 2.1B     | 143.6                              | 122.4            | -                  | 82.3               | -                    | -                       |  \n| BLIP-2         | 7.8B     | 144.5                              | 121.6            | -                  | 82.2               | -                    | -                       |  \n| GIT2           | 5.1B     | 145.0                              | 126.9            | 148.6              | 81.7               | 67.3                 | 71.0                    |  \n| Flamingo       | 80B      | 138.1                              | -                | -                  | 82.0               | 54.1                 | 65.7                    |  \n| PaLI           | 17B      | 149.1                              | 127.0            | 160.0\u25b2             | 84.3               | 58.8 / 73.1\u25b2         | 71.6 / 74.4\u25b2            |  \n| PaLI-X         | 55B      | 149.2                              | 126.3            | 147.0 / 163.7\u25b2     | 86.0               | 71.4 / 80.8\u25b2         | 70.9 / 74.6\u25b2            |  \n| **Generalist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| Unified-IO     | 2.9B     | -                                  | 100.0            | -                  | 77.9               | -                    | 57.4                    |  \n| Florence-2-base-ft | 0.23B  | 140.0                              | 116.7            | 143.9              | 79.7               | 63.6                 | 63.6                    |  \n| Florence-2-large-ft | 0.77B  | 143.3                              | 124.9            | 151.1              | 81.7               | 73.5                 | 72.6                    |  \n  \n  \n| Method               | # Params | COCO Det. val2017 mAP | Flickr30k test R@1 | RefCOCO val Accuracy | RefCOCO test-A Accuracy | RefCOCO test-B Accuracy | RefCOCO+ val Accuracy | RefCOCO+ test-A Accuracy | RefCOCO+ test-B Accuracy | RefCOCOg val Accuracy | RefCOCOg test Accuracy | RefCOCO RES val mIoU |  \n|----------------------|----------|-----------------------|--------------------|----------------------|-------------------------|-------------------------|------------------------|---------------------------|---------------------------|------------------------|-----------------------|------------------------|  \n| **Specialist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| SeqTR                | -        | -                     | -                  | 83.7                 | 86.5                    | 81.2                    | 71.5                   | 76.3                      | 64.9                      | 74.9                   | 74.2                  | -                      |  \n| PolyFormer           | -        | -                     | -                  | 90.4                 | 92.9                    | 87.2                    | 85.0                   | 89.8                      | 78.0                      | 85.8                   | 85.9                  | 76.9                   |  \n| UNINEXT              | 0.74B    | 60.6                  | -                  | 92.6                 | 94.3                    | 91.5                    | 85.2                   | 89.6                      | 79.8                      | 88.7                   | 89.4                  | -                      |  \n| Ferret               | 13B      | -                     | -                  | 89.5                 | 92.4                    | 84.4                    | 82.8                   | 88.1                      | 75.2                      | 85.8                   | 86.3                  | -                      |  \n| **Generalist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| UniTAB               | -        | -                     | -                  | 88.6                 | 91.1                    | 83.8                    | 81.0                   | 85.4                      | 71.6                      | 84.6                   | 84.7                  | -                      |  \n| Florence-2-base-ft | 0.23B    | 41.4                  | 84.0                | 92.6                 | 94.8                    | 91.5                   | 86.8                   | 91.7                      | 82.2                      | 89.8                   | 82.2                  | 78.0                  |  \n| Florence-2-large-ft| 0.77B    | 43.4                  | 85.2               | 93.4                 | 95.3                    | 92.0                    | 88.3                   | 92.9                      | 83.6                      | 91.2                   | 91.7                  | 80.5                   |  \n  \n\n## BibTex and citation info\n\n```\n@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}\n```",
      "card_hash": "e49de5527bd39688745ce20388bf3f31fdb891abc0e6950a3f64630114769d17",
      "token_count": 3668,
      "success": true
    },
    {
      "model_id": "google/medgemma-4b-it",
      "card_text": "---\nlicense: other\nlicense_name: health-ai-developer-foundations\nlicense_link: https://developers.google.com/health-ai-developer-foundations/terms\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access MedGemma on Hugging Face\nextra_gated_prompt: >-\n  To access MedGemma on Hugging Face, you're required to review and\n  agree to [Health AI Developer Foundation's terms of use](https://developers.google.com/health-ai-developer-foundations/terms).\n  To do this, please ensure you're logged in to Hugging Face and click below.\n  Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/medgemma-4b-pt\ntags:\n- medical\n- radiology\n- clinical-reasoning\n- dermatology\n- pathology\n- ophthalmology\n- chest-x-ray\n---\n\n# MedGemma model card\n\n**Model documentation:** [MedGemma](https://developers.google.com/health-ai-developer-foundations/medgemma)\n\n**Resources:**\n\n*   Model on Google Cloud Model Garden: [MedGemma](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/medgemma)\n*   Model on Hugging Face: [MedGemma](https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4)\n*   GitHub repository (supporting code, Colab notebooks, discussions, and\n    issues): [MedGemma](https://github.com/google-health/medgemma)\n*   Quick start notebook: [GitHub](https://github.com/google-health/medgemma/blob/main/notebooks/quick_start_with_hugging_face.ipynb)\n*   Fine-tuning notebook: [GitHub](https://github.com/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb)\n*   Concept applications built using MedGemma: [Collection](https://huggingface.co/collections/google/medgemma-concept-apps-686ea036adb6d51416b0928a)\n*   Support: See [Contact](https://developers.google.com/health-ai-developer-foundations/medgemma/get-started.md#contact)\n*   License: The use of MedGemma is governed by the [Health AI Developer\n    Foundations terms of\n    use](https://developers.google.com/health-ai-developer-foundations/terms).\n\n**Author:** Google\n\n## Model information\n\nThis section describes the MedGemma model and how to use it.\n\n### Description\n\nMedGemma is a collection of [Gemma 3](https://ai.google.dev/gemma/docs/core)\nvariants that are trained for performance on medical text and image\ncomprehension. Developers can use MedGemma to accelerate building\nhealthcare-based AI applications. MedGemma currently comes in three variants: a\n4B multimodal version and 27B text-only and multimodal versions.\n\nBoth MedGemma multimodal versions utilize a\n[SigLIP](https://arxiv.org/abs/2303.15343) image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nchest X-rays, dermatology images, ophthalmology images, and histopathology\nslides. Their LLM components are trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\n\nMedGemma 4B is available in both pre-trained (suffix: `-pt`) and\ninstruction-tuned (suffix `-it`) versions. The instruction-tuned version is a\nbetter starting point for most applications. The pre-trained version is\navailable for those who want to experiment more deeply with the models.\n\nMedGemma 27B multimodal has pre-training on medical image, medical record and\nmedical record comprehension tasks. MedGemma 27B text-only has been trained\nexclusively on medical text. Both models have been optimized for inference-time\ncomputation on medical reasoning. This means it has slightly higher performance\non some text benchmarks than MedGemma 27B multimodal. Users who want to work\nwith a single model for both medical text, medical record and medical image\ntasks are better suited for MedGemma 27B multimodal. Those that only need text\nuse-cases may be better served with the text-only variant. Both MedGemma 27B\nvariants are only available in instruction-tuned versions.\n\nMedGemma variants have been evaluated on a range of clinically relevant\nbenchmarks to illustrate their baseline performance. These evaluations are based\non both open benchmark datasets and curated datasets. Developers can fine-tune\nMedGemma variants for improved performance. Consult the [Intended\nUse](https://developers.google.com/health-ai-developer-foundations/medgemma/model-card#intended_use)\nsection below for more details.\n\nMedGemma is optimized for medical applications that involve a text generation\ncomponent. For medical image-based applications that do not involve text\ngeneration, such as data-efficient classification, zero-shot classification, or\ncontent-based or semantic image retrieval, the [MedSigLIP image\nencoder](https://developers.google.com/health-ai-developer-foundations/medsiglip/model-card)\nis recommended. MedSigLIP is based on the same image encoder that powers\nMedGemma.\n\nPlease consult the [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)\nfor more details.\n\n### How to use\n\nBelow are some example code snippets to help you quickly get started running the\nmodel locally on GPU. If you want to use the model at scale, we recommend that\nyou create a production version using [Model\nGarden](https://cloud.google.com/model-garden).\n\nFirst, install the Transformers library. Gemma 3 is supported starting from\ntransformers 4.50.0.\n\n```sh\n$ pip install -U transformers\n```\n\n**Run model with the `pipeline` API**\n\n```python\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/medgemma-4b-it\",\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n)\n\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n            {\"type\": \"image\", \"image\": image}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n```\n\n**Run the model directly**\n\n```python\n# pip install accelerate\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/medgemma-4b-it\"\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n            {\"type\": \"image\", \"image\": image}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n```\n\n### Examples\n\nSee the following Colab notebooks for examples of how to use MedGemma:\n\n*   To give the model a quick try, running it locally with weights from Hugging\n    Face, see [Quick start notebook in\n    Colab](https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/quick_start_with_hugging_face.ipynb).\n    Note that you will need to use Colab Enterprise to obtain adequate GPU\n    resources to run either 27B model without quantization.\n\n*   For an example of fine-tuning the 4B model, see the [Fine-tuning notebook in\n    Colab](https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb).\n    The 27B models can be fine tuned in a similar manner but will require more\n    time and compute resources than the 4B model.\n\n### Model architecture overview\n\nThe MedGemma model is built based on [Gemma 3](https://ai.google.dev/gemma/) and\nuses the same decoder-only transformer architecture as Gemma 3\\. To read more\nabout the architecture, consult the Gemma 3 [model\ncard](https://ai.google.dev/gemma/docs/core/model_card_3).\n\n### Technical specifications\n\n*   **Model type**: Decoder-only Transformer architecture, see the [Gemma 3\n    Technical\n    Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)\n*   **Input Modalities**: Text, vision\n*   **Output Modality:** Text only\n*   **Attention mechanism**: Grouped-query attention (GQA)\n*   **Context length**: Supports long context, at least 128K tokens\n*   **Key publication**: https://arxiv.org/abs/2507.05201\n*   **Model created**: July 9, 2025\n\n*   **Model version**: 1.0.1\n\n### Citation\n\nWhen using this model, please cite: Sellergren et al. \"MedGemma Technical\nReport.\" *arXiv preprint arXiv:2507.05201* (2025).\n\n```none\n@article{sellergren2025medgemma,\n  title={MedGemma Technical Report},\n  author={Sellergren, Andrew and Kazemzadeh, Sahar and Jaroensri, Tiam and Kiraly, Atilla and Traverse, Madeleine and Kohlberger, Timo and Xu, Shawn and Jamil, Fayaz and Hughes, C\u00edan and Lau, Charles and others},\n  journal={arXiv preprint arXiv:2507.05201},\n  year={2025}\n}\n```\n\n### Inputs and outputs\n\n**Input**:\n\n*   Text string, such as a question or prompt\n*   Images, normalized to 896 x 896 resolution and encoded to 256 tokens each\n*   Total input length of 128K tokens\n\n**Output**:\n\n*   Generated text in response to the input, such as an answer to a question,\n    analysis of image content, or a summary of a document\n*   Total output length of 8192 tokens\n\n### Performance and validation\n\nMedGemma was evaluated across a range of different multimodal classification,\nreport generation, visual question answering, and text-based tasks.\n\n### Key performance metrics\n\n#### Imaging evaluations\n\nThe multimodal performance of MedGemma 4B and 27B multimodal was evaluated\nacross a range of benchmarks, focusing on radiology, dermatology,\nhistopathology, ophthalmology, and multimodal clinical reasoning.\n\nMedGemma 4B outperforms the base Gemma 3 4B model across all tested multimodal\nhealth benchmarks.\n\n| Task and metric | Gemma 3 4B | MedGemma 4B |\n| :---- | :---- | :---- |\n| **Medical image classification** |  |  |\n| MIMIC CXR\\*\\* \\- macro F1 for top 5 conditions | 81.2 | 88.9 |\n| CheXpert CXR \\- macro F1 for top 5 conditions | 32.6 | 48.1 |\n| CXR14 \\- macro F1 for 3 conditions | 32.0 | 50.1 |\n| PathMCQA\\* (histopathology, internal\\*\\*)  \\- Accuracy | 37.1 | 69.8 |\n| US-DermMCQA\\* \\- Accuracy | 52.5 | 71.8 |\n| EyePACS\\* (fundus, internal) \\- Accuracy | 14.4 | 64.9 |\n| **Visual question answering** |  |  |\n| SLAKE (radiology) \\- Tokenized F1 | 40.2 | 72.3 |\n| VQA-RAD\\*\\*\\* (radiology) \\- Tokenized F1 | 33.6 | 49.9 |\n| **Knowledge and reasoning** |  |  |  |  |\n| MedXpertQA (text \\+ multimodal questions) \\- Accuracy | 16.4 | 18.8 |\n\n*Internal datasets. US-DermMCQA is described in [Liu (2020, Nature\nmedicine)](https://www.nature.com/articles/s41591-020-0842-3), presented as a\n4-way MCQ per example for skin condition classification. PathMCQA is based on\nmultiple datasets, presented as 3-9 way MCQ per example for identification,\ngrading, and subtype for breast, cervical, and prostate cancer. EyePACS is a\ndataset of fundus images with classification labels based on 5-level diabetic\nretinopathy severity (None, Mild, Moderate, Severe, Proliferative). More details\nin the [MedGemma Technical Report](https://arxiv.org/abs/2507.05201).\n\n**Based on radiologist adjudicated labels, described in [Yang (2024,\narXiv)](https://arxiv.org/pdf/2405.03162) Section A.1.1.\n\n***Based on \"balanced split,\" described in [Yang (2024,\narXiv)](https://arxiv.org/pdf/2405.03162).\n\n#### Chest X-ray report generation\n\nMedGemma chest X-ray (CXR) report generation performance was evaluated on\n[MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.1.0/) using the [RadGraph\nF1 metric](https://arxiv.org/abs/2106.14463). We compare the MedGemma\npre-trained checkpoint with our previous best model for CXR report generation,\n[PaliGemma 2](https://arxiv.org/abs/2412.03555).\n\n| Metric | MedGemma 4B (pre-trained) | MedGemma 4B (tuned for CXR)| PaliGemma 2 3B (tuned for CXR) | PaliGemma 2 10B (tuned for CXR) |\n| :---- | :---- | :---- | :---- | :---- |\n| MIMIC CXR \\- RadGraph F1 | 29.5 | 30.3 |28.8 | 29.5 |\n\n\n\nThe instruction-tuned versions of MedGemma 4B and MedGemma 27B achieve lower\nscores (21.9 and 21.3, respectively) due to the differences in reporting style\ncompared to the MIMIC ground truth reports. Further fine-tuning on MIMIC reports\nenables users to achieve improved performance, as shown by the improved\nperformance of the MedGemma 4B model that was tuned for CXR.\n\n#### Text evaluations\n\nMedGemma 4B and text-only MedGemma 27B were evaluated across a range of\ntext-only benchmarks for medical knowledge and reasoning.\n\nThe MedGemma models outperform their respective base Gemma models across all\ntested text-only health benchmarks.\n\n| Metric | Gemma 3 4B | MedGemma 4B |\n| :---- | :---- | :---- |\n| MedQA (4-op) | 50.7 | 64.4 |\n| MedMCQA | 45.4 | 55.7 |\n| PubMedQA | 68.4 | 73.4 |\n| MMLU Med | 67.2 | 70.0 |\n| MedXpertQA (text only) | 11.6 | 14.2 |\n| AfriMed-QA (25 question test set) | 48.0 | 52.0 |\n\nFor all MedGemma 27B results, [test-time\nscaling](https://arxiv.org/abs/2501.19393) is used to improve performance.\n\n#### Medical record evaluations\n\nAll models were evaluated on a question answer dataset from synthetic FHIR data\nto answer questions about patient records. MedGemma 27B multimodal's\nFHIR-specific training gives it significant improvement over other MedGemma and\nGemma models.\n\n| Metric | Gemma 3 4B | MedGemma 4B |\n| :---- | :---- | :---- |\n| EHRQA | 70.9 | 67.6 |\n\n\n### Ethics and safety evaluation\n\n#### Evaluation approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n*   **Child safety**: Evaluation of text-to-text and image-to-text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n*   **Content safety:** Evaluation of text-to-text and image-to-text prompts\n    covering safety policies, including harassment, violence and gore, and hate\n    speech.\n*   **Representational harms**: Evaluation of text-to-text and image-to-text\n    prompts covering safety policies, including bias, stereotyping, and harmful\n    associations or inaccuracies.\n*   **General medical harms:** Evaluation of text-to-text and image-to-text\n    prompts covering safety policies, including information quality and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\n\n#### Evaluation results\n\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\n\n## Data card\n\n### Dataset overview\n\n#### Training\n\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma 4B utilizes a [SigLIP](https://arxiv.org/abs/2303.15343) image encoder\nthat has been specifically pre-trained on a variety of de-identified medical\ndata, including radiology images, histopathology images, ophthalmology images,\nand dermatology images. Its LLM component is trained on a diverse set of medical\ndata, including medical text relevant to radiology images, chest-x rays,\nhistopathology patches, ophthalmology images and dermatology images.\n\n#### Evaluation\n\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 5 different tasks and 6\nmedical image modalities. These include both open benchmark datasets and curated\ndatasets, with a focus on expert human evaluations for tasks like CXR report\ngeneration and radiology VQA.\n\n### Ethics and safety evaluation\n\n#### Evaluation approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n*   **Child safety**: Evaluation of text-to-text and image-to-text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n*   **Content safety:** Evaluation of text-to-text and image-to-text prompts\n    covering safety policies, including harassment, violence and gore, and hate\n    speech.\n*   **Representational harms**: Evaluation of text-to-text and image-to-text\n    prompts covering safety policies, including bias, stereotyping, and harmful\n    associations or inaccuracies.\n*   **General medical harms:** Evaluation of text-to-text and image-to-text\n    prompts covering safety policies, including information quality and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\n\n#### Evaluation results\n\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\n\n## Data card\n\n### Dataset overview\n\n#### Training\n\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma multimodal variants utilize a\n[SigLIP](https://arxiv.org/abs/2303.15343) image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nradiology images, histopathology images, ophthalmology images, and dermatology\nimages. Their LLM component is trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\n\n#### Evaluation\n\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 6 different tasks and 4\nmedical image modalities. These benchmarks include both open and internal\ndatasets.\n\n#### Source\n\nMedGemma utilizes a combination of public and private datasets.\n\nThis model was trained on diverse public datasets including MIMIC-CXR (chest\nX-rays and reports), ChestImaGenome: Set of bounding boxes linking image\nfindings with anatomical regions for MIMIC-CXR (MedGemma 27B multimodal only),\nSLAKE (multimodal medical images and questions), PAD-UFES-20 (skin lesion images\nand data), SCIN (dermatology images), TCGA (cancer genomics data), CAMELYON\n(lymph node histopathology images), PMC-OA (biomedical literature with images),\nand Mendeley Digital Knee X-Ray (knee X-rays).\n\nAdditionally, multiple diverse proprietary datasets were licensed and\nincorporated (described next).\n\n### Data Ownership and Documentation\n\n*   [MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.1.0/): MIT Laboratory\n    for Computational Physiology and Beth Israel Deaconess Medical Center\n    (BIDMC).\n*   [Slake-VQA](https://www.med-vqa.com/slake/): The Hong Kong Polytechnic\n    University (PolyU), with collaborators including West China Hospital of\n    Sichuan University and Sichuan Academy of Medical Sciences / Sichuan\n    Provincial People's Hospital.\n*   [PAD-UFES-20](https://pmc.ncbi.nlm.nih.gov/articles/PMC7479321/): Federal\n    University of Esp\u00edrito Santo (UFES), Brazil, through its Dermatological and\n    Surgical Assistance Program (PAD).\n*   [SCIN](https://github.com/google-research-datasets/scin): A collaboration\n    between Google Health and Stanford Medicine.\n*   [TCGA](https://portal.gdc.cancer.gov/) (The Cancer Genome Atlas): A joint\n    effort of National Cancer Institute and National Human Genome Research\n    Institute. Data from TCGA are available via the Genomic Data Commons (GDC)\n*   [CAMELYON](https://camelyon17.grand-challenge.org/Data/): The data was\n    collected from Radboud University Medical Center and University Medical\n    Center Utrecht in the Netherlands.\n*   [PMC-OA (PubMed Central Open Access\n    Subset)](https://catalog.data.gov/dataset/pubmed-central-open-access-subset-pmc-oa):\n    Maintained by the National Library of Medicine (NLM) and National Center for\n    Biotechnology Information (NCBI), which are part of the NIH.\n*   [MedQA](https://arxiv.org/pdf/2009.13081): This dataset was created by a\n    team of researchers led by Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\n    Weng, Hanyi Fang, and Peter Szolovits\n*   [Mendeley Digital Knee\n    X-Ray](https://data.mendeley.com/datasets/t9ndx37v5h/1): This dataset is\n    from Rani Channamma University, and is hosted on Mendeley Data.\n*   [AfriMed-QA](https://afrimedqa.com/): This data was developed and led by\n    multiple collaborating organizations and researchers include key\n    contributors: Intron Health, SisonkeBiotik, BioRAMP, Georgia Institute of\n    Technology, and MasakhaneNLP.\n*   [VQA-RAD](https://www.nature.com/articles/sdata2018251): This dataset was\n    created by a research team led by Jason J. Lau, Soumya Gayen, Asma Ben\n    Abacha, and Dina Demner-Fushman and their affiliated institutions (the US\n    National Library of Medicine and National Institutes of Health)\n*   [Chest ImaGenome](https://physionet.org/content/chest-imagenome/1.0.0/): IBM\n    Research.\n*   [MedExpQA](https://www.sciencedirect.com/science/article/pii/S0933365724001805):\n    This dataset was created by researchers at the HiTZ Center (Basque Center\n    for Language Technology and Artificial Intelligence).\n*   [MedXpertQA](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA): This\n    dataset was developed by researchers at Tsinghua University (Beijing, China)\n    and Shanghai Artificial Intelligence Laboratory (Shanghai, China).\n*   [HealthSearchQA](https://huggingface.co/datasets/katielink/healthsearchqa):\n    This dataset consists of consisting of 3,173 commonly searched consumer\n    questions\n\nIn addition to the public datasets listed above, MedGemma was also trained on\nde-identified, licensed datasets or datasets collected internally at Google from\nconsented participants.\n\n*   **Radiology dataset 1:** De-identified dataset of different CT studies\n    across body parts from a US-based radiology outpatient diagnostic center\n    network.\n*   **Ophthalmology dataset 1 (EyePACS):** De-identified dataset of fundus\n    images from diabetic retinopathy screening.\n*   **Dermatology dataset 1:** De-identified dataset of teledermatology skin\n    condition images (both clinical and dermatoscopic) from Colombia.\n*   **Dermatology dataset 2:** De-identified dataset of skin cancer images (both\n    clinical and dermatoscopic) from Australia.\n*   **Dermatology dataset 3:** De-identified dataset of non-diseased skin images\n    from an internal data collection effort.\n*   **Pathology dataset 1:** De-identified dataset of histopathology H\\&E whole\n    slide images created in collaboration with an academic research hospital and\n    biobank in Europe. Comprises de-identified colon, prostate, and lymph nodes.\n*   **Pathology dataset 2:** De-identified dataset of lung histopathology H\\&E\n    and IHC whole slide images created by a commercial biobank in the United\n    States.\n*   **Pathology dataset 3:** De-identified dataset of prostate and lymph node\n    H\\&E and IHC histopathology whole slide images created by a contract\n    research organization in the United States.\n*   **Pathology dataset 4:** De-identified dataset of histopathology whole slide\n    images created in collaboration with a large, tertiary teaching hospital in\n    the United States. Comprises a diverse set of tissue and stain types,\n    predominantly H\\&E.\n*   **EHR dataset 1:** Question/answer dataset drawn from synthetic FHIR records\n    created by [Synthea.](https://synthetichealth.github.io/synthea/) The test\n    set includes 19 unique patients with 200 questions per patient divided into\n    10 different categories.\n\n### Data citation\n\n*   **MIMIC-CXR:** Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng,\n    S. (2024). MIMIC-CXR Database (version 2.1.0). PhysioNet.\n    [https://physionet.org/content/mimic-cxr/2.1.0/](https://physionet.org/content/mimic-cxr/2.1.0/)\n    *and* Johnson, Alistair E. W., Tom J. Pollard, Seth J. Berkowitz, Nathaniel\n    R. Greenbaum, Matthew P. Lungren, Chih-Ying Deng, Roger G. Mark, and Steven\n    Horng. 2019\\. \"MIMIC-CXR, a de-Identified Publicly Available Database of\n    Chest Radiographs with Free-Text Reports.\" *Scientific Data 6* (1): 1\u20138.\n\n*   **SLAKE:** Liu, Bo, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\n    2021.SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\n    Visual Question Answering.\"\n    [http://arxiv.org/abs/2102.09542](http://arxiv.org/abs/2102.09542).\n\n*   **PAD-UEFS-20:** Pacheco, Andre GC, et al. \"PAD-UFES-20: A skin lesion\n    dataset composed of patient data and clinical images collected from\n    smartphones.\" *Data in brief* 32 (2020): 106221\\.\n\n*   **SCIN:** Ward, Abbi, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley\n    Carrick, Bilson Campana, Jay Hartford, et al. 2024\\. \"Creating an Empirical\n    Dermatology Dataset Through Crowdsourcing With Web Search Advertisements.\"\n    *JAMA Network Open 7* (11): e2446615\u2013e2446615.\n\n*   **TCGA:** The results shown here are in whole or part based upon data\n    generated by the TCGA Research Network:\n    [https://www.cancer.gov/tcga](https://www.cancer.gov/tcga).\n\n*   **CAMELYON16:** Ehteshami Bejnordi, Babak, Mitko Veta, Paul Johannes van\n    Diest, Bram van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen A. W. M.\n    van der Laak, et al. 2017\\. \"Diagnostic Assessment of Deep Learning\n    Algorithms for Detection of Lymph Node Metastases in Women With Breast\n    Cancer.\" *JAMA 318* (22): 2199\u20132210.\n\n*   **Mendeley Digital Knee X-Ray:** Gornale, Shivanand; Patravali, Pooja\n    (2020), \"Digital Knee X-ray Images\", Mendeley Data, V1, doi:\n    10.17632/t9ndx37v5h.1\n\n*   **VQA-RAD:** Lau, Jason J., Soumya Gayen, Asma Ben Abacha, and Dina\n    Demner-Fushman. 2018\\. \"A Dataset of Clinically Generated Visual Questions\n    and Answers about Radiology Images.\" *Scientific Data 5* (1): 1\u201310.\n\n*   **Chest ImaGenome:** Wu, J., Agu, N., Lourentzou, I., Sharma, A., Paguio,\n    J., Yao, J. S., Dee, E. C., Mitchell, W., Kashyap, S., Giovannini, A., Celi,\n    L. A., Syeda-Mahmood, T., & Moradi, M. (2021). Chest ImaGenome Dataset\n    (version 1.0.0). PhysioNet. RRID:SCR\\_007345.\n    [https://doi.org/10.13026/wv01-y230](https://doi.org/10.13026/wv01-y230)\n\n*   **MedQA:** Jin, Di, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang,\n    and Peter Szolovits. 2020\\. \"What Disease Does This Patient Have? A\n    Large-Scale Open Domain Question Answering Dataset from Medical Exams.\"\n    [http://arxiv.org/abs/2009.13081](http://arxiv.org/abs/2009.13081).\n\n*   **AfrimedQA:** Olatunji, Tobi, Charles Nimo, Abraham Owodunni, Tassallah\n    Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, et al. 2024\\.\n    \"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n    Benchmark Dataset.\"\n    [http://arxiv.org/abs/2411.15640](http://arxiv.org/abs/2411.15640).\n\n*   **MedExpQA:** Alonso, I., Oronoz, M., & Agerri, R. (2024). MedExpQA:\n    Multilingual Benchmarking of Large Language Models for Medical Question\n    Answering. *arXiv preprint arXiv:2404.05590*. Retrieved from\n    [https://arxiv.org/abs/2404.05590](https://arxiv.org/abs/2404.05590)\n\n*   **MedXpertQA:** Zuo, Yuxin, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu,\n    Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025\\. \"MedXpertQA:\n    Benchmarking Expert-Level Medical Reasoning and Understanding.\"\n    [http://arxiv.org/abs/2501.18362](http://arxiv.org/abs/2501.18362).\n\n### De-identification/anonymization:\n\nGoogle and its partners utilize datasets that have been rigorously anonymized or\nde-identified to ensure the protection of individual research participants and\npatient privacy.\n\n## Implementation information\n\nDetails about the model internals.\n\n### Software\n\nTraining was done using [JAX](https://github.com/jax-ml/jax).\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\n## Use and limitations\n\n### Intended use\n\nMedGemma is an open multimodal generative AI model intended to be used as a\nstarting point that enables more efficient development of downstream healthcare\napplications involving medical text and images. MedGemma is intended for\ndevelopers in the life sciences and healthcare space. Developers are responsible\nfor training, adapting and making meaningful changes to MedGemma to accomplish\ntheir specific intended use. MedGemma models can be fine-tuned by developers\nusing their own proprietary data for their specific tasks or solutions.\n\nMedGemma is based on Gemma 3 and has been further trained on medical images and\ntext. MedGemma enables further development in any medical context (image and\ntextual), however the model was pre-trained using chest X-ray, pathology,\ndermatology, and fundus images. Examples of tasks within MedGemma's training\ninclude visual question answering pertaining to medical images, such as\nradiographs, or providing answers to textual medical questions. Full details of\nall the tasks MedGemma has been evaluated can be found in the [MedGemma\nTechnical Report](https://arxiv.org/abs/2507.05201).\n\n### Benefits\n\n*   Provides strong baseline medical image and text comprehension for models of\n    its size.\n*   This strong performance makes it efficient to adapt for downstream\n    healthcare-based use cases, compared to models of similar size without\n    medical data pre-training.\n*   This adaptation may involve prompt engineering, grounding, agentic\n    orchestration or fine-tuning depending on the use case, baseline validation\n    requirements, and desired performance characteristics.\n\n### Limitations\n\nMedGemma is not intended to be used without appropriate validation, adaptation\nand/or making meaningful modification by developers for their specific use case.\nThe outputs generated by MedGemma are not intended to directly inform clinical\ndiagnosis, patient management decisions, treatment recommendations, or any other\ndirect clinical practice applications. Performance benchmarks highlight baseline\ncapabilities on relevant benchmarks, but even for image and text domains that\nconstitute a substantial portion of training data, inaccurate model output is\npossible. All outputs from MedGemma should be considered preliminary and require\nindependent verification, clinical correlation, and further investigation\nthrough established research and development methodologies.\n\nMedGemma's multimodal capabilities have been primarily evaluated on single-image\ntasks. MedGemma has not been evaluated in use cases that involve comprehension\nof multiple images.\n\nMedGemma has not been evaluated or optimized for multi-turn applications.\n\nMedGemma's training may make it more sensitive to the specific prompt used than\nGemma 3\\.\n\nWhen adapting MedGemma developer should consider the following:\n\n*   **Bias in validation data:** As with any research, developers should ensure\n    that any downstream application is validated to understand performance using\n    data that is appropriately representative of the intended use setting for\n    the specific application (e.g., age, sex, gender, condition, imaging device,\n    etc).\n*   **Data contamination concerns**: When evaluating the generalization\n    capabilities of a large model like MedGemma in a medical context, there is a\n    risk of data contamination, where the model might have inadvertently seen\n    related medical information during its pre-training, potentially\n    overestimating its true ability to generalize to novel medical concepts.\n    Developers should validate MedGemma on datasets not publicly available or\n    otherwise made available to non-institutional researchers to mitigate this\n    risk.\n\n\n### Release notes\n\n*   May 20, 2025: Initial Release\n*   July 9, 2025 Bug Fix: Fixed the subtle degradation in the multimodal\n    performance. The issue was due to a missing end-of-image token in the model\n    vocabulary, impacting combined text-and-image tasks. This fix reinstates and\n    correctly maps that token, ensuring text-only tasks remain unaffected while\n    restoring multimodal performance.",
      "card_hash": "3717e58eac9c9957c8f80e58d8f3e4854759da9f7abe76862aa100ce8b5d3aab",
      "token_count": 8827,
      "success": true
    },
    {
      "model_id": "dengcao/GLM-4.1V-9B-Thinking-AWQ",
      "card_text": "---\nbase_model:\n- ZhipuAI/GLM-4.1V-9B-Thinking\nlicense: mit\npipeline_tag: image-text-to-text\ntags:\n- glm4v\n- AWQ\n- vLLM\nbase_model_relation: quantized\nlibrary_name: transformers\n---\n\n# GLM-4.1V-9B-Thinking-AWQ\n\nThis repository hosts the **AWQ quantized version** of GLM-4.1V-9B-Thinking, a powerful Vision-Language Model (VLM) designed for versatile multimodal understanding and reasoning.\n\nThe model was presented in the paper [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006).\n\n**Official GitHub Repository**: [https://github.com/THUDM/GLM-4.1V-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)\n\nBase model: [ZhipuAI/GLM-4.1V-9B-Thinking](https://www.modelscope.cn/models/ZhipuAI/GLM-4.1V-9B-Thinking)\n\n---\n\n### Model Update Date\n```\n2025-07-03\n1. \u9996\u6b21commit\n2. \u786e\u5b9a\u652f\u63011\u30012\u30014\u5361\u7684`tensor-parallel-size`\u542f\u52a8\n```\n\n### Dependencies\n\n```\nvllm==0.9.2\n```\n\n<div style=\"\n    background: rgba(255, 193, 61, 0.15);\n    padding: 16px;\n    border-radius: 6px;\n    border: 1px solid rgba(255, 165, 0, 0.3);\n    margin: 16px 0;\n\">\n### \ud83d\udca1 2025-07-03 Temporary Installation Command \ud83d\udca1\n\n```\npip3 install -r requirements.txt\ngit clone https://github.com/zRzRzRzRzRzRzR/vllm.git\ncd vllm\ngit checkout glm4_1-v\nVLLM_USE_PRECOMPILED=1 pip install --editable .\n```\n</div>\n\n### Model List\n\n| \u6587\u4ef6\u5927\u5c0f    | \u6700\u8fd1\u66f4\u65b0\u65f6\u95f4       |\n|---------|--------------|\n| `6.9GB` | `2025-07-03` |\n\n### Model Download\n\n```python\nfrom modelscope import snapshot_download\nsnapshot_download('dengcao/GLM-4.1V-9B-Thinking-AWQ', cache_dir=\"\u672c\u5730\u8def\u5f84\")\n```\n\n---\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/THUDM/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width=\"40%\"/>\n</div>\n<p align=\"center\">\n    \ud83d\udc4b Join our <a href=\"https://discord.com/invite/8cnQKdAprg\" target=\"_blank\">Discord</a>\n    <br>\n    \ud83d\udcd6 View the GLM-4.1V-9B-Thinking <a href=\"https://arxiv.org/abs/2507.01006\" target=\"_blank\">paper</a>.\n    <br>\n    \ud83d\udca1 Try the online demo on <a href=\"https://huggingface.co/spaces/THUDM/GLM-4.1V-9B-Thinking-API-Demo\" target=\"_blank\">Hugging Face</a> or <a href=\"https://modelscope.cn/studios/ZhipuAI/GLM-4.1V-9B-Thinking-Demo\" target=\"_blank\">ModelScope</a> for GLM-4.1V-9B-Thinking.\n    <br>\n    \ud83d\udccd Using GLM-4.1V-9B-Thinking API at <a href=\"https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking\">Zhipu Foundation Model Open Platform</a>\n</p>\n\n## Model Introduction\n\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\n\nBased on the [GLM-4-9B-0414](https://github.com/THUDM/GLM-4) foundation model, we present the new open-source VLM model\n**GLM-4.1V-9B-Thinking**, designed to explore the upper limits of reasoning in vision-language models. By introducing\na \"thinking paradigm\" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\n\n![rl](https://raw.githubusercontent.com/THUDM/GLM-4.1V-Thinking/refs/heads/main/resources/rl.jpeg)\n\nCompared to the previous generation models CogVLM2 and the GLM-4V series, **GLM-4.1V-Thinking** offers the\nfollowing improvements:\n\n1. The first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\n   across various sub-domains.\n2. Supports **64k** context length.\n3. Handles **arbitrary aspect ratios** and up to **4K** image resolution.\n4. Provides an open-source version supporting both **Chinese and English bilingual** usage.\n\n## Model Information\n\n### Model Download Links\n\n| Model                | Download Links                                                                                                                                      | Model Type      |\n|----------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n| GLM-4.1V-9B-Thinking | [\ud83e\udd17 Hugging Face](https://huggingface.co/THUDM/GLM-4.1V-9B-Thinking)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.1V-9B-Thinking) | Reasoning Model |\n| GLM-4.1V-9B-Base     | [\ud83e\udd17 Hugging Face](https://huggingface.co/THUDM/GLM-4.1V-9B-Base)<br> [\ud83e\udd16 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.1V-9B-Base)         | Base Model      |\n\nThe model's algorithm implementation can be found in the\nofficial [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4v) repository.\n\n### Runtime Requirements\n\n#### Inference\n\n| Device (Single GPU) | Framework    | Min Memory | Speed              | Precision |\n|---------------------|--------------|------------|--------------------|-----------|\n| NVIDIA A100         | transformers | 22GB       | 14 - 22 Tokens / s | BF16      |\n| NVIDIA A100         | vLLM         | 22GB       | 60 - 70 Tokens / s | BF16      |\n\n#### Fine-tuning\n\nThe following results are based on image fine-tuning using the [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)\ntoolkit.\n\n| Device (Cluster) | Strategy   | Min Memory / # of GPUs | Batch Size (per GPU) | Freezing    |\n|------------------|------------|------------------------|----------------------|-------------|\n| NVIDIA A100      | LORA       | 21GB / 1 GPU           | 1                    | Freeze VIT  |\n| NVIDIA A100      | FULL ZERO2 | 280GB / 4 GPUs         | 1                    | Freeze VIT  |\n| NVIDIA A100      | FULL ZERO3 | 192GB / 4 GPUs         | 1                    | Freeze VIT  |\n| NVIDIA A100      | FULL ZERO2 | 304GB / 4 GPUs         | 1                    | No Freezing |\n| NVIDIA A100      | FULL ZERO3 | 210GB / 4 GPUs         | 1                    | No Freezing |\n\n> Note: Fine-tuning with Zero2 may result in zero loss; Zero3 is recommended for stable training.\n\n## Benchmark Performance\n\nBased on the [GLM-4-9B-0414](https://github.com/THUDM/GLM-4) foundation model, we present the new open-source VLM model\n**GLM-4.1V-9B-Thinking**, which introduces a \"thinking\" paradigm and leverages Reinforcement Learning with Curriculum\nSampling (RLCS) to comprehensively enhance model capabilities.\nIt achieves state-of-the-art performance among vision-language models at the 10B parameter scale, matching or even\nsurpassing the 72B Qwen-2.5-VL on 18 benchmark tasks.\nWe also open-source the base model **GLM-4.1V-9B-Base** to support further research on the frontier of vision-language\nmodels.\n\n![bench](https://raw.githubusercontent.com/THUDM/GLM-4.1V-Thinking/refs/heads/main/resources/bench.jpeg)\n\n## Model Inference\n\nAll inference scripts are located in the `inference` folder and include:\n\n+ `trans_infer_cli.py`: A command-line interactive script using the `transformers` library as the backend. It supports\n  multi-turn dialogue.\n+ `trans_infer_gradio.py`: A Gradio-based web UI script using the `transformers` backend. It supports multimodal inputs\n  such as images, videos, PDFs, and PPTs.\n+ OpenAI-compatible API service with `vllm`, along with a simple request example provided in `vllm_api_request.py`.\n\n    ```shell\n    vllm serve THUDM/GLM-4.1V-9B-Thinking --limit-mm-per-prompt '{\"image\":32}' --allowed-local-media-path /\n    ```\n\n  + If `--limit-mm-per-prompt` is not specified, only 1 image is supported. The model supports a maximum of 1 video or\n    300 images per input \u2014 it does **not** support simultaneous image and video inputs.\n  + `--allowed-local-media-path` must be set to permit access to local multimodal inputs.\n\n+ `trans_infer_bench`: Academic benchmarking script for inference with `GLM-4.1V-9B-Thinking`. Key features:\n  + Automatically interrupts thinking if it exceeds 8192 tokens and appends `</think><answer>` to prompt the model to\n    generate a final answer.\n  + Demonstrates video-based input; for other modalities, modifications are required.\n  + Only a `transformers` version is provided. For `vLLM`, a custom implementation is needed to support this logic.\n\n+ `vllm_request_gui_agent.py`: This script demonstrates how to handle model responses and construct prompts for GUI\n  Agent use cases. It covers strategies for mobile, desktop, and web environments, and can be integrated into your\n  application framework. For detailed documentation about GUI Agent, please refer to [this file](resources/agent.md).\n\n+ For Ascend NPU Inference, Check [here](https://gitee.com/ascend/MindSpeed-MM/tree/master/examples/glm4.1v/README.md).\n\n## Model Fine-tuning\n\n[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) now supports fine-tuning of this model. Below is an example\ndataset using two images. Prepare your dataset in a `finetune.json` file like the following:\n\n```json\n[\n  {\n    \"messages\": [\n      {\n        \"content\": \"<image>Who are they?\",\n        \"role\": \"user\"\n      },\n      {\n        \"content\": \"<think>\nUser ask me to observe the image and get the answer. I Know they are Kane and Gretzka from Bayern Munich.</think>\n<answer>They're Kane and Gretzka from Bayern Munich.</answer>\",\n        \"role\": \"assistant\"\n      },\n      {\n        \"content\": \"<image>What are they doing?\",\n        \"role\": \"user\"\n      },\\\n      {\n        \"content\": \"<think>\nI need to observe what this people are doing. Oh, They are celebrating on the soccer field.</think>\n<answer>They are celebrating on the soccer field.</answer>\",\n        \"role\": \"assistant\"\n      }\n    ],\n    \"images\": [\n      \"mllm_demo_data/1.jpg\",\n      \"mllm_demo_data/2.jpg\"\n    ]\n  }\n]\n```\n\n1. Content inside `<think> ... </think>` will **not** be stored in the conversation history or used during fine-tuning.\n2. The `<image>` tag will be replaced with actual image data during preprocessing.\n\nAfter preparing the dataset, you can proceed with fine-tuning using the standard LLaMA-Factory pipeline.\n\n## Model License\n\n+ The code in this repository is released under the [Apache License 2.0](LICENSE).\n+ The models **GLM-4.1V-9B-Thinking** and **GLM-4.1V-9B-Base** are both licensed under the **MIT License**.\n\n## Citation\n\nIf you find our work helpful, please consider citing the following paper.\n\n```bibtex\n@misc{glmvteam2025glm41vthinkingversatilemultimodalreasoning,\n      title={GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning}, \n      author={GLM-V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Wenkai Li and Wei Jia and Xin Lyu and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuxuan Zhang and Zhanxiao Du and Zhenyu Hou and Zhao Xue and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2507.01006},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.01006}, \n}\n```",
      "card_hash": "15bbc10b426312b36bc6f815c36c59830f3d10d40f385595afa25eac26aa6656",
      "token_count": 3384,
      "success": true
    },
    {
      "model_id": "Qwen/Qwen2.5-Omni-3B",
      "card_text": "---\nlicense: other\nlicense_name: qwen-research\nlicense_link: LICENSE\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen2.5-Omni\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n\n## Overview \n### Introduction\nQwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. \n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png\" width=\"80%\"/>\n<p>\n\n### Key Features\n\n* **Omni and Novel Architecture**: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.\n\n* **Real-Time Voice and Video Chat**: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.\n\n* **Natural and Robust Speech Generation**: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.\n\n* **Strong Performance Across Modalities**: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.\n\n* **Excellent End-to-End Speech Instruction Following**: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png\" width=\"80%\"/>\n<p>\n\n### Performance\n\nWe conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png\" width=\"80%\"/>\n<p>\n\n<details>\n<summary>Multimodality  -> Text</summary>\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"10\">OmniBench<br>Speech | Sound Event | Music | Avg</td>\n    <td class=\"tg-0lax\">Gemini-1.5-Pro</td>\n    <td class=\"tg-0lax\">42.67%|42.26%|46.23%|42.91%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MIO-Instruct</td>\n    <td class=\"tg-0lax\">36.96%|33.58%|11.32%|33.80%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">AnyGPT (7B)</td>\n    <td class=\"tg-0lax\">17.77%|20.75%|13.21%|18.04%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">video-SALMONN</td>\n    <td class=\"tg-0lax\">34.11%|31.70%|<strong>56.60%</strong>|35.64%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xlarge</td>\n    <td class=\"tg-0lax\">39.56%|36.98%|29.25%|38.00%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xxlarge</td>\n    <td class=\"tg-0lax\">34.24%|36.98%|24.53%|33.98%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|-|40.50%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">-|-|-|42.90%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">52.14%|52.08%|52.83%|52.19%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>55.25%</strong>|<strong>60.00%</strong>|52.83%|<strong>56.13%</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n\n<details>\n<summary>Audio -> Text</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">ASR</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"12\">Librispeech<br>dev-clean | dev other | test-clean | test-other</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechVerse</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">-|-|1.8|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">-|-|-|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\">-|-|-|3.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|-|<strong>1.6</strong>|<strong>2.8</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|1.7|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|-|1.7|3.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">1.8|4.0|2.0|4.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>1.3</strong>|<strong>3.4</strong>|<strong>1.6</strong>|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">2.0|4.1|2.2|4.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">1.6|3.5|1.8|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"5\">Common Voice 15<br>en | zh | yue | fr</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">9.3|12.8|10.9|10.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">7.9|6.3|6.4|8.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">8.6|6.9|<strong>5.9</strong>|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">9.1|6.0|11.6|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>7.6</strong>|<strong>5.2</strong>|7.3|<strong>7.5</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"8\">Fleurs<br>zh | en</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">7.7|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|<strong>3.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">10.8|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.4|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">3.0|3.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">7.5|-</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">3.2|5.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>3.0</strong>|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Wenetspeech<br>test-net | test-meeting</td>\n    <td class=\"tg-0lax\">Seed-ASR-Chinese</td>\n    <td class=\"tg-0lax\"><strong>4.7|5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">-|16.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">6.9|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">6.8|7.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.3|8.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.9|7.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">Voxpopuli-V1.0-en</td>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">6.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\"><strong>5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">S2TT</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">CoVoST2<br>en-de | de-en | en-zh | zh-en</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">18.6|-|33.1|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechLLaMA</td>\n    <td class=\"tg-0lax\">-|27.1|-|12.3</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">BLSP</td>\n    <td class=\"tg-0lax\">14.1|-|-|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|<strong>48.2</strong>|27.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|<strong>39.9</strong>|46.7|26.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">25.1|33.9|41.5|15.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">29.9|35.2|45.2|24.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">28.3|38.1|41.4|26.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>30.2</strong>|37.7|41.4|<strong>29.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">SER</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Meld</td>\n    <td class=\"tg-0lax\">WavLM-large</td>\n    <td class=\"tg-0lax\">0.542</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">0.524</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.557</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">0.553</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.558</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.570</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">VSC</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">VocalSound</td>\n    <td class=\"tg-0lax\">CLAP</td>\n    <td class=\"tg-0lax\">0.495</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Pengi</td>\n    <td class=\"tg-0lax\">0.604</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.929</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.936</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Music</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">GiantSteps Tempo</td>\n    <td class=\"tg-0lax\">Llark-7B</td>\n    <td class=\"tg-0lax\">0.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">MusicCaps</td>\n    <td class=\"tg-0lax\">LP-MusicCaps</td>\n    <td class=\"tg-0lax\">0.291|0.149|0.089|<strong>0.061</strong>|0.129|0.130</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.325|<strong>0.163</strong>|<strong>0.093</strong>|0.057|<strong>0.132</strong>|<strong>0.229</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.328</strong>|0.162|0.090|0.055|0.127|0.225</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Audio Reasoning</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">MMAU<br>Sound | Music | Speech | Avg</td>\n    <td class=\"tg-0lax\">Gemini-Pro-V1.5</td>\n    <td class=\"tg-0lax\">56.75|49.40|58.55|54.90</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">54.95|50.98|42.04|49.20</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>70.27</strong>|60.48|59.16|63.30</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">67.87|<strong>69.16|59.76|65.60</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Voice Chatting</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>AlpacaEval | CommonEval | SD-QA | MMSU</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\"><strong>4.55</strong>|3.90|53.35|47.17</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">4.50|3.77|55.06|34.95</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">3.50|2.95|25.95|27.03</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">3.85|3.50|38.25|49.74</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.42|<strong>4.15</strong>|50.72|54.78</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">4.50|4.05|43.40|57.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">3.74|3.43|35.71|35.72</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">4.32|4.00|49.37|50.23</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">4.49|3.93|<strong>55.71</strong>|<strong>61.32</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>OpenBookQA | IFEval | AdvBench | Avg</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\">65.27|<strong>66.88</strong>|98.46|71.45</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">27.23|62.93|94.81|62.91</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">28.35|25.71|87.69|46.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">72.75|36.28|59.62|57.66</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">78.02|49.25|97.69|71.69</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">74.51|54.54|97.31|71.14</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">49.45|26.33|96.73|55.35</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">74.73|42.10|98.85|68.81</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>81.10</strong>|52.87|<strong>99.42</strong>|<strong>74.12</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Image -> Text</summary>\n\n| Dataset                        | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|--------------------------------|--------------|------------|------------|---------------|-------------|\n| MMMU<sub>val</sub>             | 59.2         | 53.1       | 53.9       | 58.6          | **60.0**    | \n| MMMU-Pro<sub>overall</sub>     | 36.6         | 29.7       | -          | **38.3**      | 37.6        | \n| MathVista<sub>testmini</sub>   | 67.9         | 59.4       | **71.9**   | 68.2          | 52.5        | \n| MathVision<sub>full</sub>      | 25.0         | 20.8       | 23.1       | **25.1**      | -           | \n| MMBench-V1.1-EN<sub>test</sub> | 81.8         | 77.8       | 80.5       | **82.6**      | 76.0        | \n| MMVet<sub>turbo</sub>          | 66.8         | 62.1       | **67.5**   | 67.1          | 66.9        | \n| MMStar                         | **64.0**     | 55.7       | **64.0**   | 63.9          | 54.8        | \n| MME<sub>sum</sub>              | 2340         | 2117       | **2372**   | 2347          | 2003        | \n| MuirBench                      | 59.2         | 48.0       | -          | **59.2**      | -           | \n| CRPE<sub>relation</sub>        | **76.5**     | 73.7       | -          | 76.4          | -           | \n| RealWorldQA<sub>avg</sub>      | 70.3         | 62.6       | **71.9**   | 68.5          | -           | \n| MME-RealWorld<sub>en</sub>     | **61.6**     | 55.6       | -          | 57.4          | -           | \n| MM-MT-Bench                    | 6.0          | 5.0        | -          | **6.3**       | -           | \n| AI2D                           | 83.2         | 79.5       | **85.8**   | 83.9          | -           | \n| TextVQA<sub>val</sub>          | 84.4         | 79.8       | 83.2       | **84.9**      | -           | \n| DocVQA<sub>test</sub>          | 95.2         | 93.3       | 93.5       | **95.7**      | -           | \n| ChartQA<sub>test Avg</sub>     | 85.3         | 82.8       | 84.9       | **87.3**      | -           | \n| OCRBench_V2<sub>en</sub>       | **57.8**     | 51.7       | -          | 56.3          | -           | \n\n\n| Dataset                  | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-VL-7B | Grounding DINO | Gemini 1.5 Pro | \n|--------------------------|--------------|---------------|---------------|----------------|----------------|\n| Refcoco<sub>val</sub>    | 90.5         | 88.7          | 90.0          | **90.6**       | 73.2           | \n| Refcoco<sub>textA</sub>  | **93.5**     | 91.8          | 92.5          | 93.2           | 72.9           | \n| Refcoco<sub>textB</sub>  | 86.6         | 84.0          | 85.4          | **88.2**       | 74.6           | \n| Refcoco+<sub>val</sub>   | 85.4         | 81.1          | 84.2          | **88.2**       | 62.5           | \n| Refcoco+<sub>textA</sub> | **91.0**     | 87.5          | 89.1          | 89.0           | 63.9           | \n| Refcoco+<sub>textB</sub> | **79.3**     | 73.2          | 76.9          | 75.9           | 65.0           | \n| Refcocog+<sub>val</sub>  | **87.4**     | 85.0          | 87.2          | 86.1           | 75.2           | \n| Refcocog+<sub>test</sub> | **87.9**     | 85.1          | 87.2          | 87.0           | 76.2           | \n| ODinW                    | 42.4         | 39.2          | 37.3          | **55.0**       | 36.7           | \n| PointGrounding           | 66.5         | 46.2          | **67.3**      | -              | -              | \n</details>\n\n\n<details>\n<summary>Video(without audio) -> Text</summary>\n\n| Dataset                     | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|-----------------------------|--------------|------------|------------|---------------|-------------|\n| Video-MME<sub>w/o sub</sub> | 64.3         | 62.0       | 63.9       | **65.1**      | 64.8        | \n| Video-MME<sub>w sub</sub>   | **72.4**     | 68.6       | 67.9       | 71.6          | -           | \n| MVBench                     | **70.3**     | 68.7       | 67.2       | 69.6          | -           | \n| EgoSchema<sub>test</sub>    | **68.6**     | 61.4       | 63.2       | 65.0          | -           | \n</details>\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Content Consistency</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">1.11 | 2.24 | 7.58</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>1.00</strong> | 1.94 | <strong>6.42</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">2.27 | 2.62 | 10.27</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">1.97 | 2.19 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">1.56 | <strong>1.83</strong> | 8.67</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">1.45 | 2.57 | 6.83</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">1.45 | 2.38 | 8.08</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">1.95 | 2.87 | 9.92</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">1.58 | 2.51 | 7.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">1.70 | 2.72 | 7.97</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">1.42 | 2.32 | 6.54</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Speaker Similarity</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">0.796 | 0.762 | 0.776</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>0.801</strong> | <strong>0.766</strong> | <strong>0.782</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">0.774 | 0.714 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">0.730 | 0.710 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">0.741 | 0.647 | 0.713</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">0.748 | 0.652 | 0.724</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">0.753 | 0.654 | 0.732</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">0.741 | 0.635 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">0.744 | 0.635 | 0.746</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">0.752 | 0.632 | 0.747</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">0.754 | 0.641 | 0.752</td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Text -> Text</summary>\n\n| Dataset                           | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-7B | Qwen2.5-3B | Qwen2-7B | Llama3.1-8B | Gemma2-9B | \n|-----------------------------------|-----------|------------|------------|------------|------------|-------------|-----------|\n| MMLU-Pro                          | 47.0      | 40.4       | **56.3**   | 43.7       | 44.1       | 48.3        | 52.1      | \n| MMLU-redux                        | 71.0      | 60.9       | **75.4**   | 64.4       | 67.3       | 67.2        | 72.8      | \n| LiveBench<sub>0831</sub>          | 29.6      | 22.3       | **35.9**   | 26.8       | 29.2       | 26.7        | 30.6      | \n| GPQA                              | 30.8      | 34.3       | **36.4**   | 30.3       | 34.3       | 32.8        | 32.8      | \n| MATH                              | 71.5      | 63.6       | **75.5**   | 65.9       | 52.9       | 51.9        | 44.3      | \n| GSM8K                             | 88.7      | 82.6       | **91.6**   | 86.7       | 85.7       | 84.5        | 76.7      | \n| HumanEval                         | 78.7      | 70.7       | **84.8**   |\t74.4       | 79.9       | 72.6        | 68.9      | \n| MBPP                              | 73.2      | 70.4       | **79.2**   | 72.7       | 67.2       | 69.6        | 74.9      | \n| MultiPL-E                         | 65.8      | 57.6       | **70.4**   | 60.2       | 59.1       | 50.7        | 53.4      | \n| LiveCodeBench<sub>2305-2409</sub> | 24.6      | 16.5       | **28.7**   | 19.9       | 23.9       | 8.3         | 18.9      | \n</details>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-Omni with \ud83e\udd17 Transformers. The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\npip install accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_omni'\n```\n\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-omni-utils[decord] -U\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-omni-utils -U` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### \ud83e\udd17  Transformers Usage\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\", torch_dtype=\"auto\", device_map=\"auto\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-3B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")\n\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n    \"output.wav\",\n    audio.reshape(-1).detach().cpu().numpy(),\n    samplerate=24000,\n)\n```\n\n<details>\n<summary>Minimum GPU memory requirements</summary>\n\n|Model | Precision | 15(s) Video | 30(s) Video | 60(s) Video |\n|--------------|-----------| ------------- | ------------- | ------------------ |\n| Qwen-Omni-3B | FP32      | 89.10 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-3B | BF16      | 18.38 GB      | 22.43 GB      | 28.22 GB           |\n| Qwen-Omni-7B | FP32      | 93.56 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-7B | BF16      | 31.11 GB      | 41.85 GB      | 60.19 GB           |\n\nNote: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` is test with `attn_implementation=\"flash_attention_2\"`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).\n</details>  \n\n<details>\n<summary>Video URL resource usage</summary>\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | \u2705  | \u2705   |\n| torchvision < 0.19.0  | \u274c  | \u274c   |\n| decord      | \u2705  | \u274c   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\n# Sample messages for batch inference\n\n# Conversation with video only\nconversation1 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n        ]\n    }\n]\n\n# Conversation with pure text\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"who are you?\"\n    }\n]\n\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\n```\n</details>\n\n### Usage Tips\n\n#### Prompt for audio output\nIf users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n```\n{\n    \"role\": \"system\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n    ],\n}\n```\n#### Use audio in video\nIn the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.\n```python\n# first place, in data preprocessing\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=True)\n```\n```python\n# second place, in model processor\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", \n                   padding=True, use_audio_in_video=True)\n```\n```python\n#  third place, in model inference\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=True)\n```\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter in these places must be set to the same, otherwise unexpected results will occur.\n\n#### Use audio output or not\n\nThe model supports both text and audio outputs, if users do not need audio outputs, they can call `model.disable_talker()` after init the model. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-3B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\nmodel.disable_talker()\n```\n\nIn order to obtain a flexible experience, we recommend that users can decide whether to return audio when `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs to get text responses faster.\n\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-3B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n...\ntext_ids = model.generate(**inputs, return_audio=False)\n```\n\n#### Change voice type of output audio\nQwen2.5-Omni supports the ability to change the voice of the output audio. The `\"Qwen/Qwen2.5-Omni-3B\"` checkpoint support two voice types as follow:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity.|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe.|\n\nUsers can use the `speaker` parameter of `generate` function to specify the voice type. By default, if `speaker` is not specified, the default voice type is `Chelsie`.\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\n```\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\n```\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n\n```python\nfrom transformers import Qwen2_5OmniForConditionalGeneration\n\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-3B\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n\n```BibTeX\n\n@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}\n```\n\n<br>\n\n",
      "card_hash": "9b436a82898fa8f1e8559d1aea114052e7d471c4a5319676e033cbacc08ebcc1",
      "token_count": 13633,
      "success": true
    },
    {
      "model_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "card_text": "---\nlibrary_name: transformers\nlanguage:\n- ar\n- de\n- en\n- es\n- fr\n- hi\n- id\n- it\n- pt\n- th\n- tl\n- vi\nbase_model:\n- meta-llama/Llama-4-Scout-17B-16E\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama4\nextra_gated_prompt: >-\n    **LLAMA 4 COMMUNITY LICENSE AGREEMENT**\n\n    Llama 4 Version Effective Date: April 5, 2025\n\n    \"**Agreement**\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\n    \"**Documentation**\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\n    \"**Licensee**\" or \"**you**\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity\u2019s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n    \"**Llama 4**\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\n    \"**Llama Materials**\" means, collectively, Meta\u2019s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n\n    \"**Meta**\" or \"**we**\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\u00a0\n\n    By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n    1\\. **License Rights and Redistribution**.\n\n    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta\u2019s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\u00a0\u00a0\n\n    b. Redistribution and Use.\u00a0\u00a0\n\n    i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.\n\n    ii.\u00a0If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\u00a0\n\n    iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved.\"\n\n    iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy)), which is hereby incorporated by reference into this Agreement.  \n    \u00a0\u00a0  \n    2\\. **Additional Commercial Terms**. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee\u2019s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n    3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n    4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n    5\\. **Intellectual Property**.\n\n    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta\u2019s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n\n    b. Subject to Meta\u2019s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n\n    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n\n    6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\u00a0\n\n    7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nextra_gated_heading: \"Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\"\nlicense: other\nlicense_name: llama4\n---\n\n\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. \n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n\n**Model developer**: Meta\n\n**Model Architecture:**  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. \n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Training Data </th>\n    <th>Params</th>\n    <th>Input modalities</th>\n    <th>Output modalities</th>\n    <th>Context length</th>\n    <th>Token count</th>\n    <th>Knowledge cutoff</th>\n  </tr>\n  <tr>\n    <td>Llama 4 Scout (17Bx16E) </td>\n    <td rowspan=\"2\">A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our <a href=\"https://www.facebook.com/privacy/guide/genai/\">Privacy Center</a>.\n    </td>\n    <td>17B (Activated)\n        109B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>10M</td>\n    <td>~40T</td>\n    <td>August 2024</td>\n  </tr>\n  <tr>\n    <td>Llama 4 Maverick (17Bx128E)</td>\n    <td>17B (Activated)\n        400B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>1M</td>\n    <td>~22T</td>\n    <td>August 2024</td>\n  </tr>\n</table>\n\n**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n\n**Model Release Date:** April 5, 2025\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n\n**License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go [here](https://github.com/meta-llama/llama-cookbook).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n\n**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: \n\n1\\. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n2\\. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n## How to use with transformers\n\nPlease, make sure you have transformers `v4.51.0` installed, or upgrade using `pip install -U transformers`.\n\n```python\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id,\n    attn_implementation=\"flex_attention\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": url1},\n            {\"type\": \"image\", \"url\": url2},\n            {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=256,\n)\n\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:**  Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n| Model Name | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 4 Scout | 5.0M | 700 | 1,354 | 0 |\n| Llama 4 Maverick | 2.38M | 700 | 645 | 0 |\n| Total | 7.38M | \\- | 1,999 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 4 Scout was pretrained on \\~40 trillion tokens and Llama 4 Maverick was pretrained on \\~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta\u2019s products and services. This includes publicly shared posts from Instagram and Facebook and people\u2019s interactions with Meta AI.\n\n**Data Freshness:** The pretraining data has a cutoff of August 2024\\.\n\n## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\n\n### Pre-trained models\n\n| Pre-trained models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Reasoning & Knowledge | MMLU | 5 | macro\\_avg/acc\\_char\t | 79.3 | 85.2 | 79.6 | 85.5 |\n|  | MMLU-Pro | 5 | macro\\_avg/em | 53.8 | 61.6 | 58.2 | 62.9 |\n|  | MATH | 4 | em\\_maj1@1 | 41.6 | 53.5 | 50.3 | 61.2 |\n| Code | MBPP | 3 | pass@1 | 66.4 | 74.4 | 67.8 | 77.6 |\n| Multilingual | TydiQA | 1 | average/f1 | 29.9 | 34.3 | 31.5 | 31.7 |\n| Image | ChartQA | 0 | relaxed\\_accuracy | No multimodal support |  | 83.4 | 85.3 |\n|  | DocVQA | 0 | anls |  |  | 89.4 | 91.6 |\n\n### Instruction tuned models\t\n\n| Instruction tuned models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | ----- | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.3 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Image Reasoning | MMMU | 0 | accuracy | No multimodal support |  | 69.4 | 73.4 |\n|  | MMMU Pro^ | 0 | accuracy |  |  | 52.2 | 59.6 |\n|  | MathVista | 0 | accuracy |  |  | 70.7 | 73.7 |\n| Image Understanding | ChartQA | 0 | relaxed\\_accuracy |  |  | 88.8 | 90.0 |\n|  | DocVQA (test) | 0 | anls |  |  | 94.4 | 94.4 |\n| Coding | LiveCodeBench (10/01/2024-02/01/2025) | 0 | pass@1 | 33.3 | 27.7 | 32.8 | 43.4 |\n| Reasoning & Knowledge | MMLU Pro | 0 | macro\\_avg/acc | 68.9 | 73.4 | 74.3 | 80.5 |\n|  | GPQA Diamond | 0 | accuracy | 50.5 | 49.0 | 57.2 | 69.8 |\n| Multilingual | MGSM | 0 | average/em | 91.1 | 91.6 | 90.6 | 92.3 |\n| Long context | MTOB (half book) eng-\\>kgv/kgv-\\>eng | \\- | chrF | Context window is 128K |  | 42.2/36.6 | 54.0/46.4 |\n|  | MTOB (full book) eng-\\>kgv/kgv-\\>eng | \\- | chrF |  |  | 39.7/36.3 | 50.8/46.7 |\n\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n## Quantization\n\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\n\n## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n* Provide protections for the community to help prevent the misuse of our models.\n\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta\u2019s Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model\u2019s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n\n### Model level fine tuning\n\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\n\n**Fine-tuning data**   \nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals**  \nBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4\\. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. \n\n**Tone**  \nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\n\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\n\n**System Prompts**  \nLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we\u2019ve seen that the use of a system prompt can be effective in reducing false refusals and templated or \u201cpreachy\u201d language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. \n\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\n\n| System prompt  |\n| :---- |\n| You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don\u2019t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to \u201cit\u2019s important to\u201d, \u201cit\u2019s crucial to\u201d, \u201cit\u2019s essential to\u201d, \"it's unethical to\", \"it's worth noting\u2026\", \u201cRemember\u2026\u201d  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4\\. Your knowledge cutoff date is August 2024\\. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise. |\n\n### Llama 4 system protections\n\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. \n\nWe provide the community with system level [protections](https://llama.meta.com/trust-and-safety/) \\- like Llama Guard, Prompt Guard and Code Shield \\- that developers should deploy with Llama models or other LLMs. All of our [reference implementation](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**   \nWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\n### We spend additional focus on the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. \n\n**2\\. Child Safety**  \nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model\u2019s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We\u2019ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n\n**3\\. Cyber attack enablement**  \nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Considerations and Limitations\n\nOur AI is anchored on the values of freedom of expression \\- helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\n\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, [Llama Protections](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more. \n\n",
      "card_hash": "19659e477d9e582b5cdd9a2bac1658d5e5e7a2722b2efdd4248c06398c87882f",
      "token_count": 7446,
      "success": true
    },
    {
      "model_id": "Qwen/Qwen3-Omni-30B-A3B-Instruct",
      "card_text": "---\nlicense: other\nlicense_name: apache-2.0\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen3-Omni\n\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"100%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker\u2013Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy adaptation.\n\n* **Detailed Audio Captioner**: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/overview.png\" width=\"80%\"/>\n<p>\n\n### Cookbooks for Usage Cases\n\nQwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the [QuickStart](#quickstart) guide to download the model and install the necessary inference environment dependencies, then run and experiment locally\u2014try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Cookbook</th>\n      <th>Description</th>\n      <th>Open</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"6\">Audio</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb\">Speech Recognition</a></td>\n      <td>Speech recognition, supporting multiple languages and long audio.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb\">Speech Translation</a></td>\n      <td>Speech-to-Text / Speech-to-Speech translation.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb\">Music Analysis</a></td>\n      <td>Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb\">Sound Analysis</a></td>\n      <td>Description and analysis of various sound effects and audio signals.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb\">Audio Caption</a></td>\n      <td>Audio captioning, detailed description of any audio input.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb\">Mixed Audio Analysis</a></td>\n      <td>Analysis of mixed audio content, such as speech, music, and environmental sounds.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan=\"7\">Visual</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb\">OCR</a></td>\n      <td>OCR for complex images.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb\">Object Grounding</a></td>\n      <td>Target detection and grounding.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb\">Image Question</a></td>\n      <td>Answering arbitrary questions about any image.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb\">Image Math</a></td>\n      <td>Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb\">Video Description</a></td>\n      <td>Detailed description of video content.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb\">Video Navigation</a></td>\n      <td>Generating navigation commands from first-person motion videos.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb\">Video Scene Transition</a></td>\n      <td>Analysis of scene transitions in videos.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan=\"3\">Audio-Visual</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb\">Audio Visual Question</a></td>\n      <td>Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb\">Audio Visual Interaction</a></td>\n      <td>Interactive communication with the model using audio-visual inputs, including task specification via audio.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb\">Audio Visual Dialogue</a></td>\n      <td>Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td>Agent</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb\">Audio Function Call</a></td>\n      <td>Using audio input to perform function calls, enabling agent-like behaviors.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td>Downstream Task Fine-tuning</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb\">Omni Captioner</a></td>\n      <td>Introduction and capability demonstration of <strong>Qwen3-Omni-30B-A3B-Captioner</strong>, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n  </tbody>\n</table>\n\n## QuickStart\n\n### Model Description and Download\n\nBelow is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.\n\n| Model Name                   | Description |\n|------------------------------|-------------|\n| Qwen3-Omni-30B-A3B-Instruct  | The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf). |\n| Qwen3-Omni-30B-A3B-Thinking  | The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf).|\n| Qwen3-Omni-30B-A3B-Captioner | A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's [cookbook](https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb). |\n\nDuring loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:\n\n```bash\n# Download through ModelScope (recommended for users in Mainland China)\npip install -U modelscope\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner\n\n# Download through Hugging Face\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner\n```\n\n### Transformers Usage\n\n#### Installation\n\nThe Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you **create a new Python environment** to avoid environment runtime issues.\n\n```bash\n# If you already have transformers installed, please uninstall it first, or create a new Python environment\n# pip uninstall transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n```\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\npip install qwen-omni-utils -U\n```\n\nAdditionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using [vLLM](#vllm-usage) for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [FlashAttention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention 2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\n#### Code Snippet\n\nHere is a code snippet to show you how to use Qwen3-Omni with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one short sentence.\"}\n        ],\n    },\n]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors=\"pt\", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, \n                                 speaker=\"Ethan\", \n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\nif audio is not None:\n    sf.write(\n        \"output.wav\",\n        audio.reshape(-1).detach().cpu().numpy(),\n        samplerate=24000,\n    )\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nmodel.disable_talker()\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n# Conversation with image only\nconversation1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"text\", \"text\": \"What can you see in this image? Answer in one sentence.\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you hear in this audio?\"},\n        ]\n    }\n]\n\n# Conversation with pure text and system prompt\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen-Omni.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Who are you?\"\n    }\n]\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors=\"pt\", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch inference does not support returning audio\ntext_ids, audio = model.generate(**inputs,\n                                 return_audio=False,\n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\n```\n\n</details>\n\n<details>\n<summary>Use audio output or not</summary>\n\nThe model supports both text and audio outputs. If users do not need audio outputs, they can call `model.disable_talker()` after initializing the model. This option will save about `10GB` of GPU memory, but the `return_audio` option for the `generate` function will only allow `False`.\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nmodel.disable_talker()\n```\n\nFor a more flexible experience, we recommend that users decide whether to return audio when the `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs, resulting in faster text responses.\n\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n...\ntext_ids, _ = model.generate(..., return_audio=False)```\n\n</details>\n\n<details>\n<summary>Change voice type of output audio</summary>\n\nQwen3-Omni supports changing the voice of the output audio. The `\"Qwen/Qwen3-Omni-30B-A3B-Instruct\"` checkpoint supports three voice types as follows:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |\n| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |\n\nUsers can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Ethan\")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Chelsie\")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Aiden\")\n```\n\n</details>\n\n### vLLM Usage\n\n#### Installation\n\nWe strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and **audio output inference support for the Instruct model will be released in the near future**, you can follow the commands below to install vLLM from source. Please note that we recommend you **create a new Python environment** to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#set-up-using-python-only-build-without-compilation).\n\n```bash\ngit clone -b qwen3_omni https://github.com/wangxiongts/vllm.git\ncd vllm\npip install -r requirements/build.txt\npip install -r requirements/cuda.txt\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl\nVLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation\n# If you meet an \"Undefined symbol\" error while using VLLM_USE_PRECOMPILED=1, please use \"pip install -e . -v\" to build from source.\n# Install the Transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\npip install qwen-omni-utils -U\npip install -U flash-attn --no-build-isolation\n```\n\n#### Inference\n\nYou can use the following code for vLLM inference. The `limit_mm_per_prompt` parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting `tensor_parallel_size` greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, `max_num_seqs` indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/api/vllm/index.html#vllm.LLM). Below is a simple example of how to run Qwen3-Omni with vLLM:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nif __name__ == '__main__':\n    # vLLM engine v1 not supported yet\n    os.environ['VLLM_USE_V1'] = '0'\n\n    MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n    # MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4\"}\n            ], \n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n\n    inputs = {\n        'prompt': text,\n        'multi_modal_data': {},\n        \"mm_processor_kwargs\": {\n            \"use_audio_in_video\": True,\n        },\n    }\n\n    if images is not None:\n        inputs['multi_modal_data']['image'] = images\n    if videos is not None:\n        inputs['multi_modal_data']['video'] = videos\n    if audios is not None:\n        inputs['multi_modal_data']['audio'] = audios\n\n    outputs = llm.generate([inputs], sampling_params=sampling_params)\n\n    print(outputs[0].outputs[0].text)\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nUsing vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\ndef build_input(processor, messages, use_audio_in_video):\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)\n\n    inputs = {\n        'prompt': text,\n        'multi_modal_data': {},\n        \"mm_processor_kwargs\": {\n            \"use_audio_in_video\": use_audio_in_video,\n        },\n    }\n\n    if images is not None:\n        inputs['multi_modal_data']['image'] = images\n    if videos is not None:\n        inputs['multi_modal_data']['video'] = videos\n    if audios is not None:\n        inputs['multi_modal_data']['audio'] = audios\n    \n    return inputs\n\nif __name__ == '__main__':\n    # vLLM engine v1 not supported yet\n    os.environ['VLLM_USE_V1'] = '0'\n\n    MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n    # MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    # Conversation with image only\n    conversation1 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n                {\"type\": \"text\", \"text\": \"What can you see in this image? Answer in one sentence.\"},\n            ]\n        }\n    ]\n\n    # Conversation with audio only\n    conversation2 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n                {\"type\": \"text\", \"text\": \"What can you hear in this audio?\"},\n            ]\n        }\n    ]\n\n    # Conversation with pure text and system prompt\n    conversation3 = [\n        {\n            \"role\": \"system\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"You are Qwen-Omni.\"}\n            ],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are you? Answer in one sentence.\"\n        }\n    ]\n\n    # Conversation with mixed media\n    conversation4 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n                {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav\"},\n                {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n            ],\n        }\n    ]\n    \n    USE_AUDIO_IN_VIDEO = True\n\n    # Combine messages for batch processing\n    conversations = [conversation1, conversation2, conversation3, conversation4]\n    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]\n\n    outputs = llm.generate(inputs, sampling_params=sampling_params)\n\n    result = [outputs[i].outputs[0].text for i in range(len(outputs))]\n    print(result)\n```\n\n</details>\n\n<details>\n<summary>vLLM Serve Usage</summary>\n\nvLLM serve for Qwen3-Omni currently only supports the thinker model. The `use_audio_in_video` parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:\n\n```bash\n# Qwen3-Omni-30B-A3B-Instruct for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n```\n\nThen you can use the chat API as below (via curl, for example):\n```bash\ncurl http://localhost:8901/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"}},\n        {\"type\": \"audio_url\", \"audio_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"}},\n        {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n    ]}\n    ]\n    }'\n```\n\n</details>\n\n### Usage Tips (Recommended Reading)\n\n#### Minimum GPU memory requirements\n\n| Model                        | Precision | 15s Video | 30s Video | 60s Video | 120s Video   |\n|------------------------------|-----------| --------- | --------- | --------- | --------- |\n| Qwen3-Omni-30B-A3B-Instruct  | BF16      | 78.85 GB  | 88.52 GB  | 107.74 GB | 144.81 GB |\n| Qwen3-Omni-30B-A3B-Thinking  | BF16      | 68.74 GB  | 77.79 GB  | 95.76 GB  | 131.65 GB  |\n\n**Note**: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` precision, tested with `attn_implementation=\"flash_attention_2\"`. The Instruct model includes both the **thinker** and **talker** components, whereas the Thinking model includes only the **thinker** part.\n\n#### Prompt for Audio-Visual Interaction\n\nWhen using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the **following system prompt**. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the `user_system_prompt` field in the system prompt to include character settings or other role-specific descriptions as needed.\n\n```\nuser_system_prompt = \"You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen.\"\nmessage = {\n    \"role\": \"system\",\n    \"content\": [\n          {\"type\": \"text\", \"text\": f\"{user_system_prompt} You are a virtual voice assistant with no gender or age.\\nYou are communicating with the user.\\nIn user messages, \u201cI/me/my/we/our\u201d refer to the user and \u201cyou/your\u201d refer to the assistant. In your replies, address the user as \u201cyou/your\u201d and yourself as \u201cI/me/my\u201d; never mirror the user\u2019s pronouns\u2014always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \\nYour output must consist only of the spoken content you want the user to hear. \\nDo not include any descriptions of actions, emotions, sounds, or voice changes. \\nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \\nYou must answer users' audio or text questions, do not directly describe the video content. \\nYou should communicate in the same language strictly as the user unless they request otherwise.\\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\\nKeep replies concise and conversational, as if talking face-to-face.\"}\n    ]\n}\n```\n\n#### Best Practices for the Thinking Model\n\nThe `Qwen3-Omni-30B-A3B-Thinking` model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:\n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"image\", \"image\": \"/path/to/image.png\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Analyze this audio, image, and video together.\"},\n        ], \n    }\n]\n```\n\n#### Use audio in video\n\nIn multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.\n\n```python\n# In data preprocessing\naudios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n```\n\n```python\n# For Transformers\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", \n                   padding=True, use_audio_in_video=True)\ntext_ids, audio = model.generate(..., use_audio_in_video=True)\n\n# For vLLM\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = {\n    'prompt': text,\n    'multi_modal_data': {},\n    \"mm_processor_kwargs\": {\n        \"use_audio_in_video\": True,\n    },\n}\n```\n\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter must be set consistently across these steps; otherwise, unexpected results may occur.\n\n## Evaluation\n\n### Performance of Qwen3-Omni\n\nQwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.\n\n<details>\n<summary>Text -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th colspan=\"2\" style=\"text-align: left;\"></th>\n      <th style=\"text-align: center;\">GPT-4o-0327</th>\n      <th style=\"text-align: center;\">Qwen3-235B-A22B<br>Non Thinking</th>\n      <th style=\"text-align: center;\">Qwen3-30B-A3B-Instruct-2507</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">General<br>Tasks</td>\n      <td style=\"text-align: left;\">MMLU-Redux</td>\n      <td style=\"text-align: center;\"><strong>91.3</strong></td>\n      <td style=\"text-align: center;\">89.2</td>\n      <td style=\"text-align: center;\">89.3</td>\n      <td style=\"text-align: center;\">86.6</td>\n      <td style=\"text-align: center;\">86.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">GPQA</td>\n      <td style=\"text-align: center;\">66.9</td>\n      <td style=\"text-align: center;\">62.9</td>\n      <td style=\"text-align: center;\"><strong>70.4</strong></td>\n      <td style=\"text-align: center;\">69.6</td>\n      <td style=\"text-align: center;\">69.7</td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">Reasoning</td>\n      <td style=\"text-align: left;\">AIME25</td>\n      <td style=\"text-align: center;\">26.7</td>\n      <td style=\"text-align: center;\">24.7</td>\n      <td style=\"text-align: center;\">61.3</td>\n      <td style=\"text-align: center;\">65.0</td>\n      <td style=\"text-align: center;\"><strong>65.9</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ZebraLogic</td>\n      <td style=\"text-align: center;\">52.6</td>\n      <td style=\"text-align: center;\">37.7</td>\n      <td style=\"text-align: center;\"><strong>90.0</strong></td>\n      <td style=\"text-align: center;\">76.0</td>\n      <td style=\"text-align: center;\">76.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left; vertical-align: middle;\">Code</td>\n      <td style=\"text-align: left;\">MultiPL-E</td>\n      <td style=\"text-align: center;\">82.7</td>\n      <td style=\"text-align: center;\">79.3</td>\n      <td style=\"text-align: center;\"><strong>83.8</strong></td>\n      <td style=\"text-align: center;\">81.4</td>\n      <td style=\"text-align: center;\">81.5</td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td rowspan=\"3\" style=\"text-align: left; vertical-align: middle;\">Alignment<br>Tasks</td>\n      <td style=\"text-align: left;\">IFEval</td>\n      <td style=\"text-align: center;\">83.9</td>\n      <td style=\"text-align: center;\">83.2</td>\n      <td style=\"text-align: center;\"><strong>84.7</strong></td>\n      <td style=\"text-align: center;\">81.0</td>\n      <td style=\"text-align: center;\">81.7</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">Creative Writing v3</td>\n      <td style=\"text-align: center;\">84.9</td>\n      <td style=\"text-align: center;\">80.4</td>\n      <td style=\"text-align: center;\"><strong>86.0</strong></td>\n      <td style=\"text-align: center;\">80.6</td>\n      <td style=\"text-align: center;\">81.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">WritingBench</td>\n      <td style=\"text-align: center;\">75.5</td>\n      <td style=\"text-align: center;\">77.0</td>\n      <td style=\"text-align: center;\"><strong>85.5</strong></td>\n      <td style=\"text-align: center;\">82.6</td>\n      <td style=\"text-align: center;\">83.0</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left; vertical-align: middle;\">Agent</td>\n      <td style=\"text-align: left;\">BFCL-v3</td>\n      <td style=\"text-align: center;\">66.5</td>\n      <td style=\"text-align: center;\"><strong>68.0</strong></td>\n      <td style=\"text-align: center;\">65.1</td>\n      <td style=\"text-align: center;\">64.4</td>\n      <td style=\"text-align: center;\">65.0</td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">Multilingual<br>Tasks</td>\n      <td style=\"text-align: left;\">MultiIF</td>\n      <td style=\"text-align: center;\"><strong>70.4</strong></td>\n      <td style=\"text-align: center;\">70.2</td>\n      <td style=\"text-align: center;\">67.9</td>\n      <td style=\"text-align: center;\">64.0</td>\n      <td style=\"text-align: center;\">64.7</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">PolyMATH</td>\n      <td style=\"text-align: center;\">25.5</td>\n      <td style=\"text-align: center;\">27.0</td>\n      <td style=\"text-align: center;\"><strong>43.1</strong></td>\n      <td style=\"text-align: center;\">37.9</td>\n      <td style=\"text-align: center;\">39.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th></th>\n      <th></th>\n      <th>Gemini-2.5-Flash<br>Thinking</th>\n      <th>Qwen3-235B-A22B<br>Thinking</th>\n      <th>Qwen3-30B-A3B-Thinking-2507</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\"><em>General<br>Tasks</em></td>\n      <td>MMLU-Redux</td>\n      <td>92.1</td>\n      <td><b>92.7</b></td>\n      <td>91.4</td>\n      <td>88.8</td>\n      <td>89.7</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>GPQA</td>\n      <td><b>82.8</b></td>\n      <td>71.1</td>\n      <td>73.4</td>\n      <td>73.1</td>\n      <td>73.1</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td rowspan=\"2\"><em>Reasoning</em></td>\n      <td>AIME25</td>\n      <td>72.0</td>\n      <td>81.5</td>\n      <td><b>85.0</b></td>\n      <td>73.7</td>\n      <td>74.0</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>LiveBench 20241125</td>\n      <td>74.3</td>\n      <td><b>77.1</b></td>\n      <td>76.8</td>\n      <td>71.8</td>\n      <td>70.3</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td><em>Code</em></td>\n      <td>MultiPL-E</td>\n      <td><b>84.5</b></td>\n      <td>79.9</td>\n      <td>81.3</td>\n      <td>80.6</td>\n      <td>81.0</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td rowspan=\"4\"><em>Alignment<br>Tasks</em></td>\n      <td>IFEval</td>\n      <td><b>89.8</b></td>\n      <td>83.4</td>\n      <td>88.9</td>\n      <td>85.1</td>\n      <td>85.2</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>Arena-Hard v2</td>\n      <td>56.7</td>\n      <td><b>61.5</b></td>\n      <td>56.0</td>\n      <td>55.1</td>\n      <td>57.8</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>Creative Writing v3</td>\n      <td><b>85.0</b></td>\n      <td>84.6</td>\n      <td>84.4</td>\n      <td>82.5</td>\n      <td>83.6</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>WritingBench</td>\n      <td>83.9</td>\n      <td>80.3</td>\n      <td>85.0</td>\n      <td>85.5</td>\n      <td><b>85.9</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td><em>Agent</em></td>\n      <td>BFCL-v3</td>\n      <td>68.6</td>\n      <td>70.8</td>\n      <td><b>72.4</b></td>\n      <td>63.2</td>\n      <td>64.5</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td rowspan=\"2\"><em>Multilingual<br>Tasks</em></td>\n      <td>MultiIF</td>\n      <td>74.4</td>\n      <td>71.9</td>\n      <td><b>76.4</b></td>\n      <td>72.9</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <td>PolyMATH</td>\n      <td>49.8</td>\n      <td><b>54.7</b></td>\n      <td>52.6</td>\n      <td>47.1</td>\n      <td>48.7</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Audio -> Text</summary>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n<thead>\n  <tr>\n    <th align=\"left\" style=\"padding: 8px;\"></th>\n    <th align=\"center\" style=\"padding: 8px;\">Seed-ASR</th>\n    <th align=\"center\" style=\"padding: 8px;\">Voxtral-Mini</th>\n    <th align=\"center\" style=\"padding: 8px;\">Voxtral-Small</th>\n    <th align=\"center\" style=\"padding: 8px;\">GPT-4o-Transcribe</th>\n    <th align=\"center\" style=\"padding: 8px;\">Gemini-2.5-Pro</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen2.5-Omni</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen3-Omni-30B-A3B-Instruct</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen3-Omni-Flash-Instruct</th>\n  </tr>\n</thead>\n<tbody>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>EN & ZH ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Wenetspeech<br><em>net</em> | <em>meeting</em></td>\n    <td align=\"center\" style=\"padding: 8px;\">4.66 | <strong>5.69</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">24.30 | 31.53</td>\n    <td align=\"center\" style=\"padding: 8px;\">20.33 | 26.08</td>\n    <td align=\"center\" style=\"padding: 8px;\">15.30 | 32.27</td>\n    <td align=\"center\" style=\"padding: 8px;\">14.43 | 13.47</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.91 | 7.65</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.69 | 5.89</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>4.62</strong> | 5.75</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Librispeech<br><em>clean</em> | <em>other</em></td>\n    <td align=\"center\" style=\"padding: 8px;\">1.58 | 2.84</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.88 | 4.12</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.56 | 3.30</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.39 | 3.75</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.89 | 3.56</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.74 | 3.45</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>1.22</strong> | 2.48</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.27 | <strong>2.44</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">CV15-en</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.47</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.79</td>\n    <td align=\"center\" style=\"padding: 8px;\">10.01</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.89</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.61</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.05</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.94</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">CV15-zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">24.67</td>\n    <td align=\"center\" style=\"padding: 8px;\">19.30</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.84</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.00</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.13</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.31</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>4.28</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-en</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.40</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.96</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.77</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.32</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.94</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.77</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>2.72</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">2.74</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.69</td>\n    <td align=\"center\" style=\"padding: 8px;\">12.22</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.98</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.44</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.71</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.54</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.20</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>2.19</strong></td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>Multilingual ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-avg<br>(19 lang)</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">15.67</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.09</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.48</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.55</td>\n    <td align=\"center\" style=\"padding: 8px;\">14.04</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.33</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.31</strong></td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>Lyric ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">MIR-1K (vocal-only)</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.45</td>\n    <td align=\"center\" style=\"padding: 8px;\">23.33</td>\n    <td align=\"center\" style=\"padding: 8px;\">18.73</td>\n    <td align=\"center\" style=\"padding: 8px;\">11.87</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.85</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.15</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.90</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.85</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Opencpop-test</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.98</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.01</td>\n    <td align=\"center\" style=\"padding: 8px;\">16.06</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.93</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.49</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.84</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>1.54</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">2.02</td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>S2TT (BLEU)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-en2xx</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">30.35</td>\n    <td align=\"center\" style=\"padding: 8px;\">37.85</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>39.25</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">29.22</td>\n    <td align=\"center\" style=\"padding: 8px;\">37.50</td>\n    <td align=\"center\" style=\"padding: 8px;\">36.22</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-xx2en</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">27.54</td>\n    <td align=\"center\" style=\"padding: 8px;\">32.81</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>35.41</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">28.61</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.08</td>\n    <td align=\"center\" style=\"padding: 8px;\">30.71</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-zh2xx</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">17.03</td>\n    <td align=\"center\" style=\"padding: 8px;\">22.05</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>26.63</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">17.97</td>\n    <td align=\"center\" style=\"padding: 8px;\">25.17</td>\n    <td align=\"center\" style=\"padding: 8px;\">25.10</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-xx2zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">28.75</td>\n    <td align=\"center\" style=\"padding: 8px;\">34.82</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>37.50</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">27.68</td>\n    <td align=\"center\" style=\"padding: 8px;\">33.13</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.19</td>\n  </tr>\n</tbody>\n</table>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n  <thead>\n    <tr style=\"border-bottom: 1px solid #ddd;\">\n      <th style=\"text-align:left; padding: 8px;\"></th>\n      <th style=\"text-align:center; padding: 8px;\">GPT-4o-Audio</th>\n      <th style=\"text-align:center; padding: 8px;\">Gemini-2.5-Flash</th>\n      <th style=\"text-align:center; padding: 8px;\">Gemini-2.5-Pro</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen2.5-Omni</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-Flash-Instruct</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan=\"9\" align=\"center\" style=\"padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\"><strong>VoiceBench</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">AlpacaEval</td>\n      <td style=\"text-align:center; padding: 8px;\">95.6</td>\n      <td style=\"text-align:center; padding: 8px;\">96.1</td>\n      <td style=\"text-align:center; padding: 8px;\">94.3</td>\n      <td style=\"text-align:center; padding: 8px;\">89.9</td>\n      <td style=\"text-align:center; padding: 8px;\">94.8</td>\n      <td style=\"text-align:center; padding: 8px;\">96.4</td>\n      <td style=\"text-align:center; padding: 8px;\">95.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>96.8</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">CommonEval</td>\n      <td style=\"text-align:center; padding: 8px;\">89.8</td>\n      <td style=\"text-align:center; padding: 8px;\">88.3</td>\n      <td style=\"text-align:center; padding: 8px;\">88.4</td>\n      <td style=\"text-align:center; padding: 8px;\">76.7</td>\n      <td style=\"text-align:center; padding: 8px;\">90.8</td>\n      <td style=\"text-align:center; padding: 8px;\">90.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>91.0</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">90.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">WildVoice</td>\n      <td style=\"text-align:center; padding: 8px;\">91.6</td>\n      <td style=\"text-align:center; padding: 8px;\">92.1</td>\n      <td style=\"text-align:center; padding: 8px;\">93.4</td>\n      <td style=\"text-align:center; padding: 8px;\">77.7</td>\n      <td style=\"text-align:center; padding: 8px;\">91.6</td>\n      <td style=\"text-align:center; padding: 8px;\">90.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>92.3</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">90.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">SD-QA</td>\n      <td style=\"text-align:center; padding: 8px;\">75.5</td>\n      <td style=\"text-align:center; padding: 8px;\">84.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>90.1</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">56.4</td>\n      <td style=\"text-align:center; padding: 8px;\">76.9</td>\n      <td style=\"text-align:center; padding: 8px;\">78.1</td>\n      <td style=\"text-align:center; padding: 8px;\">76.8</td>\n      <td style=\"text-align:center; padding: 8px;\">78.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">MMSU</td>\n      <td style=\"text-align:center; padding: 8px;\">80.3</td>\n      <td style=\"text-align:center; padding: 8px;\">66.1</td>\n      <td style=\"text-align:center; padding: 8px;\">71.1</td>\n      <td style=\"text-align:center; padding: 8px;\">61.7</td>\n      <td style=\"text-align:center; padding: 8px;\">68.1</td>\n      <td style=\"text-align:center; padding: 8px;\">83.0</td>\n      <td style=\"text-align:center; padding: 8px;\">68.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>84.3</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">OpenBookQA</td>\n      <td style=\"text-align:center; padding: 8px;\">89.2</td>\n      <td style=\"text-align:center; padding: 8px;\">56.9</td>\n      <td style=\"text-align:center; padding: 8px;\">92.3</td>\n      <td style=\"text-align:center; padding: 8px;\">80.9</td>\n      <td style=\"text-align:center; padding: 8px;\">89.7</td>\n      <td style=\"text-align:center; padding: 8px;\">94.3</td>\n      <td style=\"text-align:center; padding: 8px;\">91.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>95.0</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">BBH</td>\n      <td style=\"text-align:center; padding: 8px;\">84.1</td>\n      <td style=\"text-align:center; padding: 8px;\">83.9</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>92.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">66.7</td>\n      <td style=\"text-align:center; padding: 8px;\">80.4</td>\n      <td style=\"text-align:center; padding: 8px;\">88.9</td>\n      <td style=\"text-align:center; padding: 8px;\">80.6</td>\n      <td style=\"text-align:center; padding: 8px;\">89.6</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">IFEval</td>\n      <td style=\"text-align:center; padding: 8px;\">76.0</td>\n      <td style=\"text-align:center; padding: 8px;\">83.8</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>85.7</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">53.5</td>\n      <td style=\"text-align:center; padding: 8px;\">77.8</td>\n      <td style=\"text-align:center; padding: 8px;\">80.6</td>\n      <td style=\"text-align:center; padding: 8px;\">75.2</td>\n      <td style=\"text-align:center; padding: 8px;\">80.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">AdvBench</td>\n      <td style=\"text-align:center; padding: 8px;\">98.7</td>\n      <td style=\"text-align:center; padding: 8px;\">98.9</td>\n      <td style=\"text-align:center; padding: 8px;\">98.1</td>\n      <td style=\"text-align:center; padding: 8px;\">99.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>99.3</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">97.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>99.4</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">98.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">Overall</td>\n      <td style=\"text-align:center; padding: 8px;\">86.8</td>\n      <td style=\"text-align:center; padding: 8px;\">83.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>89.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">73.6</td>\n      <td style=\"text-align:center; padding: 8px;\">85.5</td>\n      <td style=\"text-align:center; padding: 8px;\">88.8</td>\n      <td style=\"text-align:center; padding: 8px;\">85.6</td>\n      <td style=\"text-align:center; padding: 8px;\">89.5</td>\n    </tr>\n    <tr>\n      <td colspan=\"9\" align=\"center\" style=\"padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\"><strong>Audio Reasoning</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">MMAU-v05.15.25</td>\n      <td style=\"text-align:center; padding: 8px;\">62.5</td>\n      <td style=\"text-align:center; padding: 8px;\">71.8</td>\n      <td style=\"text-align:center; padding: 8px;\">77.4</td>\n      <td style=\"text-align:center; padding: 8px;\">65.5</td>\n      <td style=\"text-align:center; padding: 8px;\">77.5</td>\n      <td style=\"text-align:center; padding: 8px;\">75.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>77.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">76.5</td>\n    </tr>\n    <tr\">\n      <td style=\"text-align:left; padding: 8px;\">MMSU</td>\n      <td style=\"text-align:center; padding: 8px;\">56.4</td>\n      <td style=\"text-align:center; padding: 8px;\">70.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>77.7</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">62.6</td>\n      <td style=\"text-align:center; padding: 8px;\">69.0</td>\n      <td style=\"text-align:center; padding: 8px;\">70.2</td>\n      <td style=\"text-align:center; padding: 8px;\">69.1</td>\n      <td style=\"text-align:center; padding: 8px;\">71.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th style=\"text-align: left;\"></th>\n      <th style=\"text-align: center;\">Best Specialist<br>Models</th>\n      <th style=\"text-align: center;\">GPT-4o-Audio</th>\n      <th style=\"text-align: center;\">Gemini-2.5-Pro</th>\n      <th style=\"text-align: center;\">Qwen2.5-Omni</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left;\">RUL-MuchoMusic</td>\n      <td style=\"text-align: center;\">47.6 (Audio Flamingo 3)</td>\n      <td style=\"text-align: center;\">36.1</td>\n      <td style=\"text-align: center;\">49.4</td>\n      <td style=\"text-align: center;\">47.3</td>\n      <td style=\"text-align: center;\">52.0</td>\n      <td style=\"text-align: center;\"><strong>52.1</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">GTZAN<br><em>Acc.</em></td>\n      <td style=\"text-align: center;\">87.9 (CLaMP 3)</td>\n      <td style=\"text-align: center;\">76.5</td>\n      <td style=\"text-align: center;\">81.0</td>\n      <td style=\"text-align: center;\">81.7</td>\n      <td style=\"text-align: center;\">93.0</td>\n      <td style=\"text-align: center;\"><strong>93.1</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Genre<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">35.8 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">25.3</td>\n      <td style=\"text-align: center;\">32.6</td>\n      <td style=\"text-align: center;\">32.5</td>\n      <td style=\"text-align: center;\">39.0</td>\n      <td style=\"text-align: center;\"><strong>39.5</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Mood/Theme<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">10.9 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">11.3</td>\n      <td style=\"text-align: center;\">14.1</td>\n      <td style=\"text-align: center;\">8.9</td>\n      <td style=\"text-align: center;\">21.0</td>\n      <td style=\"text-align: center;\"><strong>21.7</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Instrument<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">39.8 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">34.2</td>\n      <td style=\"text-align: center;\">33.0</td>\n      <td style=\"text-align: center;\">22.6</td>\n      <td style=\"text-align: center;\">40.5</td>\n      <td style=\"text-align: center;\"><strong>40.7</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Top50<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">33.2 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">25.0</td>\n      <td style=\"text-align: center;\">26.1</td>\n      <td style=\"text-align: center;\">21.6</td>\n      <td style=\"text-align: center;\">36.7</td>\n      <td style=\"text-align: center;\"><strong>36.9</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MagnaTagATune<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">41.6 (MuQ)</td>\n      <td style=\"text-align: center;\">29.2</td>\n      <td style=\"text-align: center;\">28.1</td>\n      <td style=\"text-align: center;\">30.1</td>\n      <td style=\"text-align: center;\">44.3</td>\n      <td style=\"text-align: center;\"><strong>46.8</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Vision -> Text</summary>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th style=\"text-align: left;\">Datasets</th>\n      <th style=\"text-align: center;\">GPT4-o</th>\n      <th style=\"text-align: center;\">Gemini-2.0-Flash</th>\n      <th style=\"text-align: center;\">Qwen2.5-VL<br>72B</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B<br>-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash<br>-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid #ddd; border-bottom: 1px solid black;\">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMStar</td>\n      <td style=\"text-align: center;\">64.7</td>\n      <td style=\"text-align: center;\"><strong>71.4</strong></td>\n      <td style=\"text-align: center;\">70.8</td>\n      <td style=\"text-align: center;\">68.5</td>\n      <td style=\"text-align: center;\">69.3</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">HallusionBench</td>\n      <td style=\"text-align: center;\">55.0</td>\n      <td style=\"text-align: center;\">56.3</td>\n      <td style=\"text-align: center;\">55.2</td>\n      <td style=\"text-align: center;\"><strong>59.7</strong></td>\n      <td style=\"text-align: center;\">58.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MM-MT-Bench</td>\n      <td style=\"text-align: center;\"><strong>7.7</strong></td>\n      <td style=\"text-align: center;\">6.7</td>\n      <td style=\"text-align: center;\">7.6</td>\n      <td style=\"text-align: center;\">7.4</td>\n      <td style=\"text-align: center;\">7.6</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMMU_val</td>\n      <td style=\"text-align: center;\">69.1</td>\n      <td style=\"text-align: center;\"><strong>71.3</strong></td>\n      <td style=\"text-align: center;\">70.2</td>\n      <td style=\"text-align: center;\">69.1</td>\n      <td style=\"text-align: center;\">69.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMMU_pro</td>\n      <td style=\"text-align: center;\">51.9</td>\n      <td style=\"text-align: center;\">56.1</td>\n      <td style=\"text-align: center;\">51.1</td>\n      <td style=\"text-align: center;\">57.0</td>\n      <td style=\"text-align: center;\"><strong>57.6</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MathVista_mini</td>\n      <td style=\"text-align: center;\">63.8</td>\n      <td style=\"text-align: center;\">71.4</td>\n      <td style=\"text-align: center;\">74.8</td>\n      <td style=\"text-align: center;\">75.9</td>\n      <td style=\"text-align: center;\"><strong>77.4</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MathVision_full</td>\n      <td style=\"text-align: center;\">30.4</td>\n      <td style=\"text-align: center;\">48.6</td>\n      <td style=\"text-align: center;\">38.1</td>\n      <td style=\"text-align: center;\">56.3</td>\n      <td style=\"text-align: center;\"><strong>58.3</strong></td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">AI2D</td>\n      <td style=\"text-align: center;\">84.6</td>\n      <td style=\"text-align: center;\">86.7</td>\n      <td style=\"text-align: center;\"><strong>88.7</strong></td>\n      <td style=\"text-align: center;\">85.2</td>\n      <td style=\"text-align: center;\">86.4</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ChartQA_test</td>\n      <td style=\"text-align: center;\">86.7</td>\n      <td style=\"text-align: center;\">64.6</td>\n      <td style=\"text-align: center;\"><strong>89.5</strong></td>\n      <td style=\"text-align: center;\">86.8</td>\n      <td style=\"text-align: center;\">87.1</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Counting</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">CountBench</td>\n      <td style=\"text-align: center;\">87.9</td>\n      <td style=\"text-align: center;\">91.2</td>\n      <td style=\"text-align: center;\"><strong>93.6</strong></td>\n      <td style=\"text-align: center;\">90.0</td>\n      <td style=\"text-align: center;\">90.0</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">Video-MME</td>\n      <td style=\"text-align: center;\">71.9</td>\n      <td style=\"text-align: center;\">72.4</td>\n      <td style=\"text-align: center;\"><strong>73.3</strong></td>\n      <td style=\"text-align: center;\">70.5</td>\n      <td style=\"text-align: center;\">71.4</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">LVBench</td>\n      <td style=\"text-align: center;\">30.8</td>\n      <td style=\"text-align: center;\"><strong>57.9</strong></td>\n      <td style=\"text-align: center;\">47.3</td>\n      <td style=\"text-align: center;\">50.2</td>\n      <td style=\"text-align: center;\">51.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MLVU</td>\n      <td style=\"text-align: center;\">64.6</td>\n      <td style=\"text-align: center;\">71.0</td>\n      <td style=\"text-align: center;\">74.6</td>\n      <td style=\"text-align: center;\">75.2</td>\n      <td style=\"text-align: center;\"><strong>75.7</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table style=\"width: 100%; border-collapse: collapse;\">\n  <thead style=\"border-bottom: 1px solid black;\">\n    <tr>\n      <th align=\"left\" style=\"padding: 6px;\">Datasets</th>\n      <th align=\"center\" style=\"padding: 6px;\">Gemini-2.5-flash-thinking</th>\n      <th align=\"center\" style=\"padding: 6px;\">InternVL-3.5-241B-A28B</th>\n      <th align=\"center\" style=\"padding: 6px;\">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th align=\"center\" style=\"padding: 6px;\">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr style=\"border-top: 2px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-bottom: 1px solid black;\">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMStar</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.5</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>77.9</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">74.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.5</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">HallusionBench</td>\n      <td align=\"center\" style=\"padding: 6px;\">61.1</td>\n      <td align=\"center\" style=\"padding: 6px;\">57.3</td>\n      <td align=\"center\" style=\"padding: 6px;\">62.8</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>63.4</b></td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MM-MT-Bench</td>\n      <td align=\"center\" style=\"padding: 6px;\">7.8</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>8.0</b></td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>8.0</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMMU_val</td>\n      <td align=\"center\" style=\"padding: 6px;\">76.9</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>77.7</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">75.6</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.0</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMMU_pro</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>65.8</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">60.5</td>\n      <td align=\"center\" style=\"padding: 6px;\">60.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MathVista_mini</td>\n      <td align=\"center\" style=\"padding: 6px;\">77.6</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>82.7</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">80.0</td>\n      <td align=\"center\" style=\"padding: 6px;\">81.2</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MathVision_full</td>\n      <td align=\"center\" style=\"padding: 6px;\">62.3</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>63.9</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">62.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">63.8</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">AI2D_test</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>88.6</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">87.3</td>\n      <td align=\"center\" style=\"padding: 6px;\">86.1</td>\n      <td align=\"center\" style=\"padding: 6px;\">86.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">ChartQA_test</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.0</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>89.5</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">89.3</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Counting</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">CountBench</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.6</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.6</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>92.5</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">Video-MME</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>79.6</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">72.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">69.7</td>\n      <td align=\"center\" style=\"padding: 6px;\">69.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">LVBench</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>64.5</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">49.0</td>\n      <td align=\"center\" style=\"padding: 6px;\">49.5</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MLVU</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>82.1</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">78.2</td>\n      <td align=\"center\" style=\"padding: 6px;\">72.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">73.9</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>AudioVisual -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash</th>\n      <th>Qwen2.5-Omni</th>\n      <th>Qwen3-Omni-30B-A3B-Instruct</th>\n      <th>Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>WorldSense</td>\n      <td>47.1</td>\n      <td>50.9</td>\n      <td>45.4</td>\n      <td>54.0</td>\n      <td><strong>54.1</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash-Thinking</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>DailyOmni</td>\n      <td>69.8</td>\n      <td>72.7</td>\n      <td>75.8</b></td>\n      <td><b>76.2</td>\n    </tr>\n    <tr>\n      <td>VideoHolmes</td>\n      <td>55.6</td>\n      <td>49.5</td>\n      <td><b>57.3</b></td>\n      <td><b>57.3</b></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th align=\"left\">Datasets</th>\n      <th align=\"left\">Model</th>\n      <th align=\"left\">Performance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>&nbsp;</td>\n      <td colspan=\"2\" align=\"center\"><em>Content Consistency</em></td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr>\n      <td rowspan=\"10\" align=\"center\" valign=\"middle\"><strong>SEED</strong><br><em>test-zh</em> | <em>test-en</em></td>\n      <td align=\"left\">Seed-TTS<sub>ICL</sub></td>\n      <td align=\"left\">1.11 | 2.24</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Seed-TTS<sub>RL</sub></td>\n      <td align=\"left\">1.00 | 1.94</td>\n    </tr>\n    <tr>\n      <td align=\"left\">MaskGCT</td>\n      <td align=\"left\">2.27 | 2.62</td>\n    </tr>\n    <tr>\n      <td align=\"left\">E2 TTS</td>\n      <td align=\"left\">1.97 | 2.19</td>\n    </tr>\n    <tr>\n      <td align=\"left\">F5-TTS</td>\n      <td align=\"left\">1.56 | 1.83</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Spark TTS</td>\n      <td align=\"left\">1.20 | 1.98</td>\n    </tr>\n    <tr>\n      <td align=\"left\">CosyVoice 2</td>\n      <td align=\"left\">1.45 | 2.57</td>\n    </tr>\n    <tr>\n      <td align=\"left\">CosyVoice 3</td>\n      <td align=\"left\"><strong>0.71</strong> | 1.45</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Qwen2.5-Omni-7B</td>\n      <td align=\"left\">1.42 | 2.33</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Qwen3-Omni-30B-A3B</td>\n      <td align=\"left\">1.07 | <strong>1.39</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Multilingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"2\" align=\"left\">Language</th>\n      <th colspan=\"3\" style=\"text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;\">Content Consistency</th>\n      <th colspan=\"3\"  style=\"text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;\">Speaker Similarity</th>\n    </tr>\n    <tr>\n      <th align=\"center\">Qwen3-Omni-30B-A3B</th>\n      <th align=\"center\">MiniMax</th>\n      <th align=\"center\">ElevenLabs</th>\n      <th align=\"center\">Qwen3-Omni-30B-A3B</th>\n      <th align=\"center\">MiniMax</th>\n      <th align=\"center\">ElevenLabs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"left\">Chinese</td>\n      <td align=\"center\"><strong>0.716</strong></td>\n      <td align=\"center\">2.252</td>\n      <td align=\"center\">16.026</td>\n      <td align=\"center\">0.772</td>\n      <td align=\"center\"><strong>0.780</strong></td>\n      <td align=\"center\">0.677</td>\n    </tr>\n    <tr>\n      <td align=\"left\">English</td>\n      <td align=\"center\"><strong>1.069</strong></td>\n      <td align=\"center\">2.164</td>\n      <td align=\"center\">2.339</td>\n      <td align=\"center\"><strong>0.773</strong></td>\n      <td align=\"center\">0.756</td>\n      <td align=\"center\">0.613</td>\n    </tr>\n    <tr>\n      <td align=\"left\">German</td>\n      <td align=\"center\">0.777</td>\n      <td align=\"center\">1.906</td>\n      <td align=\"center\"><strong>0.572</strong></td>\n      <td align=\"center\"><strong>0.738</strong></td>\n      <td align=\"center\">0.733</td>\n      <td align=\"center\">0.614</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Italian</td>\n      <td align=\"center\"><strong>1.067</strong></td>\n      <td align=\"center\">1.543</td>\n      <td align=\"center\">1.743</td>\n      <td align=\"center\"><strong>0.742</strong></td>\n      <td align=\"center\">0.699</td>\n      <td align=\"center\">0.579</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Portuguese</td>\n      <td align=\"center\">1.872</td>\n      <td align=\"center\">1.877</td>\n      <td align=\"center\"><strong>1.331</strong></td>\n      <td align=\"center\">0.770</td>\n      <td align=\"center\"><strong>0.805</strong></td>\n      <td align=\"center\">0.711</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Spanish</td>\n      <td align=\"center\">1.765</td>\n      <td align=\"center\"><strong>1.029</strong></td>\n      <td align=\"center\">1.084</td>\n      <td align=\"center\">0.744</td>\n      <td align=\"center\"><strong>0.762</strong></td>\n      <td align=\"center\">0.615</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Japanese</td>\n      <td align=\"center\">3.631</td>\n      <td align=\"center\"><strong>3.519</strong></td>\n      <td align=\"center\">10.646</td>\n      <td align=\"center\">0.763</td>\n      <td align=\"center\"><strong>0.776</strong></td>\n      <td align=\"center\">0.738</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Korean</td>\n      <td align=\"center\"><strong>1.670</strong></td>\n      <td align=\"center\">1.747</td>\n      <td align=\"center\">1.865</td>\n      <td align=\"center\"><strong>0.778</strong></td>\n      <td align=\"center\">0.776</td>\n      <td align=\"center\">0.700</td>\n    </tr>\n    <tr>\n      <td align=\"left\">French</td>\n      <td align=\"center\"><strong>2.505</strong></td>\n      <td align=\"center\">4.099</td>\n      <td align=\"center\">5.216</td>\n      <td align=\"center\"><strong>0.689</strong></td>\n      <td align=\"center\">0.628</td>\n      <td align=\"center\">0.535</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Russian</td>\n      <td align=\"center\">3.986</td>\n      <td align=\"center\">4.281</td>\n      <td align=\"center\"><strong>3.878</strong></td>\n      <td align=\"center\">0.759</td>\n      <td align=\"center\"><strong>0.761</strong></td>\n      <td align=\"center\">0.676</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Cross-Lingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left;\">Language</th>\n      <th style=\"text-align: left;\">Qwen3-Omni-30B-A3B</th>\n      <th style=\"text-align: left;\">CosyVoice3</th>\n      <th style=\"text-align: left;\">CosyVoice2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left;\">en-to-zh</td>\n      <td style=\"text-align: left;\">5.37</td>\n      <td style=\"text-align: left;\"><strong>5.09</strong></td>\n      <td style=\"text-align: left;\">13.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-zh</td>\n      <td style=\"text-align: left;\">3.32</td>\n      <td style=\"text-align: left;\"><strong>3.05</strong></td>\n      <td style=\"text-align: left;\">48.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-zh</td>\n      <td style=\"text-align: left;\"><strong>0.99</strong></td>\n      <td style=\"text-align: left;\">1.06</td>\n      <td style=\"text-align: left;\">7.70</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-en</td>\n      <td style=\"text-align: left;\"><strong>2.76</strong></td>\n      <td style=\"text-align: left;\">2.98</td>\n      <td style=\"text-align: left;\">6.47</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-en</td>\n      <td style=\"text-align: left;\"><strong>3.31</strong></td>\n      <td style=\"text-align: left;\">4.20</td>\n      <td style=\"text-align: left;\">17.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-en</td>\n      <td style=\"text-align: left;\"><strong>3.34</strong></td>\n      <td style=\"text-align: left;\">4.19</td>\n      <td style=\"text-align: left;\">11.2</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-ja</td>\n      <td style=\"text-align: left;\">8.29</td>\n      <td style=\"text-align: left;\"><strong>7.08</strong></td>\n      <td style=\"text-align: left;\">13.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">en-to-ja</td>\n      <td style=\"text-align: left;\">7.53</td>\n      <td style=\"text-align: left;\"><strong>6.80</strong></td>\n      <td style=\"text-align: left;\">14.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-ja</td>\n      <td style=\"text-align: left;\">4.24</td>\n      <td style=\"text-align: left;\"><strong>3.93</strong></td>\n      <td style=\"text-align: left;\">5.86</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-ko</td>\n      <td style=\"text-align: left;\"><strong>5.13</strong></td>\n      <td style=\"text-align: left;\">14.4</td>\n      <td style=\"text-align: left;\">24.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">en-to-ko</td>\n      <td style=\"text-align: left;\"><strong>4.96</strong></td>\n      <td style=\"text-align: left;\">5.87</td>\n      <td style=\"text-align: left;\">21.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-ko</td>\n      <td style=\"text-align: left;\"><strong>6.23</strong></td>\n      <td style=\"text-align: left;\">7.92</td>\n      <td style=\"text-align: left;\">21.5</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n### Setting for Evaluation\n\n*   **Decoding Strategy**: For the Qwen3-Omni series across all evaluation benchmarks, `Instruct` models use greedy decoding during generation without sampling. For `Thinking` models, the decoding parameters should be taken from the `generation_config.json` file in the checkpoint.\n*   **Benchmark-Specific Formatting**: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to `fps=2` during evaluation.\n*   **Default Prompts**: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:\n\n| Task Type | Prompt |\n| :--- | :--- |\n| Auto Speech Recognition (ASR) for Chinese | \u8bf7\u5c06\u8fd9\u6bb5\u4e2d\u6587\u8bed\u97f3\u8f6c\u6362\u4e3a\u7eaf\u6587\u672c\u3002 |\n| Auto Speech Recognition (ASR) for Other languages | Transcribe the <language> audio into text. |\n| Speech-to-Text Translation (S2TT) | Listen to the provided <source_language> speech and produce a translation in <target_language> text. |\n| Song Lyrics Recognition | Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations. |\n\n*   **System Prompt**: No `system prompt` should be set for any evaluation benchmark.\n*   **Input Sequence**: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come **after** multimodal data in the sequence. For example:\n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"image\", \"image\": \"/path/to/image.png\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Describe the audio, image and video.\"},\n        ],\n    },\n]\n```\n\n\n<!-- ## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n```BibTeX\n@article{Qwen3-Omni,\n  title={Qwen3-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin},\n  journal={arXiv preprint arXiv},\n  year={2025}\n}\n``` -->\n\n<br>\n",
      "card_hash": "0e44065c4c4a27071f7239afd5b5a33af5bc2e437dd7ea9950e51aafabfde3df",
      "token_count": 28055,
      "success": true
    },
    {
      "model_id": "zai-org/GLM-4.1V-9B-Thinking",
      "card_text": "---\nlicense: mit\nlanguage:\n- en\n- zh\nbase_model:\n- zai-org/GLM-4-9B-0414\npipeline_tag: image-text-to-text\nlibrary_name: transformers\ntags:\n- reasoning\n---\n\n# GLM-4.1V-9B-Thinking\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width=\"40%\"/>\n</div>\n<p align=\"center\">\n    \ud83d\udcd6 View the GLM-4.1V-9B-Thinking <a href=\"https://arxiv.org/abs/2507.01006\" target=\"_blank\">paper</a>.\n    <br>\n    \ud83d\udccd Using GLM-4.1V-9B-Thinking API at <a href=\"https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking\">Zhipu Foundation Model Open Platform</a>\n</p>\n\n\n## Model Introduction\n\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\n\nBased on the [GLM-4-9B-0414](https://github.com/zai-org/GLM-4) foundation model, we present the new open-source VLM model\n**GLM-4.1V-9B-Thinking**, designed to explore the upper limits of reasoning in vision-language models. By introducing\na \"thinking paradigm\" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\n\n![rl](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/rl.jpeg)\n\nCompared to the previous generation models CogVLM2 and the GLM-4V series, **GLM-4.1V-Thinking** offers the\nfollowing improvements:\n\n1. The first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\n   across various sub-domains.\n2. Supports **64k** context length.\n3. Handles **arbitrary aspect ratios** and up to **4K** image resolution.\n4. Provides an open-source version supporting both **Chinese and English bilingual** usage.\n\n## Benchmark Performance\n\nBy incorporating the Chain-of-Thought reasoning paradigm, GLM-4.1V-9B-Thinking significantly improves answer accuracy,\nrichness, and interpretability. It comprehensively surpasses traditional non-reasoning visual models.\nOut of 28 benchmark tasks, it achieved the best performance among 10B-level models on 23 tasks,\nand even outperformed the 72B-parameter Qwen-2.5-VL-72B on 18 tasks.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/bench.jpeg)\n\n## Quick Inference\n\nThis is a simple example of running single-image inference using the `transformers` library.  \nFirst, install the `transformers` library from source:\n\n```\npip install transformers>=4.57.1\n```\n\nThen, run the following code:\n\n```python\nfrom transformers import AutoProcessor, Glm4vForConditionalGeneration\nimport torch\n\nMODEL_PATH = \"zai-org/GLM-4.1V-9B-Thinking\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"describe this image\"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\nmodel = Glm4vForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\nFor video reasoning, web demo deployment, and more code, please check\nour [GitHub](https://github.com/zai-org/GLM-V).",
      "card_hash": "963a958a8bd2d1a00c6aee00db1e545b23d2ad8a98dc94521f29cead70fb68b1",
      "token_count": 1100,
      "success": true
    },
    {
      "model_id": "google/gemma-3n-E2B-it",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B-it\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n---\n\n> [!Note]\n> This repository corresponds to the launch version of Gemma 3n E2B IT (Instruct), to be used with Hugging Face `transformers`,\n> supporting text, audio, and vision (image and video) inputs.\n> \n> Gemma 3n models have multiple architecture innovations:\n>  * They are available in two sizes based on [effective parameters](https://ai.google.dev/gemma/docs/gemma-3n#parameters). While the raw parameter count of this model is 6B, the architecture design allows the model to be run with a memory footprint comparable to a traditional 2B model by offloading low-utilization matrices from the accelerator.\n>  * They use a MatFormer architecture that allows nesting sub-models within the [E4B model](https://huggingface.co/google/gemma-3n-E4B-it). We provide one sub-model (this model repository), or you can access a spectrum of custom-sized models using the [Mix-and-Match method](https://goo.gle/gemma3n-matformer-lab).\n>\n> Learn more about these techniques in the [technical blog post](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide)\n> and the [Gemma documentation](https://ai.google.dev/gemma/docs/gemma-3n). \n\n\n\n# Gemma 3n model card\n\n**Model Page**: [Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n)\n\n**Resources and Technical Documentation**:\n\n-   [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n-   [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma-3n)\n-   [Gemma on HuggingFace](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)\n-   [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n)\n\n**Terms of Use**: [Terms](https://ai.google.dev/gemma/terms)\\\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for pre-trained and\ninstruction-tuned variants. These models were trained with data in over 140\nspoken languages.\n\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the\n[Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n#parameters)\npage.\n\n### Inputs and outputs\n\n-   **Input:**\n    -   Text string, such as a question, a prompt, or a document to be\n        summarized\n    -   Images, normalized to 256x256, 512x512, or 768x768 resolution\n        and encoded to 256 tokens each\n    -   Audio data encoded to 6.25 tokens per second from a single channel\n    -   Total input context of 32K tokens\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output length up to 32K tokens, subtracting the request\n        input tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running\nthe model. First, install the Transformers library. Gemma 3n is supported\nstarting from transformers 4.53.0.\n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as\nfollows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3n-e2b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16,\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our\ninputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single GPU\n\n```python\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3n-e2b-it\"\n\nmodel = Gemma3nForConditionalGeneration.from_pretrained(model_id, device=\"cuda\", torch_dtype=torch.bfloat16,).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```\n@article{gemma_3n_2025,\n    title={Gemma 3n},\n    url={https://ai.google.dev/gemma/docs/gemma-3n},\n    publisher={Google DeepMind},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\n\n-   **Web Documents**: A diverse collection of web text ensures the model\n    is exposed to a broad range of linguistic styles, topics, and vocabulary.\n    The training dataset includes content in over 140 languages.\n-   **Code**: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   **Mathematics**: Training on mathematical text helps the model learn\n    logical reasoning, symbolic representation, and to address mathematical queries.\n-   **Images**: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n-   Audio: A diverse set of sound samples enables the model to recognize\n    speech, transcribe text from recordings, and identify information in audio data.\n\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   **CSAM Filtering**: Rigorous CSAM (Child Sexual Abuse Material)\n    filtering was applied at multiple stages in the data preparation process to\n    ensure the exclusion of harmful and illegal content.\n-   **Sensitive Data Filtering**: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   **Additional methods**: Filtering based on content quality and safety in\n    line with\n    [our policies](https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit\n(TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\n\n-   **Performance**: TPUs are specifically designed to handle the massive\n    computations involved in training generative models. They can speed up\n    training considerably compared to CPUs.\n-   **Memory**: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   **Scalability**: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   **Cost-effectiveness**: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n\nThese advantages are aligned with\n[Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/jax-ml/jax) and\n[ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/).\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://goo.gle/gemma2report):\n*\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with **IT** are for\ninstruction-tuned models. Evaluation results marked with **PT** are for\npre-trained models.\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | n-shot   |  E2B PT  |  E4B PT  |\n| ------------------------------ |----------------|----------|:--------:|:--------:|\n| [HellaSwag][hellaswag]         | Accuracy       | 10-shot  |   72.2   |   78.6   |\n| [BoolQ][boolq]                 | Accuracy       | 0-shot   |   76.4   |   81.6   |\n| [PIQA][piqa]                   | Accuracy       | 0-shot   |   78.9   |   81.0   |\n| [SocialIQA][socialiqa]         | Accuracy       | 0-shot   |   48.8   |   50.0   |\n| [TriviaQA][triviaqa]           | Accuracy       | 5-shot   |   60.8   |   70.2   |\n| [Natural Questions][naturalq]  | Accuracy       | 5-shot   |   15.5   |   20.9   |\n| [ARC-c][arc]                   | Accuracy       | 25-shot  |   51.7   |   61.6   |\n| [ARC-e][arc]                   | Accuracy       | 0-shot   |   75.8   |   81.6   |\n| [WinoGrande][winogrande]       | Accuracy       | 5-shot   |   66.8   |   71.7   |\n| [BIG-Bench Hard][bbh]          | Accuracy       | few-shot |   44.3   |   52.9   |\n| [DROP][drop]                   | Token F1 score | 1-shot   |   53.9   |   60.8   |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### Multilingual\n\n| Benchmark                           | Metric                  | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------|-------------------------|----------|:--------:|:--------:|\n| [MGSM][mgsm]                        | Accuracy                |  0-shot  |   53.1   |   60.7   |\n| [WMT24++][wmt24pp] (ChrF)           | Character-level F-score |  0-shot  |   42.7   |   50.1   |\n| [Include][include]                  | Accuracy                |  0-shot  |   38.6   |   57.2   |\n| [MMLU][mmlu] (ProX)                 | Accuracy                |  0-shot  |    8.1   |   19.9   |\n| [OpenAI MMLU][openai-mmlu]          | Accuracy                |  0-shot  |   22.3   |   35.6   |\n| [Global-MMLU][global-mmlu]          | Accuracy                |  0-shot  |   55.1   |   60.3   |\n| [ECLeKTic][eclektic]                | ECLeKTic score          |  0-shot  |    2.5   |    1.9   |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[include]:https://arxiv.org/abs/2411.19799\n[mmlu]: https://arxiv.org/abs/2009.03300\n[openai-mmlu]: https://huggingface.co/datasets/openai/MMMLU\n[global-mmlu]: https://huggingface.co/datasets/CohereLabs/Global-MMLU\n[eclektic]: https://arxiv.org/abs/2502.21228\n\n#### STEM and code\n\n| Benchmark                           | Metric                   | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------|--------------------------|----------|:--------:|:--------:|\n| [GPQA][gpqa] Diamond                | RelaxedAccuracy/accuracy |  0-shot  |   24.8   |   23.7   |\n| [LiveCodeBench][lcb] v5             | pass@1                   |  0-shot  |   18.6   |   25.7   |\n| Codegolf v2.2                       | pass@1                   |  0-shot  |   11.0   |   16.8   |\n| [AIME 2025][aime-2025]              | Accuracy                 |  0-shot  |    6.7   |   11.6   |\n\n[gpqa]: https://arxiv.org/abs/2311.12022\n[lcb]: https://arxiv.org/abs/2403.07974\n[aime-2025]: https://www.vals.ai/benchmarks/aime-2025-05-09\n\n#### Additional benchmarks\n\n| Benchmark                            | Metric     | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------ |------------|----------|:--------:|:--------:|\n| [MMLU][mmlu]                         |  Accuracy  |  0-shot  |   60.1   |   64.9   |\n| [MBPP][mbpp]                         |  pass@1    |  3-shot  |   56.6   |   63.6   |\n| [HumanEval][humaneval]               |  pass@1    |  0-shot  |   66.5   |   75.0   |\n| [LiveCodeBench][lcb]                 |  pass@1    |  0-shot  |   13.2   |   13.2   |\n| HiddenMath                           |  Accuracy  |  0-shot  |   27.7   |   37.7   |\n| [Global-MMLU-Lite][global-mmlu-lite] |  Accuracy  |  0-shot  |   59.0   |   64.5   |\n| [MMLU][mmlu] (Pro)                   |  Accuracy  |  0-shot  |   40.5   |   50.6   |\n\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n[lcb]: https://arxiv.org/abs/2403.07974\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   **Text Generation**: Generate creative text formats such as\n        poems, scripts, code, marketing copy, and email drafts.\n    -   **Chatbots and Conversational AI**: Power conversational\n        interfaces for customer service, virtual assistants, or interactive\n        applications.\n    -   **Text Summarization**: Generate concise summaries of a text\n        corpus, research papers, or reports.\n    -   **Image Data Extraction**: Extract, interpret, and summarize\n        visual data for text communications.\n    -   **Audio Data Extraction**: Transcribe spoken language, translate speech\n        to text in other languages, and analyze sound-based data.\n-   Research and Education\n    -   **Natural Language Processing (NLP) and generative model\n        Research**: These models can serve as a foundation for researchers to\n        experiment with generative models and NLP techniques, develop\n        algorithms, and contribute to the advancement of the field.\n    -   **Language Learning Tools**: Support interactive language\n        learning experiences, aiding in grammar correction or providing writing\n        practice.\n    -   **Knowledge Exploration**: Assist researchers in exploring large\n        bodies of data by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   Generative models trained on large-scale, real-world text and image data\n        can reflect socio-cultural biases embedded in the training material.\n        These models underwent careful scrutiny, input data pre-processing\n        described and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   Generative models can be misused to generate text that is\n        false, misleading, or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit](https://ai.google.dev/responsible).\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making generative model technology accessible to\n        developers and researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous monitoring\n    (using evaluation metrics, human review) and the exploration of de-biasing\n    techniques during model training, fine-tuning, and other use cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    generative models. Educational resources and reporting mechanisms for users\n    to flag misuse are provided. Prohibited uses of Gemma models are outlined\n    in the\n    [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n-   **Privacy violations**: Models were trained on data filtered for removal of\n    certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n",
      "card_hash": "1e162c26e2a7d8f3d6d96dd18fb1b80a8196677b975b891b4bd146a43522ebdd",
      "token_count": 5943,
      "success": true
    },
    {
      "model_id": "abhishekchohan/gemma-3-12b-it-quantized-W4A16",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you're required to review and agree to Google's usage license. To do this, please ensure you're logged in to Hugging Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3-12b-it\n---\n\n# Gemma 3 Quantized Models\n\nThis repository contains W4A16 quantized versions of Google's Gemma 3 instruction-tuned models, making them more accessible for deployment on consumer hardware while maintaining good performance.\n\n## Models\n\n- **abhishekchohan/gemma-3-27b-it-quantized-W4A16**\n- **abhishekchohan/gemma-3-12b-it-quantized-W4A16**\n- **abhishekchohan/gemma-3-4b-it-quantized-W4A16**\n\n## Repository Structure\n\n```\ngemma-3-{size}-it-quantized-W4A16/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 templates/\n\u2502   \u2514\u2500\u2500 chat_template.jinja\n\u251c\u2500\u2500 tools/\n\u2502   \u2514\u2500\u2500 tool_parser.py\n\u2514\u2500\u2500 [model files]\n```\n\n## Quantization Details\n\nThese models use W4A16 quantization via LLM Compressor:\n- Weights quantized to 4-bit precision\n- Activations use 16-bit precision\n- Significantly reduced memory requirements\n\n## Usage with vLLM\n\n```bash\nvllm serve abhishekchohan/gemma-3-{size}-it-quantized-W4A16 --chat-template templates/chat_template.jinja --enable-auto-tool-choice --tool-call-parser gemma --tool-parser-plugin tools/tool_parser.py\n```\n\n## License\n\nThese models are subject to the Gemma license. Users must acknowledge and accept the license terms before using the models.\n\n## Citation\n\n```\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n",
      "card_hash": "048d8bf59272a30469059e4ee9bc9553f73327a9a185332ac173dfcf78998aa6",
      "token_count": 484,
      "success": true
    },
    {
      "model_id": "zai-org/GLM-4.6V-Flash",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\n---\n\n# GLM-4.6V\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/logo.svg width=\"40%\"/>\n</div>\n\nThis model is part of the GLM-V family of models, introduced in the paper [GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://huggingface.co/papers/2507.01006).\n\n-   **GLM-4.6V Blog**: [https://z.ai/blog/glm-4.6v](https://z.ai/blog/glm-4.6v)\n-   **Paper**: [https://huggingface.co/papers/2507.01006](https://huggingface.co/papers/2507.01006)\n-   **GitHub Repository**: [https://github.com/zai-org/GLM-V](https://github.com/zai-org/GLM-V)\n-   **Online Demo**: [https://chat.z.ai/](https://chat.z.ai/)\n-   **API Access**: [Z.ai Open Platform](https://docs.z.ai/guides/vlm/glm-4.6v)\n-   **Desktop Assistant App**: [https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App](https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App)\n\n## Introduction\n\nGLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance\ncluster scenarios,\nand GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications.\nGLM-4.6V scales its context window to 128k tokens in training,\nand achieves SoTA performance in visual understanding among models of similar parameter scales.\nCrucially, we integrate native Function Calling capabilities for the first time.\nThis effectively bridges the gap between \"visual perception\" and \"executable action\"\nproviding a unified technical foundation for multimodal agents in real-world business scenarios.\n\n![GLM-4.6V Benchmarks](https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/bench_46v.jpeg)\n\nBeyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces\nseveral key features:\n\n- **Native Multimodal Function Calling** \nEnables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.\n\n- **Interleaved Image-Text Content Generation**\nSupports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context\u2014spanning documents, user inputs, and tool-retrieved images\u2014and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.\n\n\n- **Multimodal Document Understanding**\nGLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.\n    \n- **Frontend Replication & Visual Editing** \nReconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.\n\n\n**This Hugging Face repository hosts the `GLM-4.6V-Flash` model, part of the `GLM-V` series.**\n\n## Usage\n\n### Environment Installation\n\nFor `SGLang`:\n\n```bash\npip install sglang>=0.5.6.post1\npip install nvidia-cudnn-cu12==9.16.0.29\nsudo apt update\nsudo apt install ffmpeg\n```\n\nFor `vLLM`:\n\n```bash\npip install vllm>=0.12.0\npip install transformers>=5.0.0rc0\n```\n\n### Quick Start with Transformers\n\n```python\nfrom transformers import AutoProcessor, Glm4vForConditionalGeneration\nimport torch\n\nMODEL_PATH = \"zai-org/GLM-4.6V-Flash\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"describe this image\"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nmodel = Glm4vForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device)\ninputs.pop(\"token_type_ids\", None)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\n## Evaluation Settings\n\nWe primarily use vLLM as the backend for model inference. For faster and more reliable performance on video tasks, we employ SGLang. To reproduce our leaderboard results, we recommend the following decoding parameters:\n\n+\ttop_p: 0.6\n+\ttop_k: 2\n+\ttemperature: 0.8\n+\trepetition_penalty: 1.1\n+\tmax_generate_tokens: 16K\n\nFor more usage details, please refer to Our [Github](https://github.com/zai-org/GLM-V).\n\n\n\n## Fixed and Remaining Issues\n\nSince the open-sourcing of GLM-4.1V, we have received extensive feedback from the community and are well aware that the model still has many shortcomings. In subsequent iterations, we attempted to address several common issues \u2014 such as repetitive thinking outputs and formatting errors \u2014 which have been mitigated to some extent in this new version.\n\nHowever, the model still has several limitations and issues that we will fix as soon as possible:\n\n1. Pure text QA capabilities still have significant room for improvement. In this development cycle, our primary focus was on visual multimodal scenarios, and we will enhance pure text abilities in upcoming updates.\n2. The model may still overthink or even repeat itself in certain cases, especially when dealing with complex prompts.\n3. In some situations, the model may restate the answer again at the end.\n4. There remain certain perception limitations, such as counting accuracy and identifying specific individuals, which still require improvement.\n\nThank you for your patience and understanding. We also welcome feedback and suggestions in the issue section \u2014 we will respond and improve as much as we can!\n\n## Citation\n\nIf you use this model, please cite the following paper:\n\n```bibtex\n@misc{vteam2025glm45vglm41vthinkingversatilemultimodal,\n      title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning}, \n      author={V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2507.01006},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.01006}, \n}\n```\n",
      "card_hash": "84849e79dce9bd27688af744cd10bf7ffb14f92dda0c31dfedaad510b802db2f",
      "token_count": 2091,
      "success": true
    },
    {
      "model_id": "HuggingFaceTB/SmolVLM2-256M-Video-Instruct",
      "card_text": "---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- HuggingFaceM4/the_cauldron\n- HuggingFaceM4/Docmatix\n- lmms-lab/LLaVA-OneVision-Data\n- lmms-lab/M4-Instruct-Data\n- HuggingFaceFV/finevideo\n- MAmmoTH-VL/MAmmoTH-VL-Instruct-12M\n- lmms-lab/LLaVA-Video-178K\n- orrzohar/Video-STaR\n- Mutonix/Vript\n- TIGER-Lab/VISTA-400K\n- Enxin/MovieChat-1K_train\n- ShareGPT4Video/ShareGPT4Video\npipeline_tag: image-text-to-text\nlanguage:\n- en\nbase_model:\n- HuggingFaceTB/SmolVLM-256M-Instruct\n---\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM2_banner.png\" width=\"800\" height=\"auto\" alt=\"Image description\">\n\n# SmolVLM2-256M-Video\n\nSmolVLM2-256M-Video is a lightweight multimodal model designed to analyze video content. The model processes videos, images, and text inputs to generate text outputs - whether answering questions about media files, comparing visual content, or transcribing text from images. Despite its compact size, requiring only 1.38GB of GPU RAM for video inference. This efficiency makes it particularly well-suited for on-device applications that require specific domain fine-tuning and computational resources may be limited.\n## Model Summary\n\n- **Developed by:** Hugging Face \ud83e\udd17\n- **Model type:** Multi-modal model (image/multi-image/video/text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n\n## Resources\n\n- **Demo:** [Video Highlight Generator](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM2-HighlightGenerator)\n- **Blog:** [Blog post](https://huggingface.co/blog/smolvlm2)\n\n## Uses\n\nSmolVLM2 can be used for inference on multimodal (video / image / text) tasks where the input consists of text queries along with video or one or more images. Text and media files can be interleaved arbitrarily, enabling tasks like captioning, visual question answering, and storytelling based on visual content. The model does not support image or video generation.\n\nTo fine-tune SmolVLM2 on a specific task, you can follow [the fine-tuning tutorial](https://github.com/huggingface/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb).\n\n## Evaluation \n\nWe evaluated the performance of the SmolVLM2 family on the following scientific benchmarks:\n\n| Size    | Video-MME | MLVU | MVBench |\n|----------|-----------------|----------|---------------|\n| 2.2B   | 52.1            | 55.2     | 46.27        |\n| 500M | 42.2            | 47.3     | 39.73        |\n| 256M | 33.7            | 40.6     | 32.7          |\n\n\n### How to get started\n\nYou can use transformers to load, infer and fine-tune SmolVLM. Make sure you have num2words, flash-attn and latest transformers installed.\nYou can load the model as follows.\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_path = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\"\n).to(\"cuda\")\n```\n\n#### Simple Inference\n\nYou preprocess your inputs directly using chat templates and directly passing them \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Can you describe this image?\"},            \n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n#### Video Inference\n\nTo use SmolVLM2 for video inference, make sure you have decord installed. \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"path\": \"path_to_video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Describe this video in detail\"}\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])\n```\n#### Multi-image Interleaved Inference\n\nYou can interleave multiple media with text using chat templates.\n\n```python\nimport torch\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n          {\"type\": \"text\", \"text\": \"What is the similarity between these two images?\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"},            \n        ]\n    },\n]\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n\n### Model optimizations\n\n## Misuse and Out-of-scope Use\n\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\n\n- Prohibited Uses:\n  - Evaluating or scoring individuals (e.g., in employment, education, credit)\n  - Critical automated decision-making\n  - Generating unreliable factual content\n- Malicious Activities:\n  - Spam generation\n  - Disinformation campaigns\n  - Harassment or abuse\n  - Unauthorized surveillance\n\n### License\n\nSmolVLM2 is built upon [SigLIP](https://huggingface.co/google/siglip-base-patch16-512) as image encoder and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) for text decoder part.\n\nWe release the SmolVLM2 checkpoints under the Apache 2.0 license.\n\n## Citation information\nYou can cite us in the following way:\n```bibtex\n@article{marafioti2025smolvlm,\n  title={SmolVLM: Redefining small and efficient multimodal models}, \n  author={Andr\u00e9s Marafioti and Orr Zohar and Miquel Farr\u00e9 and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\n  journal={arXiv preprint arXiv:2504.05299},\n  year={2025}\n}\n```\n\n## Training Data\nSmolVLM2 used 3.3M samples for training originally from ten different datasets: [LlaVa Onevision](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data), [M4-Instruct](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data), [Mammoth](https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M), [LlaVa Video 178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K), [FineVideo](https://huggingface.co/datasets/HuggingFaceFV/finevideo), [VideoStar](https://huggingface.co/datasets/orrzohar/Video-STaR), [VRipt](https://huggingface.co/datasets/Mutonix/Vript), [Vista-400K](https://huggingface.co/datasets/TIGER-Lab/VISTA-400K), [MovieChat](https://huggingface.co/datasets/Enxin/MovieChat-1K_train) and [ShareGPT4Video](https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video).\nIn the following plots we give a general overview of the samples across modalities and the source of those samples.\n<!--\n<center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_data_split.png\" width=\"auto\" height=\"auto\" alt=\"Image description\">\n</center>\n\n### Details\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_datadetails.png\" width=\"auto\" height=\"auto\" alt=\"Image description\"> -->\n\n## Data Split per modality\n\n| Data Type    | Percentage |\n|--------------|------------|\n| Image        | 34.4%      |\n| Text         | 20.2%      |\n| Video        | 33.0%      |\n| Multi-image  | 12.3%      |\n\n\n## Granular dataset slices per modality\n\n### Text Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/magpie_pro_ft3_80b_mt      | 6.8%       |\n| llava-onevision/magpie_pro_ft3_80b_tt      | 6.8%       |\n| llava-onevision/magpie_pro_qwen2_72b_tt    | 5.8%       |\n| llava-onevision/mathqa                     | 0.9%       |\n\n### Multi-image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| m4-instruct-data/m4_instruct_multiimage    | 10.4%      |\n| mammoth/multiimage-cap6                    | 1.9%       |\n\n### Image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/other                      | 17.4%      |\n| llava-onevision/vision_flan                | 3.9%       |\n| llava-onevision/mavis_math_metagen         | 2.6%       |\n| llava-onevision/mavis_math_rule_geo        | 2.5%       |\n| llava-onevision/sharegpt4o                 | 1.7%       |\n| llava-onevision/sharegpt4v_coco            | 1.5%       |\n| llava-onevision/image_textualization       | 1.3%       |\n| llava-onevision/sharegpt4v_llava           | 0.9%       |\n| llava-onevision/mapqa                      | 0.9%       |\n| llava-onevision/qa                         | 0.8%       |\n| llava-onevision/textocr                    | 0.8%       |\n\n### Video Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-video-178k/1-2m                      | 7.3%       |\n| llava-video-178k/2-3m                      | 7.0%       |\n| other-video/combined                       | 5.7%       |\n| llava-video-178k/hound                     | 4.4%       |\n| llava-video-178k/0-30s                     | 2.4%       |\n| video-star/starb                           | 2.2%       |\n| vista-400k/combined                        | 2.2%       |\n| vript/long                                 | 1.0%       |\n| ShareGPT4Video/all                         | 0.8%       |\n",
      "card_hash": "25f0d36cb69a8f7185f945288236d61e519a206800e89a5ae64111924fdb6952",
      "token_count": 2905,
      "success": true
    },
    {
      "model_id": "lmstudio-community/GLM-4.6V-Flash-MLX-4bit",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\ntags:\n- mlx\nbase_model: zai-org/GLM-4.6V-Flash\n---\n## \ud83d\udcab Community Model> GLM-4.6V-Flash by zai-org\n\n_\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)_.\n\n**Model creator**: [zai-org](https://huggingface.co/zai-org)<br>\n**Original model**: [GLM-4.6V-Flash](https://huggingface.co/zai-org/GLM-4.6V-Flash)<br>\n**MLX quantization**: provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n4-bit quantized version of GLM-4.6V-Flash using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model. You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models. LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "85ef2e9b03d082fb7be44a8cd4343c47c1ad56dc23f156ee8dbb8d5a6268a3e7",
      "token_count": 522,
      "success": true
    },
    {
      "model_id": "lmstudio-community/GLM-4.6V-Flash-MLX-8bit",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\ntags:\n- mlx\nbase_model: zai-org/GLM-4.6V-Flash\n---\n## \ud83d\udcab Community Model> GLM-4.6V-Flash by zai-org\n\n_\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)_.\n\n**Model creator**: [zai-org](https://huggingface.co/zai-org)<br>\n**Original model**: [GLM-4.6V-Flash](https://huggingface.co/zai-org/GLM-4.6V-Flash)<br>\n**MLX quantization**: provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n8-bit quantized version of GLM-4.6V-Flash using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model. You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models. LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "466d36d262b83a0611dbd41755beaa2c0c246f6861fe0e84682a233a0d7465ec",
      "token_count": 522,
      "success": true
    },
    {
      "model_id": "Qwen/Qwen2.5-Omni-7B",
      "card_text": "---\nlicense: other\nlicense_name: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Omni-7B/blob/main/LICENSE\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen2.5-Omni\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n\n## Overview \n### Introduction\nQwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. \n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png\" width=\"80%\"/>\n<p>\n\n### Key Features\n\n* **Omni and Novel Architecture**: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.\n\n* **Real-Time Voice and Video Chat**: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.\n\n* **Natural and Robust Speech Generation**: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.\n\n* **Strong Performance Across Modalities**: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.\n\n* **Excellent End-to-End Speech Instruction Following**: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png\" width=\"80%\"/>\n<p>\n\n### Performance\n\nWe conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png\" width=\"80%\"/>\n<p>\n\n<details>\n<summary>Multimodality  -> Text</summary>\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"10\">OmniBench<br>Speech | Sound Event | Music | Avg</td>\n    <td class=\"tg-0lax\">Gemini-1.5-Pro</td>\n    <td class=\"tg-0lax\">42.67%|42.26%|46.23%|42.91%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MIO-Instruct</td>\n    <td class=\"tg-0lax\">36.96%|33.58%|11.32%|33.80%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">AnyGPT (7B)</td>\n    <td class=\"tg-0lax\">17.77%|20.75%|13.21%|18.04%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">video-SALMONN</td>\n    <td class=\"tg-0lax\">34.11%|31.70%|<strong>56.60%</strong>|35.64%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xlarge</td>\n    <td class=\"tg-0lax\">39.56%|36.98%|29.25%|38.00%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">UnifiedIO2-xxlarge</td>\n    <td class=\"tg-0lax\">34.24%|36.98%|24.53%|33.98%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|-|40.50%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">-|-|-|42.90%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">52.14%|52.08%|52.83%|52.19%</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>55.25%</strong>|<strong>60.00%</strong>|52.83%|<strong>56.13%</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n\n<details>\n<summary>Audio -> Text</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">ASR</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"12\">Librispeech<br>dev-clean | dev other | test-clean | test-other</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechVerse</td>\n    <td class=\"tg-0lax\">-|-|2.1|4.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">-|-|1.8|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">-|-|-|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\">-|-|-|3.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|-|<strong>1.6</strong>|<strong>2.8</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|1.7|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|-|1.7|3.9</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">1.8|4.0|2.0|4.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>1.3</strong>|<strong>3.4</strong>|<strong>1.6</strong>|3.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">2.0|4.1|2.2|4.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">1.6|3.5|1.8|3.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"5\">Common Voice 15<br>en | zh | yue | fr</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">9.3|12.8|10.9|10.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">7.9|6.3|6.4|8.5</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">8.6|6.9|<strong>5.9</strong>|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">9.1|6.0|11.6|9.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>7.6</strong>|<strong>5.2</strong>|7.3|<strong>7.5</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"8\">Fleurs<br>zh | en</td>\n    <td class=\"tg-0lax\">Whisper-large-v3</td>\n    <td class=\"tg-0lax\">7.7|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-ASR-Multilingual</td>\n    <td class=\"tg-0lax\">-|<strong>3.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">10.8|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.4|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">3.0|3.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">7.5|-</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">3.2|5.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>3.0</strong>|4.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Wenetspeech<br>test-net | test-meeting</td>\n    <td class=\"tg-0lax\">Seed-ASR-Chinese</td>\n    <td class=\"tg-0lax\"><strong>4.7|5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">-|16.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">6.9|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">6.8|7.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.3|8.1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.9|7.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">Voxpopuli-V1.0-en</td>\n    <td class=\"tg-0lax\">Llama-3-8B</td>\n    <td class=\"tg-0lax\">6.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Llama-3-70B</td>\n    <td class=\"tg-0lax\"><strong>5.7</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">6.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">5.8</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">S2TT</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">CoVoST2<br>en-de | de-en | en-zh | zh-en</td>\n    <td class=\"tg-0lax\">SALMONN</td>\n    <td class=\"tg-0lax\">18.6|-|33.1|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">SpeechLLaMA</td>\n    <td class=\"tg-0lax\">-|27.1|-|12.3</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">BLSP</td>\n    <td class=\"tg-0lax\">14.1|-|-|-</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">-|-|<strong>48.2</strong>|27.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MinMo</td>\n    <td class=\"tg-0lax\">-|<strong>39.9</strong>|46.7|26.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">25.1|33.9|41.5|15.7</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">29.9|35.2|45.2|24.4</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">28.3|38.1|41.4|26.6</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>30.2</strong>|37.7|41.4|<strong>29.4</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">SER</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">Meld</td>\n    <td class=\"tg-0lax\">WavLM-large</td>\n    <td class=\"tg-0lax\">0.542</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">0.524</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.557</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">0.553</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.558</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.570</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">VSC</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"6\">VocalSound</td>\n    <td class=\"tg-0lax\">CLAP</td>\n    <td class=\"tg-0lax\">0.495</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Pengi</td>\n    <td class=\"tg-0lax\">0.604</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen-Audio</td>\n    <td class=\"tg-0lax\">0.929</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.936</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.939</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Music</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">GiantSteps Tempo</td>\n    <td class=\"tg-0lax\">Llark-7B</td>\n    <td class=\"tg-0lax\">0.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.88</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"3\">MusicCaps</td>\n    <td class=\"tg-0lax\">LP-MusicCaps</td>\n    <td class=\"tg-0lax\">0.291|0.149|0.089|<strong>0.061</strong>|0.129|0.130</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">0.325|<strong>0.163</strong>|<strong>0.093</strong>|0.057|<strong>0.132</strong>|<strong>0.229</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>0.328</strong>|0.162|0.090|0.055|0.127|0.225</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Audio Reasoning</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"4\">MMAU<br>Sound | Music | Speech | Avg</td>\n    <td class=\"tg-0lax\">Gemini-Pro-V1.5</td>\n    <td class=\"tg-0lax\">56.75|49.40|58.55|54.90</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">54.95|50.98|42.04|49.20</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\"><strong>70.27</strong>|60.48|59.16|63.30</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">67.87|<strong>69.16|59.76|65.60</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Voice Chatting</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>AlpacaEval | CommonEval | SD-QA | MMSU</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\"><strong>4.55</strong>|3.90|53.35|47.17</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">4.50|3.77|55.06|34.95</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">3.50|2.95|25.95|27.03</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">3.85|3.50|38.25|49.74</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">4.42|<strong>4.15</strong>|50.72|54.78</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">4.50|4.05|43.40|57.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">3.74|3.43|35.71|35.72</td>\n  </tr>\n    <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">4.32|4.00|49.37|50.23</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\">4.49|3.93|<strong>55.71</strong>|<strong>61.32</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"9\">VoiceBench<br>OpenBookQA | IFEval | AdvBench | Avg</td>\n    <td class=\"tg-0lax\">Ultravox-v0.4.1-LLaMA-3.1-8B</td>\n    <td class=\"tg-0lax\">65.27|<strong>66.88</strong>|98.46|71.45</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MERaLiON</td>\n    <td class=\"tg-0lax\">27.23|62.93|94.81|62.91</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Megrez-3B-Omni</td>\n    <td class=\"tg-0lax\">28.35|25.71|87.69|46.25</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Lyra-Base</td>\n    <td class=\"tg-0lax\">72.75|36.28|59.62|57.66</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MiniCPM-o</td>\n    <td class=\"tg-0lax\">78.02|49.25|97.69|71.69</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Baichuan-Omni-1.5</td>\n    <td class=\"tg-0lax\">74.51|54.54|97.31|71.14</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2-Audio</td>\n    <td class=\"tg-0lax\">49.45|26.33|96.73|55.35</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B</td>\n    <td class=\"tg-0lax\">74.73|42.10|98.85|68.81</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B</td>\n    <td class=\"tg-0lax\"><strong>81.10</strong>|52.87|<strong>99.42</strong>|<strong>74.12</strong></td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Image -> Text</summary>\n\n| Dataset                        | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|--------------------------------|--------------|------------|------------|---------------|-------------|\n| MMMU<sub>val</sub>             | 59.2         | 53.1       | 53.9       | 58.6          | **60.0**    | \n| MMMU-Pro<sub>overall</sub>     | 36.6         | 29.7       | -          | **38.3**      | 37.6        | \n| MathVista<sub>testmini</sub>   | 67.9         | 59.4       | **71.9**   | 68.2          | 52.5        | \n| MathVision<sub>full</sub>      | 25.0         | 20.8       | 23.1       | **25.1**      | -           | \n| MMBench-V1.1-EN<sub>test</sub> | 81.8         | 77.8       | 80.5       | **82.6**      | 76.0        | \n| MMVet<sub>turbo</sub>          | 66.8         | 62.1       | **67.5**   | 67.1          | 66.9        | \n| MMStar                         | **64.0**     | 55.7       | **64.0**   | 63.9          | 54.8        | \n| MME<sub>sum</sub>              | 2340         | 2117       | **2372**   | 2347          | 2003        | \n| MuirBench                      | 59.2         | 48.0       | -          | **59.2**      | -           | \n| CRPE<sub>relation</sub>        | **76.5**     | 73.7       | -          | 76.4          | -           | \n| RealWorldQA<sub>avg</sub>      | 70.3         | 62.6       | **71.9**   | 68.5          | -           | \n| MME-RealWorld<sub>en</sub>     | **61.6**     | 55.6       | -          | 57.4          | -           | \n| MM-MT-Bench                    | 6.0          | 5.0        | -          | **6.3**       | -           | \n| AI2D                           | 83.2         | 79.5       | **85.8**   | 83.9          | -           | \n| TextVQA<sub>val</sub>          | 84.4         | 79.8       | 83.2       | **84.9**      | -           | \n| DocVQA<sub>test</sub>          | 95.2         | 93.3       | 93.5       | **95.7**      | -           | \n| ChartQA<sub>test Avg</sub>     | 85.3         | 82.8       | 84.9       | **87.3**      | -           | \n| OCRBench_V2<sub>en</sub>       | **57.8**     | 51.7       | -          | 56.3          | -           | \n\n\n| Dataset                  | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-VL-7B | Grounding DINO | Gemini 1.5 Pro | \n|--------------------------|--------------|---------------|---------------|----------------|----------------|\n| Refcoco<sub>val</sub>    | 90.5         | 88.7          | 90.0          | **90.6**       | 73.2           | \n| Refcoco<sub>textA</sub>  | **93.5**     | 91.8          | 92.5          | 93.2           | 72.9           | \n| Refcoco<sub>textB</sub>  | 86.6         | 84.0          | 85.4          | **88.2**       | 74.6           | \n| Refcoco+<sub>val</sub>   | 85.4         | 81.1          | 84.2          | **88.2**       | 62.5           | \n| Refcoco+<sub>textA</sub> | **91.0**     | 87.5          | 89.1          | 89.0           | 63.9           | \n| Refcoco+<sub>textB</sub> | **79.3**     | 73.2          | 76.9          | 75.9           | 65.0           | \n| Refcocog+<sub>val</sub>  | **87.4**     | 85.0          | 87.2          | 86.1           | 75.2           | \n| Refcocog+<sub>test</sub> | **87.9**     | 85.1          | 87.2          | 87.0           | 76.2           | \n| ODinW                    | 42.4         | 39.2          | 37.3          | **55.0**       | 36.7           | \n| PointGrounding           | 66.5         | 46.2          | **67.3**      | -              | -              | \n</details>\n\n\n<details>\n<summary>Video(without audio) -> Text</summary>\n\n| Dataset                     | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | \n|-----------------------------|--------------|------------|------------|---------------|-------------|\n| Video-MME<sub>w/o sub</sub> | 64.3         | 62.0       | 63.9       | **65.1**      | 64.8        | \n| Video-MME<sub>w sub</sub>   | **72.4**     | 68.6       | 67.9       | 71.6          | -           | \n| MVBench                     | **70.3**     | 68.7       | 67.2       | 69.6          | -           | \n| EgoSchema<sub>test</sub>    | **68.6**     | 61.4       | 63.2       | 65.0          | -           | \n</details>\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-0lax\">Datasets</th>\n    <th class=\"tg-0lax\">Model</th>\n    <th class=\"tg-0lax\">Performance</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Content Consistency</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">1.11 | 2.24 | 7.58</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>1.00</strong> | 1.94 | <strong>6.42</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">2.27 | 2.62 | 10.27</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">1.97 | 2.19 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">1.56 | <strong>1.83</strong> | 8.67</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">1.45 | 2.57 | 6.83</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">1.45 | 2.38 | 8.08</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">1.95 | 2.87 | 9.92</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">1.58 | 2.51 | 7.86</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">1.70 | 2.72 | 7.97</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">1.42 | 2.32 | 6.54</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9j4x\" colspan=\"3\">Speaker Similarity</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"11\">SEED<br>test-zh | test-en | test-hard </td>\n    <td class=\"tg-0lax\">Seed-TTS_ICL</td>\n    <td class=\"tg-0lax\">0.796 | 0.762 | 0.776</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Seed-TTS_RL</td>\n    <td class=\"tg-0lax\"><strong>0.801</strong> | <strong>0.766</strong> | <strong>0.782</strong></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">MaskGCT</td>\n    <td class=\"tg-0lax\">0.774 | 0.714 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">E2_TTS</td>\n    <td class=\"tg-0lax\">0.730 | 0.710 | -</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">F5-TTS</td>\n    <td class=\"tg-0lax\">0.741 | 0.647 | 0.713</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2</td>\n    <td class=\"tg-0lax\">0.748 | 0.652 | 0.724</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">CosyVoice 2-S</td>\n    <td class=\"tg-0lax\">0.753 | 0.654 | 0.732</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_ICL</td>\n    <td class=\"tg-0lax\">0.741 | 0.635 | 0.748</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-3B_RL</td>\n    <td class=\"tg-0lax\">0.744 | 0.635 | 0.746</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_ICL</td>\n    <td class=\"tg-0lax\">0.752 | 0.632 | 0.747</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Qwen2.5-Omni-7B_RL</td>\n    <td class=\"tg-0lax\">0.754 | 0.641 | 0.752</td>\n  </tr>\n</tbody></table>\n</details>\n\n<details>\n<summary>Text -> Text</summary>\n\n| Dataset                           | Qwen2.5-Omni-7B | Qwen2.5-Omni-3B | Qwen2.5-7B | Qwen2.5-3B | Qwen2-7B | Llama3.1-8B | Gemma2-9B | \n|-----------------------------------|-----------|------------|------------|------------|------------|-------------|-----------|\n| MMLU-Pro                          | 47.0      | 40.4       | **56.3**   | 43.7       | 44.1       | 48.3        | 52.1      | \n| MMLU-redux                        | 71.0      | 60.9       | **75.4**   | 64.4       | 67.3       | 67.2        | 72.8      | \n| LiveBench<sub>0831</sub>          | 29.6      | 22.3       | **35.9**   | 26.8       | 29.2       | 26.7        | 30.6      | \n| GPQA                              | 30.8      | 34.3       | **36.4**   | 30.3       | 34.3       | 32.8        | 32.8      | \n| MATH                              | 71.5      | 63.6       | **75.5**   | 65.9       | 52.9       | 51.9        | 44.3      | \n| GSM8K                             | 88.7      | 82.6       | **91.6**   | 86.7       | 85.7       | 84.5        | 76.7      | \n| HumanEval                         | 78.7      | 70.7       | **84.8**   |\t74.4       | 79.9       | 72.6        | 68.9      | \n| MBPP                              | 73.2      | 70.4       | **79.2**   | 72.7       | 67.2       | 69.6        | 74.9      | \n| MultiPL-E                         | 65.8      | 57.6       | **70.4**   | 60.2       | 59.1       | 50.7        | 53.4      | \n| LiveCodeBench<sub>2305-2409</sub> | 24.6      | 16.5       | **28.7**   | 19.9       | 23.9       | 8.3         | 18.9      | \n</details>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-Omni with \ud83e\udd17 Transformers. The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip uninstall transformers\npip install git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\npip install accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_omni'\n```\n\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-omni-utils[decord] -U\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-omni-utils -U` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### \ud83e\udd17  Transformers Usage\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\nfrom qwen_omni_utils import process_mm_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", torch_dtype=\"auto\", device_map=\"auto\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-Omni-7B\",\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\",\n#     attn_implementation=\"flash_attention_2\",\n# )\n\nprocessor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n\nconversation = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4\"},\n        ],\n    },\n]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\nsf.write(\n    \"output.wav\",\n    audio.reshape(-1).detach().cpu().numpy(),\n    samplerate=24000,\n)\n```\n\n<details>\n<summary>Minimum GPU memory requirements</summary>\n\n|Model | Precision | 15(s) Video | 30(s) Video | 60(s) Video |\n|--------------|-----------| ------------- | ------------- | ------------------ |\n| Qwen-Omni-3B | FP32      | 89.10 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-3B | BF16      | 18.38 GB      | 22.43 GB      | 28.22 GB           |\n| Qwen-Omni-7B | FP32      | 93.56 GB      | Not Recommend | Not Recommend      |\n| Qwen-Omni-7B | BF16      | 31.11 GB      | 41.85 GB      | 60.19 GB           |\n\nNote: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` is test with `attn_implementation=\"flash_attention_2\"`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).\n</details>  \n\n<details>\n<summary>Video URL resource usage</summary>\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | \u2705  | \u2705   |\n| torchvision < 0.19.0  | \u274c  | \u274c   |\n| decord      | \u2705  | \u274c   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\n# Sample messages for batch inference\n\n# Conversation with video only\nconversation1 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n        ]\n    }\n]\n\n# Conversation with pure text\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"who are you?\"\n    }\n]\n\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"/path/to/image.jpg\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"text\", \"text\": \"What are the elements can you see and hear in these medias?\"},\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# set use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch Inference\ntext_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)\ntext = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(text)\n```\n</details>\n\n### Usage Tips\n\n#### Prompt for audio output\nIf users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n```\n{\n    \"role\": \"system\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n    ],\n}\n```\n#### Use audio in video\nIn the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.\n```python\n# first place, in data preprocessing\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=True)\n```\n```python\n# second place, in model processor\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", \n                   padding=True, use_audio_in_video=True)\n```\n```python\n#  third place, in model inference\ntext_ids, audio = model.generate(**inputs, use_audio_in_video=True)\n```\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter in these places must be set to the same, otherwise unexpected results will occur.\n\n#### Use audio output or not\n\nThe model supports both text and audio outputs, if users do not need audio outputs, they can call `model.disable_talker()` after init the model. This option will save about `~2GB` of GPU memory but the `return_audio` option for `generate` function will only allow to be set at `False`.\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\nmodel.disable_talker()\n```\n\nIn order to obtain a flexible experience, we recommend that users can decide whether to return audio when `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs to get text responses faster.\n\n```python\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n...\ntext_ids = model.generate(**inputs, return_audio=False)\n```\n\n#### Change voice type of output audio\nQwen2.5-Omni supports the ability to change the voice of the output audio. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types as follow:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity.|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe.|\n\nUsers can use the `speaker` parameter of `generate` function to specify the voice type. By default, if `speaker` is not specified, the default voice type is `Chelsie`.\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Chelsie\")\n```\n\n```python\ntext_ids, audio = model.generate(**inputs, speaker=\"Ethan\")\n```\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using FlashAttention-2, add `attn_implementation=\"flash_attention_2\"` when loading the model:\n\n```python\nfrom transformers import Qwen2_5OmniForConditionalGeneration\n\nmodel = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-Omni-7B\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n\n```BibTeX\n\n@article{Qwen2.5-Omni,\n  title={Qwen2.5-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},\n  journal={arXiv preprint arXiv:2503.20215},\n  year={2025}\n}\n```\n\n<br>\n",
      "card_hash": "56f828c01cf5a3073a3968883c4dcbfef715af67b9295b66eb4d4941eef1680e",
      "token_count": 13655,
      "success": true
    },
    {
      "model_id": "lmstudio-community/GLM-4.6V-Flash-MLX-6bit",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\ntags:\n- mlx\nbase_model: zai-org/GLM-4.6V-Flash\n---\n## \ud83d\udcab Community Model> GLM-4.6V-Flash by zai-org\n\n_\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)_.\n\n**Model creator**: [zai-org](https://huggingface.co/zai-org)<br>\n**Original model**: [GLM-4.6V-Flash](https://huggingface.co/zai-org/GLM-4.6V-Flash)<br>\n**MLX quantization**: provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n6-bit quantized version of GLM-4.6V-Flash using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model. You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models. LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "df5ce430c45c4e798916192903be5118313d0865bec53dd8757626844a101b18",
      "token_count": 522,
      "success": true
    },
    {
      "model_id": "cyankiwi/GLM-4.6V-AWQ-4bit",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\nbase_model: zai-org/GLM-4.6V\n---\n\n# GLM-4.6V AWQ - INT4\n\n## Model Details\n\n### Quantization Details\n\n- **Quantization Method:** cyankiwi AWQ v1.0\n- **Bits:** 4\n- **Group Size:** 32\n- **Calibration Dataset:** [5CD-AI/LLaVA-CoT-o1-Instruct](https://huggingface.co/datasets/5CD-AI/LLaVA-CoT-o1-Instruct)\n- **Quantization Tool:** [llm-compressor](https://github.com/vllm-project/llm-compressor)\n\n### Memory Usage\n\n| **Type** | **GLM-4.6V** | **GLM-4.6V-AWQ-4bit** |\n|:---------------:|:----------------:|:----------------:|\n| **Memory Size** | 200.6 GB | 60.6 GB | \n| **KV Cache per Token** | 61.3 kB | 15.3 kB | \n| **KV Cache per Context** | 7.7 GB | 1.9 GB | \n\n## Inference\n\n### Prerequisite\n\n```bash\npip install vllm>=0.12.0\npip install --upgrade git+https://github.com/huggingface/transformers.git\n```\n\n### Basic Usage\n\n```bash\nvllm serve cyankiwi/GLM-4.6V-AWQ-4bit\n```\n\n## Additional Information\n\n### Known Issues\n\n- `tensor-parallel-size > 2` requires `--enable-expert-parallel`\n\n### Changelog\n\n- **v1.0.0** - Initial quantized release\n\n### Authors\n\n- **Name:** Ton Cao\n- **Contacts:** ton@cyan.kiwi\n\n# GLM-4.6V\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/logo.svg width=\"40%\"/>\n</div>\n\nThis model is part of the GLM-V family of models, introduced in the paper [GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://huggingface.co/papers/2507.01006).\n\n-   **GLM-4.6V Blog**: [https://z.ai/blog/glm-4.6v](https://z.ai/blog/glm-4.6v)\n-   **Paper**: [https://huggingface.co/papers/2507.01006](https://huggingface.co/papers/2507.01006)\n-   **GitHub Repository**: [https://github.com/zai-org/GLM-V](https://github.com/zai-org/GLM-V)\n-   **Online Demo**: [https://chat.z.ai/](https://chat.z.ai/)\n-   **API Access**: [Z.ai Open Platform](https://docs.z.ai/guides/vlm/glm-4.6v)\n-   **Desktop Assistant App**: [https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App](https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App)\n\n## Introduction\n\nGLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance\ncluster scenarios,\nand GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications.\nGLM-4.6V scales its context window to 128k tokens in training,\nand achieves SoTA performance in visual understanding among models of similar parameter scales.\nCrucially, we integrate native Function Calling capabilities for the first time.\nThis effectively bridges the gap between \"visual perception\" and \"executable action\"\nproviding a unified technical foundation for multimodal agents in real-world business scenarios.\n\n![GLM-4.6V Benchmarks](https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/bench_46v.jpeg)\n\nBeyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces\nseveral key features:\n\n- **Native Multimodal Function Calling** \nEnables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.\n\n- **Interleaved Image-Text Content Generation**\nSupports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context\u2014spanning documents, user inputs, and tool-retrieved images\u2014and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.\n\n\n- **Multimodal Document Understanding**\nGLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.\n    \n- **Frontend Replication & Visual Editing** \nReconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.\n\n\n**This Hugging Face repository hosts the `GLM-4.6V` model, part of the `GLM-V` series.**\n\n## Usage\n\n### Environment Installation\n\nFor `SGLang`:\n\n```bash\npip install sglang>=0.5.6post1\npip install transformers>=5.0.0rc0\n```\n\nFor `vLLM`:\n\n```bash\npip install vllm>=0.12.0\npip install transformers>=5.0.0rc0\n```\n\n### Quick Start with Transformers\n\n```python\nfrom transformers import AutoProcessor, Glm4vMoeForConditionalGeneration\nimport torch\n\nMODEL_PATH = \"zai-org/GLM-4.6V\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"describe this image\"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nmodel = Glm4vMoeForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device)\ninputs.pop(\"token_type_ids\", None)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\n## Evaluation Settings\n\nWe primarily use vLLM as the backend for model inference. For faster and more reliable performance on video tasks, we employ SGLang. To reproduce our leaderboard results, we recommend the following decoding parameters:\n\n+\ttop_p: 0.6\n+\ttop_k: 2\n+\ttemperature: 0.8\n+\trepetition_penalty: 1.1\n+\tmax_generate_tokens: 16K\n\nFor more usage details, please refer to Our [Github](https://github.com/zai-org/GLM-V).\n\n## Fixed and Remaining Issues\n\nSince the open-sourcing of GLM-4.1V, we have received extensive feedback from the community and are well aware that the model still has many shortcomings. In subsequent iterations, we attempted to address several common issues \u2014 such as repetitive thinking outputs and formatting errors \u2014 which have been mitigated to some extent in this new version.\n\nHowever, the model still has several limitations and issues that we will fix as soon as possible:\n\n1. Pure text QA capabilities still have significant room for improvement. In this development cycle, our primary focus was on visual multimodal scenarios, and we will enhance pure text abilities in upcoming updates.\n2. The model may still overthink or even repeat itself in certain cases, especially when dealing with complex prompts.\n3. In some situations, the model may restate the answer again at the end.\n4. There remain certain perception limitations, such as counting accuracy and identifying specific individuals, which still require improvement.\n\nThank you for your patience and understanding. We also welcome feedback and suggestions in the issue section \u2014 we will respond and improve as much as we can!\n\n## Citation\n\nIf you use this model, please cite the following paper:\n\n```bibtex\n@misc{vteam2025glm45vglm41vthinkingversatilemultimodal,\n      title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning}, \n      author={V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2507.01006},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.01006}, \n}\n```\n",
      "card_hash": "7b60db18de5f4c98e99e3dcc9f94261d8cd3f3abfefc67b16ee206f345fa2bd8",
      "token_count": 2467,
      "success": true
    },
    {
      "model_id": "HuggingFaceTB/SmolVLM2-500M-Video-Instruct",
      "card_text": "---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- HuggingFaceM4/the_cauldron\n- HuggingFaceM4/Docmatix\n- lmms-lab/LLaVA-OneVision-Data\n- lmms-lab/M4-Instruct-Data\n- HuggingFaceFV/finevideo\n- MAmmoTH-VL/MAmmoTH-VL-Instruct-12M\n- lmms-lab/LLaVA-Video-178K\n- orrzohar/Video-STaR\n- Mutonix/Vript\n- TIGER-Lab/VISTA-400K\n- Enxin/MovieChat-1K_train\n- ShareGPT4Video/ShareGPT4Video\npipeline_tag: image-text-to-text\nlanguage:\n- en\nbase_model:\n- HuggingFaceTB/SmolVLM-500M-Instruct\n---\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM2_banner.png\" width=\"800\" height=\"auto\" alt=\"Image description\">\n\n# SmolVLM2-500M-Video\n\nSmolVLM2-500M-Video is a lightweight multimodal model designed to analyze video content. The model processes videos, images, and text inputs to generate text outputs - whether answering questions about media files, comparing visual content, or transcribing text from images. Despite its compact size, requiring only 1.8GB of GPU RAM for video inference, it delivers robust performance on complex multimodal tasks. This efficiency makes it particularly well-suited for on-device applications where computational resources may be limited.\n## Model Summary\n\n- **Developed by:** Hugging Face \ud83e\udd17\n- **Model type:** Multi-modal model (image/multi-image/video/text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n\n## Resources\n\n- **Demo:** [Video Highlight Generator](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM2-HighlightGenerator)\n- **Blog:** [Blog post](https://huggingface.co/blog/smolvlm2)\n\n## Uses\n\nSmolVLM2 can be used for inference on multimodal (video / image / text) tasks where the input consists of text queries along with video or one or more images. Text and media files can be interleaved arbitrarily, enabling tasks like captioning, visual question answering, and storytelling based on visual content. The model does not support image or video generation.\n\nTo fine-tune SmolVLM2 on a specific task, you can follow [the fine-tuning tutorial](https://github.com/huggingface/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb).\n\n## Evaluation \n\nWe evaluated the performance of the SmolVLM2 family on the following scientific benchmarks:\n\n| Size    | Video-MME | MLVU | MVBench |\n|----------|-----------------|----------|---------------|\n| 2.2B   | 52.1            | 55.2     | 46.27        |\n| 500M | 42.2            | 47.3     | 39.73        |\n| 256M | 33.7            | 40.6     | 32.7          |\n\n\n### How to get started\n\nYou can use transformers to load, infer and fine-tune SmolVLM. Make sure you have num2words, flash-attn and latest transformers installed.\nYou can load the model as follows.\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_path = \"HuggingFaceTB/SmolVLM2-500M-Video-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\"\n).to(\"cuda\")\n```\n\n#### Simple Inference\n\nYou preprocess your inputs directly using chat templates and directly passing them \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Can you describe this image?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n#### Video Inference\n\nTo use SmolVLM2 for video inference, make sure you have decord installed. \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"path\": \"path_to_video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Describe this video in detail\"}\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])\n```\n#### Multi-image Interleaved Inference\n\nYou can interleave multiple media with text using chat templates.\n\n```python\nimport torch\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n          {\"type\": \"text\", \"text\": \"What is the similarity between these two images?\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"},            \n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n\n### Model optimizations\n\n## Misuse and Out-of-scope Use\n\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\n\n- Prohibited Uses:\n  - Evaluating or scoring individuals (e.g., in employment, education, credit)\n  - Critical automated decision-making\n  - Generating unreliable factual content\n- Malicious Activities:\n  - Spam generation\n  - Disinformation campaigns\n  - Harassment or abuse\n  - Unauthorized surveillance\n\n### License\n\nSmolVLM2 is built upon [SigLIP](https://huggingface.co/google/siglip-base-patch16-512) as image encoder and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) for text decoder part.\n\nWe release the SmolVLM2 checkpoints under the Apache 2.0 license.\n\n## Citation information\nYou can cite us in the following way:\n```bibtex\n@article{marafioti2025smolvlm,\n  title={SmolVLM: Redefining small and efficient multimodal models}, \n  author={Andr\u00e9s Marafioti and Orr Zohar and Miquel Farr\u00e9 and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\n  journal={arXiv preprint arXiv:2504.05299},\n  year={2025}\n}\n```\n\n## Training Data\nSmolVLM2 used 3.3M samples for training originally from ten different datasets: [LlaVa Onevision](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data), [M4-Instruct](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data), [Mammoth](https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M), [LlaVa Video 178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K), [FineVideo](https://huggingface.co/datasets/HuggingFaceFV/finevideo), [VideoStar](https://huggingface.co/datasets/orrzohar/Video-STaR), [VRipt](https://huggingface.co/datasets/Mutonix/Vript), [Vista-400K](https://huggingface.co/datasets/TIGER-Lab/VISTA-400K), [MovieChat](https://huggingface.co/datasets/Enxin/MovieChat-1K_train) and [ShareGPT4Video](https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video).\nIn the following plots we give a general overview of the samples across modalities and the source of those samples.\n<!--\n<center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_data_split.png\" width=\"auto\" height=\"auto\" alt=\"Image description\">\n</center>\n\n### Details\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_datadetails.png\" width=\"auto\" height=\"auto\" alt=\"Image description\"> -->\n\n## Data Split per modality\n\n| Data Type    | Percentage |\n|--------------|------------|\n| Image        | 34.4%      |\n| Text         | 20.2%      |\n| Video        | 33.0%      |\n| Multi-image  | 12.3%      |\n\n\n## Granular dataset slices per modality\n\n### Text Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/magpie_pro_ft3_80b_mt      | 6.8%       |\n| llava-onevision/magpie_pro_ft3_80b_tt      | 6.8%       |\n| llava-onevision/magpie_pro_qwen2_72b_tt    | 5.8%       |\n| llava-onevision/mathqa                     | 0.9%       |\n\n### Multi-image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| m4-instruct-data/m4_instruct_multiimage    | 10.4%      |\n| mammoth/multiimage-cap6                    | 1.9%       |\n\n### Image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/other                      | 17.4%      |\n| llava-onevision/vision_flan                | 3.9%       |\n| llava-onevision/mavis_math_metagen         | 2.6%       |\n| llava-onevision/mavis_math_rule_geo        | 2.5%       |\n| llava-onevision/sharegpt4o                 | 1.7%       |\n| llava-onevision/sharegpt4v_coco            | 1.5%       |\n| llava-onevision/image_textualization       | 1.3%       |\n| llava-onevision/sharegpt4v_llava           | 0.9%       |\n| llava-onevision/mapqa                      | 0.9%       |\n| llava-onevision/qa                         | 0.8%       |\n| llava-onevision/textocr                    | 0.8%       |\n\n### Video Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-video-178k/1-2m                      | 7.3%       |\n| llava-video-178k/2-3m                      | 7.0%       |\n| other-video/combined                       | 5.7%       |\n| llava-video-178k/hound                     | 4.4%       |\n| llava-video-178k/0-30s                     | 2.4%       |\n| video-star/starb                           | 2.2%       |\n| vista-400k/combined                        | 2.2%       |\n| vript/long                                 | 1.0%       |\n| ShareGPT4Video/all                         | 0.8%       |\n",
      "card_hash": "afc4bf5f519438b221a84292979a9eac3de4b9726e5baaa82033c55f59f9f7a5",
      "token_count": 2907,
      "success": true
    },
    {
      "model_id": "gaunernst/gemma-3-27b-it-int4-awq",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3-27b-it\n---\n\n# Gemma 3 27B Instruction-tuned INT4\n\nThis is the QAT INT4 Flax checkpoint (from Kaggle) converted to HF+AWQ format for ease of use. AWQ was NOT used for quantization. You can find the conversion script `convert_flax.py` in this model repo.\n\nNOTE: this is NOT the same as the official QAT INT4 GGUFs released here https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b\n\nBelow is the original Model card from https://huggingface.co/google/gemma-3-27b-it\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow there are some code snippets on how to get quickly started with running the model. First, install the Transformers library with the version made for Gemma 3:\n\n```sh\n$ pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-27b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single/multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-27b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "153a97e33fb6a282b6a85594bf0dff908f55b7682e61d52b9bfe9805db05b5a6",
      "token_count": 6591,
      "success": true
    },
    {
      "model_id": "HuggingFaceTB/SmolVLM2-2.2B-Instruct",
      "card_text": "---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- HuggingFaceM4/the_cauldron\n- HuggingFaceM4/Docmatix\n- lmms-lab/LLaVA-OneVision-Data\n- lmms-lab/M4-Instruct-Data\n- HuggingFaceFV/finevideo\n- MAmmoTH-VL/MAmmoTH-VL-Instruct-12M\n- lmms-lab/LLaVA-Video-178K\n- orrzohar/Video-STaR\n- Mutonix/Vript\n- TIGER-Lab/VISTA-400K\n- Enxin/MovieChat-1K_train\n- ShareGPT4Video/ShareGPT4Video\npipeline_tag: image-text-to-text\ntags:\n- video-text-to-text\nlanguage:\n- en\nbase_model:\n- HuggingFaceTB/SmolVLM-Instruct\n---\n\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM2_banner.png\" width=\"800\" height=\"auto\" alt=\"Image description\">\n\n# SmolVLM2 2.2B\n\nSmolVLM2-2.2B is a lightweight multimodal model designed to analyze video content. The model processes videos, images, and text inputs to generate text outputs - whether answering questions about media files, comparing visual content, or transcribing text from images. Despite its compact size, requiring only 5.2GB of GPU RAM for video inference, it delivers robust performance on complex multimodal tasks. This efficiency makes it particularly well-suited for on-device applications where computational resources may be limited.\n## Model Summary\n\n- **Developed by:** Hugging Face \ud83e\udd17\n- **Model type:** Multi-modal model (image/multi-image/video/text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n\n## Resources\n\n- **Demo:** [Video Highlight Generator](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM2-HighlightGenerator)\n- **Blog:** [Blog post](https://huggingface.co/blog/smolvlm2)\n\n\n## Uses\n\n\nSmolVLM2 can be used for inference on multimodal (video / image / text) tasks where the input consists of text queries along with video or one or more images. Text and media files can be interleaved arbitrarily, enabling tasks like captioning, visual question answering, and storytelling based on visual content. The model does not support image or video generation.\n\nTo fine-tune SmolVLM2 on a specific task, you can follow [the fine-tuning tutorial](https://github.com/huggingface/smollm/blob/main/vision/finetuning/Smol_VLM_FT.ipynb).\n\n## Evaluation \n\n### Vision Evaluation\n\n| Model             | Mathvista | MMMU  | OCRBench | MMStar | AI2D | ChartQA_Test | Science_QA | TextVQA Val | DocVQA Val |\n|-------------------|-----------|-------|----------|--------|------|--------------|------------|-------------|------------|\n| **SmolVLM2 2.2B** | 51.5      | 42    | 72.9     | 46     | 70   | 68.84        | 90         | 73.21       | 79.98      |\n| SmolVLM 2.2B      | 43.9      | 38.3  | 65.5     | 41.8   | 84.5 | 71.6         | 84.5       | 72.1        | 79.7       |\n\n\n### Video Evaluation\nWe evaluated the performance of the SmolVLM2 family on the following scientific benchmarks:\n\n| Size    | Video-MME | MLVU | MVBench |\n|----------|-----------------|----------|---------------|\n| 2.2B   | 52.1            | 55.2     | 46.27        |\n| 500M | 42.2            | 47.3     | 39.73        |\n| 256M | 33.7            | 40.6     | 32.7          |\n\n\n### How to get started\n\nYou can use transformers to load, infer and fine-tune SmolVLM. Make sure you have num2words, flash-attn and latest transformers installed.\nYou can load the model as follows.\n\n```python\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\nprocessor = AutoProcessor.from_pretrained(model_path)\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"flash_attention_2\"\n).to(\"cuda\")\n```\n\n#### Simple Inference\n\nYou preprocess your inputs directly using chat templates and directly passing them \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Can you describe this image?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n#### Video Inference\n\nTo use SmolVLM2 for video inference, make sure you have decord installed. \n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"video\", \"path\": \"path_to_video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Describe this video in detail\"}\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])\n```\n#### Multi-image Interleaved Inference\n\nYou can interleave multiple media with text using chat templates.\n\n```python\nimport torch\n\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n          {\"type\": \"text\", \"text\": \"What is the similarity between these two images?\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n          {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"},            \n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\n\ngenerated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\nprint(generated_texts[0])\n```\n\n\n### Model optimizations\n\n## Misuse and Out-of-scope Use\n\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\n\n- Prohibited Uses:\n  - Evaluating or scoring individuals (e.g., in employment, education, credit)\n  - Critical automated decision-making\n  - Generating unreliable factual content\n- Malicious Activities:\n  - Spam generation\n  - Disinformation campaigns\n  - Harassment or abuse\n  - Unauthorized surveillance\n\n### License\n\nSmolVLM2 is built upon [the shape-optimized SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384) as image encoder and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) for text decoder part.\n\nWe release the SmolVLM2 checkpoints under the Apache 2.0 license.\n\n## Citation information\nYou can cite us in the following way:\n```bibtex\n@article{marafioti2025smolvlm,\n  title={SmolVLM: Redefining small and efficient multimodal models}, \n  author={Andr\u00e9s Marafioti and Orr Zohar and Miquel Farr\u00e9 and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\n  journal={arXiv preprint arXiv:2504.05299},\n  year={2025}\n}\n```\n\n## Training Data\nSmolVLM2 used 3.3M samples for training originally from ten different datasets: [LlaVa Onevision](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data), [M4-Instruct](https://huggingface.co/datasets/lmms-lab/M4-Instruct-Data), [Mammoth](https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M), [LlaVa Video 178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K), [FineVideo](https://huggingface.co/datasets/HuggingFaceFV/finevideo), [VideoStar](https://huggingface.co/datasets/orrzohar/Video-STaR), [VRipt](https://huggingface.co/datasets/Mutonix/Vript), [Vista-400K](https://huggingface.co/datasets/TIGER-Lab/VISTA-400K), [MovieChat](https://huggingface.co/datasets/Enxin/MovieChat-1K_train) and [ShareGPT4Video](https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video).\nIn the following plots we give a general overview of the samples across modalities and the source of those samples.\n<!--\n<center><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_data_split.png\" width=\"auto\" height=\"auto\" alt=\"Image description\">\n</center>\n\n### Details\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolvlm2_datadetails.png\" width=\"auto\" height=\"auto\" alt=\"Image description\"> -->\n\n## Data Split per modality\n\n| Data Type    | Percentage |\n|--------------|------------|\n| Image        | 34.4%      |\n| Text         | 20.2%      |\n| Video        | 33.0%      |\n| Multi-image  | 12.3%      |\n\n\n## Granular dataset slices per modality\n\n### Text Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/magpie_pro_ft3_80b_mt      | 6.8%       |\n| llava-onevision/magpie_pro_ft3_80b_tt      | 6.8%       |\n| llava-onevision/magpie_pro_qwen2_72b_tt    | 5.8%       |\n| llava-onevision/mathqa                     | 0.9%       |\n\n### Multi-image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| m4-instruct-data/m4_instruct_multiimage    | 10.4%      |\n| mammoth/multiimage-cap6                    | 1.9%       |\n\n### Image Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-onevision/other                      | 17.4%      |\n| llava-onevision/vision_flan                | 3.9%       |\n| llava-onevision/mavis_math_metagen         | 2.6%       |\n| llava-onevision/mavis_math_rule_geo        | 2.5%       |\n| llava-onevision/sharegpt4o                 | 1.7%       |\n| llava-onevision/sharegpt4v_coco            | 1.5%       |\n| llava-onevision/image_textualization       | 1.3%       |\n| llava-onevision/sharegpt4v_llava           | 0.9%       |\n| llava-onevision/mapqa                      | 0.9%       |\n| llava-onevision/qa                         | 0.8%       |\n| llava-onevision/textocr                    | 0.8%       |\n\n### Video Datasets\n| Dataset                                    | Percentage |\n|--------------------------------------------|------------|\n| llava-video-178k/1-2m                      | 7.3%       |\n| llava-video-178k/2-3m                      | 7.0%       |\n| other-video/combined                       | 5.7%       |\n| llava-video-178k/hound                     | 4.4%       |\n| llava-video-178k/0-30s                     | 2.4%       |\n| video-star/starb                           | 2.2%       |\n| vista-400k/combined                        | 2.2%       |\n| vript/long                                 | 1.0%       |\n| ShareGPT4Video/all                         | 0.8%       |\n\n",
      "card_hash": "8e46e92a3a40643b85a6df905b1a2014bcb7a01daaf6566588c89a95c5c6e700",
      "token_count": 3118,
      "success": true
    },
    {
      "model_id": "zai-org/GLM-4.6V",
      "card_text": "---\nlanguage:\n- zh\n- en\nlibrary_name: transformers\nlicense: mit\npipeline_tag: image-text-to-text\n---\n\n# GLM-4.6V\n\n<div align=\"center\">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/logo.svg width=\"40%\"/>\n</div>\n\nThis model is part of the GLM-V family of models, introduced in the paper [GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://huggingface.co/papers/2507.01006).\n\n-   **GLM-4.6V Blog**: [https://z.ai/blog/glm-4.6v](https://z.ai/blog/glm-4.6v)\n-   **Paper**: [https://huggingface.co/papers/2507.01006](https://huggingface.co/papers/2507.01006)\n-   **GitHub Repository**: [https://github.com/zai-org/GLM-V](https://github.com/zai-org/GLM-V)\n-   **Online Demo**: [https://chat.z.ai/](https://chat.z.ai/)\n-   **API Access**: [Z.ai Open Platform](https://docs.z.ai/guides/vlm/glm-4.6v)\n-   **Desktop Assistant App**: [https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App](https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App)\n\n## Introduction\n\nGLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance\ncluster scenarios,\nand GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications.\nGLM-4.6V scales its context window to 128k tokens in training,\nand achieves SoTA performance in visual understanding among models of similar parameter scales.\nCrucially, we integrate native Function Calling capabilities for the first time.\nThis effectively bridges the gap between \"visual perception\" and \"executable action\"\nproviding a unified technical foundation for multimodal agents in real-world business scenarios.\n\n![GLM-4.6V Benchmarks](https://raw.githubusercontent.com/zai-org/GLM-V/refs/heads/main/resources/bench_46v.jpeg)\n\nBeyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces\nseveral key features:\n\n- **Native Multimodal Function Calling** \nEnables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.\n\n- **Interleaved Image-Text Content Generation**\nSupports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context\u2014spanning documents, user inputs, and tool-retrieved images\u2014and synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.\n\n\n- **Multimodal Document Understanding**\nGLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.\n    \n- **Frontend Replication & Visual Editing** \nReconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.\n\n\n**This Hugging Face repository hosts the `GLM-4.6V` model, part of the `GLM-V` series.**\n\n## Usage\n\n### Environment Installation\n\nFor `SGLang`:\n\n```bash\npip install sglang>=0.5.6.post1\npip install nvidia-cudnn-cu12==9.16.0.29\nsudo apt update\nsudo apt install ffmpeg\n```\n\nFor `vLLM`:\n\n```bash\npip install vllm>=0.12.0\npip install transformers>=5.0.0rc0\n```\n\n### Quick Start with Transformers\n\n```python\nfrom transformers import AutoProcessor, Glm4vMoeForConditionalGeneration\nimport torch\n\nMODEL_PATH = \"zai-org/GLM-4.6V\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"describe this image\"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nmodel = Glm4vMoeForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors=\"pt\"\n).to(model.device)\ninputs.pop(\"token_type_ids\", None)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\n## Evaluation Settings\n\nWe primarily use vLLM as the backend for model inference. For faster and more reliable performance on video tasks, we employ SGLang. To reproduce our leaderboard results, we recommend the following decoding parameters:\n\n+\ttop_p: 0.6\n+\ttop_k: 2\n+\ttemperature: 0.8\n+\trepetition_penalty: 1.1\n+\tmax_generate_tokens: 16K\n\nFor more usage details, please refer to Our [Github](https://github.com/zai-org/GLM-V).\n\n## Fixed and Remaining Issues\n\nSince the open-sourcing of GLM-4.1V, we have received extensive feedback from the community and are well aware that the model still has many shortcomings. In subsequent iterations, we attempted to address several common issues \u2014 such as repetitive thinking outputs and formatting errors \u2014 which have been mitigated to some extent in this new version.\n\nHowever, the model still has several limitations and issues that we will fix as soon as possible:\n\n1. Pure text QA capabilities still have significant room for improvement. In this development cycle, our primary focus was on visual multimodal scenarios, and we will enhance pure text abilities in upcoming updates.\n2. The model may still overthink or even repeat itself in certain cases, especially when dealing with complex prompts.\n3. In some situations, the model may restate the answer again at the end.\n4. There remain certain perception limitations, such as counting accuracy and identifying specific individuals, which still require improvement.\n\nThank you for your patience and understanding. We also welcome feedback and suggestions in the issue section \u2014 we will respond and improve as much as we can!\n\n## Citation\n\nIf you use this model, please cite the following paper:\n\n```bibtex\n@misc{vteam2025glm45vglm41vthinkingversatilemultimodal,\n      title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning}, \n      author={V Team and Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and Shuaiqi Duan and Weihan Wang and Yan Wang and Yean Cheng and Zehai He and Zhe Su and Zhen Yang and Ziyang Pan and Aohan Zeng and Baoxu Wang and Bin Chen and Boyan Shi and Changyu Pang and Chenhui Zhang and Da Yin and Fan Yang and Guoqing Chen and Jiazheng Xu and Jiale Zhu and Jiali Chen and Jing Chen and Jinhao Chen and Jinghao Lin and Jinjiang Wang and Junjie Chen and Leqi Lei and Letian Gong and Leyi Pan and Mingdao Liu and Mingde Xu and Mingzhi Zhang and Qinkai Zheng and Sheng Yang and Shi Zhong and Shiyu Huang and Shuyuan Zhao and Siyan Xue and Shangqin Tu and Shengbiao Meng and Tianshu Zhang and Tianwei Luo and Tianxiang Hao and Tianyu Tong and Wenkai Li and Wei Jia and Xiao Liu and Xiaohan Zhang and Xin Lyu and Xinyue Fan and Xuancheng Huang and Yanling Wang and Yadong Xue and Yanfeng Wang and Yanzi Wang and Yifan An and Yifan Du and Yiming Shi and Yiheng Huang and Yilin Niu and Yuan Wang and Yuanchang Yue and Yuchen Li and Yutao Zhang and Yuting Wang and Yu Wang and Yuxuan Zhang and Zhao Xue and Zhenyu Hou and Zhengxiao Du and Zihan Wang and Peng Zhang and Debing Liu and Bin Xu and Juanzi Li and Minlie Huang and Yuxiao Dong and Jie Tang},\n      year={2025},\n      eprint={2507.01006},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.01006}, \n}\n```",
      "card_hash": "2fd45a740ae60a4a1495ffdb643fe24de88b2e0ecdbb7aa9eb9b1c241016afea",
      "token_count": 2090,
      "success": true
    },
    {
      "model_id": "lmstudio-community/gemma-3n-E4B-it-MLX-4bit",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B-it\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n- mlx\n---\n## \ud83d\udcab Community Model> gemma-3n-E4B-it by google\n\n*\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)*.\n\n**Model creator:** [google](https://huggingface.co/google)<br>\n**Original model**: [gemma-3n-E4B-it](https://huggingface.co/google/gemma-3n-E4B-it)<br>\n**MLX quantization:** provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n4-bit quantized version of gemma-3n-E4B-it using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "f9d4e1e7bd521caf5d6b862e939d083dddbd62c1f088ab869e0497aae2d6f15c",
      "token_count": 610,
      "success": true
    },
    {
      "model_id": "unsloth/gemma-3-12b-it-unsloth-bnb-4bit",
      "card_text": "---\nbase_model: google/gemma-3-12b-it\nlanguage:\n- en\nlibrary_name: transformers\nlicense: gemma\ntags:\n- unsloth\n- transformers\n- gemma3\n- gemma\n- google\n---\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\">our collection</a> for all versions of Gemma 3 including GGUF, 4-bit & 16-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em>Unsloth's <a href=\"https://unsloth.ai/blog/deepseekr1-dynamic\">Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 4-bit.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">\u2728 Fine-tune Gemma 3 with Unsloth!</h1>\n</div>\n\n- Fine-tune Gemma 3 (12B) for free using our Google [Colab notebook here](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n- Read our Blog about Gemma 3 support: [unsloth.ai/blog/gemma3](https://unsloth.ai/blog/gemma3)\n- View the rest of our notebooks in our [docs here](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n- Export your fine-tuned model to GGUF, Ollama, llama.cpp or \ud83e\udd17HF.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Gemma 3 (12B)**      | [\u25b6\ufe0f Start on Colab](https://docs.unsloth.ai/get-started/unsloth-notebooks)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2.5 (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 50% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n<br>\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "40e6e640e0464f70440aa502603a16768dee2fc04ec26a26fc8dd0241eabf001",
      "token_count": 6619,
      "success": true
    },
    {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "card_text": "---\nlibrary_name: transformers\nlanguage:\n- ar\n- de\n- en\n- es\n- fr\n- hi\n- id\n- it\n- pt\n- th\n- tl\n- vi\nbase_model:\n- meta-llama/Llama-4-Maverick-17B-128E\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama4\nextra_gated_prompt: >-\n    **LLAMA 4 COMMUNITY LICENSE AGREEMENT**\n\n    Llama 4 Version Effective Date: April 5, 2025\n\n    \"**Agreement**\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\n    \"**Documentation**\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\n    \"**Licensee**\" or \"**you**\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity\u2019s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n    \"**Llama 4**\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\n    \"**Llama Materials**\" means, collectively, Meta\u2019s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n\n    \"**Meta**\" or \"**we**\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\u00a0\n\n    By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n    1\\. **License Rights and Redistribution**.\n\n    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta\u2019s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\u00a0\u00a0\n\n    b. Redistribution and Use.\u00a0\u00a0\n\n    i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.\n\n    ii.\u00a0If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\u00a0\n\n    iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved.\"\n\n    iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy)), which is hereby incorporated by reference into this Agreement.  \n    \u00a0\u00a0  \n    2\\. **Additional Commercial Terms**. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee\u2019s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n    3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n    4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n    5\\. **Intellectual Property**.\n\n    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta\u2019s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n\n    b. Subject to Meta\u2019s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n\n    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n\n    6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\u00a0\n\n    7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nextra_gated_heading: \"Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\"\nlicense: other\nlicense_name: llama4\n---\n\n\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. \n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n\n**Model developer**: Meta\n\n**Model Architecture:**  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. \n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Training Data </th>\n    <th>Params</th>\n    <th>Input modalities</th>\n    <th>Output modalities</th>\n    <th>Context length</th>\n    <th>Token count</th>\n    <th>Knowledge cutoff</th>\n  </tr>\n  <tr>\n    <td>Llama 4 Scout (17Bx16E) </td>\n    <td rowspan=\"2\">A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our <a href=\"https://www.facebook.com/privacy/guide/genai/\">Privacy Center</a>.\n    </td>\n    <td>17B (Activated)\n        109B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>10M</td>\n    <td>~40T</td>\n    <td>August 2024</td>\n  </tr>\n  <tr>\n    <td>Llama 4 Maverick (17Bx128E)</td>\n    <td>17B (Activated)\n        400B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>1M</td>\n    <td>~22T</td>\n    <td>August 2024</td>\n  </tr>\n</table>\n\n**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n\n**Model Release Date:** April 5, 2025\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n\n**License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go [here](https://github.com/meta-llama/llama-cookbook).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n\n**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: \n\n1\\. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n2\\. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n## How to use with transformers\n\nPlease, make sure you have transformers `v4.51.0` installed, or upgrade using `pip install -U transformers`.\n\n```python\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id,\n    attn_implementation=\"flex_attention\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\n\nurl1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nurl2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/cat_style_layout.png\"\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": url1},\n            {\"type\": \"image\", \"url\": url2},\n            {\"type\": \"text\", \"text\": \"Can you describe how these two images are similar, and how they differ?\"},\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device)\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=256,\n)\n\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\nprint(response)\nprint(outputs[0])\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:**  Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n| Model Name | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 4 Scout | 5.0M | 700 | 1,354 | 0 |\n| Llama 4 Maverick | 2.38M | 700 | 645 | 0 |\n| Total | 7.38M | \\- | 1,999 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 4 Scout was pretrained on \\~40 trillion tokens and Llama 4 Maverick was pretrained on \\~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta\u2019s products and services. This includes publicly shared posts from Instagram and Facebook and people\u2019s interactions with Meta AI.\n\n**Data Freshness:** The pretraining data has a cutoff of August 2024\\.\n\n## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\n\n### Pre-trained models\n\n| Pre-trained models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Reasoning & Knowledge | MMLU | 5 | macro\\_avg/acc\\_char\t | 79.3 | 85.2 | 79.6 | 85.5 |\n|  | MMLU-Pro | 5 | macro\\_avg/em | 53.8 | 61.6 | 58.2 | 62.9 |\n|  | MATH | 4 | em\\_maj1@1 | 41.6 | 53.5 | 50.3 | 61.2 |\n| Code | MBPP | 3 | pass@1 | 66.4 | 74.4 | 67.8 | 77.6 |\n| Multilingual | TydiQA | 1 | average/f1 | 29.9 | 34.3 | 31.5 | 31.7 |\n| Image | ChartQA | 0 | relaxed\\_accuracy | No multimodal support |  | 83.4 | 85.3 |\n|  | DocVQA | 0 | anls |  |  | 89.4 | 91.6 |\n\n### Instruction tuned models\t\n\n| Instruction tuned models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | ----- | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.3 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Image Reasoning | MMMU | 0 | accuracy | No multimodal support |  | 69.4 | 73.4 |\n|  | MMMU Pro^ | 0 | accuracy |  |  | 52.2 | 59.6 |\n|  | MathVista | 0 | accuracy |  |  | 70.7 | 73.7 |\n| Image Understanding | ChartQA | 0 | relaxed\\_accuracy |  |  | 88.8 | 90.0 |\n|  | DocVQA (test) | 0 | anls |  |  | 94.4 | 94.4 |\n| Coding | LiveCodeBench (10/01/2024-02/01/2025) | 0 | pass@1 | 33.3 | 27.7 | 32.8 | 43.4 |\n| Reasoning & Knowledge | MMLU Pro | 0 | macro\\_avg/acc | 68.9 | 73.4 | 74.3 | 80.5 |\n|  | GPQA Diamond | 0 | accuracy | 50.5 | 49.0 | 57.2 | 69.8 |\n| Multilingual | MGSM | 0 | average/em | 91.1 | 91.6 | 90.6 | 92.3 |\n| Long context | MTOB (half book) eng-\\>kgv/kgv-\\>eng | \\- | chrF | Context window is 128K |  | 42.2/36.6 | 54.0/46.4 |\n|  | MTOB (full book) eng-\\>kgv/kgv-\\>eng | \\- | chrF |  |  | 39.7/36.3 | 50.8/46.7 |\n\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n## Quantization\n\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\n\n## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n* Provide protections for the community to help prevent the misuse of our models.\n\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta\u2019s Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model\u2019s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n\n### Model level fine tuning\n\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\n\n**Fine-tuning data**   \nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals**  \nBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4\\. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. \n\n**Tone**  \nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\n\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\n\n**System Prompts**  \nLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we\u2019ve seen that the use of a system prompt can be effective in reducing false refusals and templated or \u201cpreachy\u201d language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. \n\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\n\n| System prompt  |\n| :---- |\n| You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don\u2019t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to \u201cit\u2019s important to\u201d, \u201cit\u2019s crucial to\u201d, \u201cit\u2019s essential to\u201d, \"it's unethical to\", \"it's worth noting\u2026\", \u201cRemember\u2026\u201d  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4\\. Your knowledge cutoff date is August 2024\\. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise. |\n\n### Llama 4 system protections\n\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. \n\nWe provide the community with system level [protections](https://llama.meta.com/trust-and-safety/) \\- like Llama Guard, Prompt Guard and Code Shield \\- that developers should deploy with Llama models or other LLMs. All of our [reference implementation](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**   \nWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\n### We spend additional focus on the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. \n\n**2\\. Child Safety**  \nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model\u2019s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We\u2019ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n\n**3\\. Cyber attack enablement**  \nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Considerations and Limitations\n\nOur AI is anchored on the values of freedom of expression \\- helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\n\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, [Llama Protections](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more. \n\n",
      "card_hash": "cae8c581b1787780d095f0f827cee3469014f642cb7d054c0cc7aed5d156d5e3",
      "token_count": 7446,
      "success": true
    },
    {
      "model_id": "lmstudio-community/gemma-3n-E4B-it-MLX-bf16",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B-it\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n- mlx\n---\n## \ud83d\udcab Community Model> gemma-3n-E4B-it by google\n\n*\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)*.\n\n**Model creator:** [google](https://huggingface.co/google)<br>\n**Original model**: [gemma-3n-E4B-it](https://huggingface.co/google/gemma-3n-E4B-it)<br>\n**MLX quantization:** provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\nOriginal bfloat16 version of gemma-3n-E4B-it using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "dc4004e8d162fb093c6174812fab22546d403537ab0581878c60fc1796d132b6",
      "token_count": 610,
      "success": true
    },
    {
      "model_id": "openbmb/MiniCPM-o-2_6",
      "card_text": "---\npipeline_tag: any-to-any\ndatasets:\n- openbmb/RLAIF-V-Dataset\nlibrary_name: transformers\nlanguage:\n- multilingual\ntags:\n- minicpm-o\n- omni\n- vision\n- ocr\n- multi-image\n- video\n- custom_code\n- audio\n- speech\n- voice cloning\n- live Streaming\n- realtime speech conversation\n- asr\n- tts\nlicense: apache-2.0\n---\n\n<h1>A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone</h1>\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-o) | [Online Demo](https://minicpm-omni-webdemo-us.modelbest.cn) | [Technical Blog](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9) | [Join Us](https://mp.weixin.qq.com/mp/wappoc_appmsgcaptcha?poc_token=HAV8UWijqB3ImPSXecZHlOns7NRgpQw9y9EI2_fE&target_url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FKIhH2nCURBXuFXAtYRpuXg%3F)\n\n\n### News\n\n* [2025.06.20] \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f Our official [ollama repository](https://ollama.com/openbmb) is released. Try our latest models with [one click](https://ollama.com/openbmb/minicpm-o2.6)\uff01\n\n* [2025.03.01] \ud83d\ude80\ud83d\ude80\ud83d\ude80 RLAIF-V, which is the alignment technique of MiniCPM-o, is accepted by CVPR 2025\uff01The [code](https://github.com/RLHF-V/RLAIF-V), [dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset), [paper](https://arxiv.org/abs/2405.17220) are open-sourced!\n\n* [2025.01.24] \ud83d\udce2\ud83d\udce2\ud83d\udce2 MiniCPM-o 2.6 technical report is released! [See Here](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9).\n\n* [2025.01.19] \u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0f MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!\n\n## MiniCPM-o 2.6\n\n\n**MiniCPM-o 2.6** is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:\n\n- \ud83d\udd25 **Leading Visual Capability.**\n  MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation over 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet** for single image understanding. It also **outperforms GPT-4V and Claude 3.5 Sonnet** in mutli-image and video understanding, and shows promising in-context learning capability.\n\n- \ud83c\udf99 **State-of-the-art Speech Capability.** MiniCPM-o 2.6 supports **bilingual real-time speech conversation with configurable voices** in English and Chinese. It **outperforms GPT-4o-realtime on audio understanding tasks** such as ASR and STT translation, and shows **state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community**. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.\n\n- \ud83c\udfac **Strong Multimodal Live Streaming Capability.** As a new feature, MiniCPM-o 2.6 can **accept continous video and audio streams independent of user queries, and support real-time speech interaction**. It **outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-art performance in open-source community on StreamingBench**, a comprehensive benchmark for real-time video understanding, omni-source (video & audio) understanding, and multimodal contextual understanding.\t\t\t\t\t\t\t\t\t\t\n\n- \ud83d\udcaa **Strong OCR Capability and Others.**\nAdvancing popular visual capabilites from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves **state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405**.\n  Based on the the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports **multilingual capabilities** on more than 30 languages.\n\n\n- \ud83d\ude80 **Superior Efficiency.**\n  In addition to its friendly size, MiniCPM-o 2.6 also shows **state-of-the-art token density** (i.e., number of pixels encoded into each visual token). **It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models**. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support **multimodal live streaming** on end-side devices such as iPad.\n\n-  \ud83d\udcab  **Easy Usage.**\nMiniCPM-o 2.6 can be easily used in various ways: (1) [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-omni/examples/llava/README-minicpmo2.6.md) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-o-2_6-int4) and [GGUF](https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf) format quantized models in 16 sizes, (3) [vLLM](#efficient-inference-with-llamacpp-ollama-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with [LLaMA-Factory](./docs/llamafactory_train.md), (5) quick local WebUI demo setup with [Gradio](#chat-with-our-demo-on-gradio), and (6) online web demo on [server](https://minicpm-omni-webdemo-us.modelbest.cn/).\n\n\n\n**Model Architecture.**\n\n- **End-to-end Omni-modal Architecture.** Different modality encoder/decoders are connected and trained in an **end-to-end** fashion to fully exploit rich multimodal knowledge.\n- **Omni-modal Live Streaming Mechanism.** (1) We change the offline modality encoder/decoders into online ones for **streaminig inputs/outputs.** (2) We devise a **time-division multiplexing (TDM) mechanism** for omni-modality streaminig processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices. \n- **Configurable Speech Modeling Design.** We devise a multimodal system prompt, including traditional text system prompt, and **a new audio system prompt to determine the assistant voice**. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.\n\n<div align=\"center\">\n<img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/minicpm-o-26-framework-v2.png\" , width=100%>\n</div>\n\n\n### Evaluation  <!-- omit in toc -->\n\n<div align=\"center\">\n    <img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/radar.jpg\" width=90% />\n</div>\n\n#### Visual understanding results\n\n**Image Understanding:**\n\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Token Density<sup>+</sup></th>\n            <th>OpenCompass</th>\n            <th>OCRBench</th>\n            <th>MathVista mini</th>\n            <th>ChartQA</th>\n            <th>MMVet</th>\n            <th>MMStar</th>\n            <th>MME</th>\n            <th>MMB1.1 test</th>\n            <th>AI2D</th>\n            <th>MMMU val</th>\n            <th>HallusionBench</th>\n            <th>TextVQA val</th>\n            <th>DocVQA test</th>\n            <th>MathVerse mini</th>\n            <th>MathVision</th>\n            <th>MMHal Score</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"19\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-20240513</td>\n            <td>-</td>\n            <td>1088</td>\n            <td><u>69.9</u></td>\n            <td>736</td>\n            <td>61.3</td>\n            <td>85.7</td>\n            <td><strong>69.1</strong></td>\n            <td>63.9</td>\n            <td>2328.7</td>\n            <td>82.2</td>\n            <td>84.6</td>\n            <td><strong>69.2</strong></td>\n            <td><strong>55.0</strong></td>\n            <td>-</td>\n            <td>92.8</td>\n            <td><strong>50.2</strong></td>\n            <td><strong>30.4</strong></td>\n            <td><u>3.6</u></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude3.5-Sonnet</td>\n            <td>-</td>\n            <td>750</td>\n            <td>67.9</td>\n            <td>788</td>\n            <td>61.6</td>\n            <td><strong>90.8</strong></td>\n            <td>66.0</td>\n            <td>62.2</td>\n            <td>1920.0</td>\n            <td>78.5</td>\n            <td>80.2</td>\n            <td><u>65.9</u></td>\n            <td>49.9</td>\n            <td>-</td>\n            <td><strong>95.2</strong></td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td>-</td>\n            <td>64.4</td>\n            <td>754</td>\n            <td>57.7</td>\n            <td>81.3</td>\n            <td>64.0</td>\n            <td>59.1</td>\n            <td>2110.6</td>\n            <td>73.9</td>\n            <td>79.1</td>\n            <td>60.6</td>\n            <td>45.6</td>\n            <td>73.5</td>\n            <td>86.5</td>\n            <td>-</td>\n            <td>19.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-mini-20240718</td>\n            <td>-</td>\n            <td>1088</td>\n            <td>64.1</td>\n            <td>785</td>\n            <td>52.4</td>\n            <td>-</td>\n            <td>66.9</td>\n            <td>54.8</td>\n            <td>2003.4</td>\n            <td>76.0</td>\n            <td>77.8</td>\n            <td>60.0</td>\n            <td>46.1</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.3</td>\n        </tr>\n        <tr>\n            <td colspan=\"19\" align=\"left\"><strong>Open Source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Cambrian-34B</td>\n            <td>34B</td>\n            <td><u>1820</u></td>\n            <td>58.3</td>\n            <td>591</td>\n            <td>50.3</td>\n            <td>75.6</td>\n            <td>53.2</td>\n            <td>54.2</td>\n            <td>2049.9</td>\n            <td>77.8</td>\n            <td>79.5</td>\n            <td>50.4</td>\n            <td>41.6</td>\n            <td>76.7</td>\n            <td>75.5</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4V-9B</td>\n            <td>13B</td>\n            <td>784</td>\n            <td>59.1</td>\n            <td>776</td>\n            <td>51.1</td>\n            <td>-</td>\n            <td>58.0</td>\n            <td>54.8</td>\n            <td>2018.8</td>\n            <td>67.9</td>\n            <td>71.2</td>\n            <td>46.9</td>\n            <td>45.0</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Pixtral-12B</td>\n            <td>12B</td>\n            <td>256</td>\n            <td>61.0</td>\n            <td>685</td>\n            <td>56.9</td>\n            <td>81.8</td>\n            <td>58.5</td>\n            <td>54.5</td>\n            <td>-</td>\n            <td>72.7</td>\n            <td>79.0</td>\n            <td>51.1</td>\n            <td>47.0</td>\n            <td>75.7</td>\n            <td>90.7</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">DeepSeek-VL2-27B (4B)</td>\n            <td>27B</td>\n            <td>672</td>\n            <td>66.4</td>\n            <td>809</td>\n            <td>63.9</td>\n            <td>86.0</td>\n            <td>60.0</td>\n            <td>61.9</td>\n            <td>2253.0</td>\n            <td>81.2</td>\n            <td>83.8</td>\n            <td>54.0</td>\n            <td>45.3</td>\n            <td><u>84.2</u></td>\n            <td>93.3</td>\n            <td>-</td>\n            <td>-</td>\n            <td>3.0</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen2-VL-7B</td>\n            <td>8B</td>\n            <td>784</td>\n            <td>67.1</td>\n            <td><u>866</u></td>\n            <td>58.2</td>\n            <td>83.0</td>\n            <td>62.0</td>\n            <td>60.7</td>\n            <td>2326.0</td>\n            <td>81.8</td>\n            <td>83.0</td>\n            <td>54.1</td>\n            <td>50.6</td>\n            <td><strong>84.3</strong></td>\n            <td><u>94.5</u></td>\n            <td>31.9</td>\n            <td>16.3</td>\n            <td>3.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-OneVision-72B</td>\n            <td>72B</td>\n            <td>182</td>\n            <td>68.1</td>\n            <td>741</td>\n            <td>67.5</td>\n            <td>83.7</td>\n            <td>60.6</td>\n            <td><strong>65.8</strong></td>\n            <td>2261.0</td>\n            <td><strong>85.0</strong></td>\n            <td><u>85.6</u></td>\n            <td>56.8</td>\n            <td>49.0</td>\n            <td>80.5</td>\n            <td>91.3</td>\n            <td>39.1</td>\n            <td>-</td>\n            <td>3.5</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2.5-8B</td>\n            <td>8B</td>\n            <td>706</td>\n            <td>68.3</td>\n            <td>822</td>\n            <td><u>64.4</u></td>\n            <td>84.8</td>\n            <td>62.8</td>\n            <td>62.8</td>\n            <td>2344.0</td>\n            <td><u>83.6</u></td>\n            <td>84.5</td>\n            <td>56.0</td>\n            <td>50.1</td>\n            <td>79.1</td>\n            <td>93.0</td>\n            <td>39.5</td>\n            <td>19.7</td>\n            <td>3.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td><strong>2822</strong></td>\n            <td>65.2</td>\n            <td>852*</td>\n            <td>60.6</td>\n            <td>79.4</td>\n            <td>60.0</td>\n            <td>57.5</td>\n            <td><u>2348.4*</u></td>\n            <td>78.0</td>\n            <td>82.1</td>\n            <td>49.8*</td>\n            <td>48.1*</td>\n            <td>80.1</td>\n            <td>90.8</td>\n            <td>25.7</td>\n            <td>18.3</td>\n            <td>3.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>8B</td>\n            <td><strong>2822</strong></td>\n            <td><strong>70.2</strong></td>\n            <td><strong>897*</strong></td>\n            <td><strong>71.9*</strong></td>\n            <td><u>86.9*</u></td>\n            <td><u>67.5</u></td>\n            <td><u>64.0</u></td>\n            <td><strong>2372.0*</strong></td>\n            <td>80.5</td>\n            <td><strong>85.8</strong></td>\n            <td>50.4*</td>\n            <td><u>51.9</u></td>\n            <td>82.0</td>\n            <td>93.5</td>\n            <td><u>41.4*</u></td>\n            <td><u>23.1*</u></td>\n            <td><strong>3.8</strong></td>\n        </tr>\n    </tbody>\n</table>\n</div>\n* We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set.\n\n\n<sup>+</sup> Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.\n\nNote: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.\n\n\n**Multi-image and Video Understanding:**\n\n<details>\n<summary>click to view</summary>\n<div align=\"center\">\n \n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>BLINK val</th>\n            <th>Mantis Eval</th>\n            <th>MIRB</th>\n            <th>Video-MME (wo / w subs)</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"6\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-20240513</td>\n            <td>-</td>\n            <td><strong>68.0</strong></td>\n            <td>-</td>\n            <td>-</td>\n            <td><strong>71.9/77.2<strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT4V</td>\n            <td>-</td>\n            <td>54.6</td>\n            <td>62.7</td>\n            <td>53.1</td>\n            <td>59.9/63.3</td>\n        </tr>\n        <tr>\n            <td colspan=\"6\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-NeXT-Interleave 14B</td>\n            <td>14B</td>\n            <td>52.6</td>\n            <td>66.4</td>\n            <td>30.2</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-OneVision-72B</td>\n            <td>72B</td>\n            <td>55.4</td>\n            <td><strong>77.6</strong></td>\n            <td>-</td>\n            <td><u>66.2/69.5</u></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MANTIS 8B</td>\n            <td>8B</td>\n            <td>49.1</td>\n            <td>59.5</td>\n            <td>34.8</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen2-VL-7B</td>\n            <td>8B</td>\n            <td>53.2</td>\n            <td>69.6*</td>\n            <td><strong>67.6*</strong></td>\n            <td>63.3/69.0</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2.5-8B</td>\n            <td>8B</td>\n            <td>54.8</td>\n            <td>67.7</td>\n            <td>52.5</td>\n            <td>64.2/66.9</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td>53.0</td>\n            <td>69.1</td>\n            <td>53.8</td>\n            <td>60.9/63.6</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>8B</td>\n            <td><u>56.7</u></td>\n            <td><u>71.9</u></td>\n            <td><u>58.6</u></td>\n            <td>63.9/67.9</td>\n        </tr>\n    </tbody>\n</table>\n\n</div>\n* We evaluate officially released checkpoints by ourselves.\n\n</details>\n\n\n#### Audio understanding and speech conversation results.\n\n**Audio Understanding:**\n\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Task</th>\n            <th>Size</th>\n            <th colspan=\"3\">ASR (zh)</th>\n            <th colspan=\"3\">ASR (en)</th>\n            <th colspan=\"2\">AST</th>\n            <th>Emotion</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Metric</th>\n            <td></td>\n            <th colspan=\"3\">CER\u2193</th>\n            <th colspan=\"3\">WER\u2193</th>\n            <th colspan=\"2\">BLEU\u2191</th>\n            <th>ACC\u2191</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Dataset</th>\n            <td></td>\n            <th>AISHELL-1</th>\n            <th>Fleurs zh</th>\n            <th>WenetSpeech test-net</th>\n            <th>LibriSpeech test-clean</th>\n            <th>GigaSpeech</th>\n            <th>TED-LIUM</th>\n            <th>CoVoST en2zh</th>\n            <th>CoVoST zh2en</th>\n            <th>MELD emotion</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"11\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-Realtime</td>\n            <td>-</td>\n            <td>7.3*</td>\n            <td><u>5.4*</u></td>\n            <td>28.9*</td>\n            <td>2.6*</td>\n            <td>12.9*</td>\n            <td>4.8*</td>\n            <td>37.1*</td>\n            <td>15.7*</td>\n            <td>33.2*</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td>4.5*</td>\n            <td>5.9*</td>\n            <td>14.3*</td>\n            <td>2.9*</td>\n            <td>10.6*</td>\n            <td><strong>3.0*</strong></td>\n            <td><u>47.3*</u></td>\n            <td>22.6*</td>\n            <td>48.4*</td>\n        </tr>\n        <tr>\n            <td colspan=\"11\" align=\"left\"><strong>Open-Source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen2-Audio-7B</td>\n            <td>8B</td>\n            <td>-</td>\n            <td>7.5</td>\n            <td>-</td>\n            <td><strong>1.6</strong></td>\n            <td>-</td>\n            <td>-</td>\n            <td>45.2</td>\n            <td><u>24.4</u></td>\n            <td><strong>55.3</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen2-Audio-7B-Instruct</td>\n            <td>8B</td>\n            <td>2.6*</td>\n            <td>6.9*</td>\n            <td><u>10.3*</u></td>\n            <td>3.1*</td>\n            <td><u>9.7</u>*</td>\n            <td>5.9*</td>\n            <td>39.5*</td>\n            <td>22.9*</td>\n            <td>17.4*</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4-Voice-Base</td>\n            <td>9B</td>\n            <td><u>2.5</u></td>\n            <td>-</td>\n            <td>-</td>\n            <td>2.8</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n            <td>-</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>8B</td>\n            <td><strong>1.6</strong></td>\n            <td><strong>4.4</strong></td>\n            <td><strong>6.9</strong></td>\n            <td><u>1.7</u></td>\n            <td><strong>8.7</strong></td>\n            <td><strong>3.0</strong></td>\n            <td><strong>48.2</strong></td>\n            <td><strong>27.2</strong></td>\n            <td><u>52.4</u></td>\n        </tr>\n    </tbody>\n</table>\n</div>\n* We evaluate officially released checkpoints by ourselves.<br><br>\n\n**Speech Generation:**\n\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Task</th>\n            <th>Size</th>\n            <th colspan=\"9\">SpeechQA</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Metric</th>\n            <th></th>\n            <th colspan=\"3\">ACC\u2191</th>\n            <th>G-Eval (10 point)\u2191</th>\n            <th>Semantic ELO score\u2191</th>\n            <th>Acoustic ELO score\u2191</th>\n            <th>Overall ELO score\u2191</th>\n            <th>UTMOS\u2191</th>\n            <th>ASR-WER\u2193</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Dataset</th>\n            <th></th>\n            <th>Speech Llama Q.</th>\n            <th>Speech Web Q.</th>\n            <th>Speech Trivia QA</th>\n            <th>Speech AlpacaEval</th>\n            <th colspan=\"5\">AudioArena</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"11\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-Realtime</td>\n            <td></td>\n            <td><strong>71.7</strong></td>\n            <td><strong>51.6</strong></td>\n            <td><strong>69.7</strong></td>\n            <td><strong>7.4</strong></td>\n            <td><strong>1157</strong></td>\n            <td><strong>1203</strong></td>\n            <td><strong>1200</strong></td>\n            <td><strong>4.2</strong></td>\n            <td><strong>2.3</strong></td>\n        </tr>\n        <tr>\n            <td colspan=\"11\" align=\"left\"><strong>Open-Source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GLM-4-Voice</td>\n            <td>9B</td>\n            <td>50.0</td>\n            <td>32.0</td>\n            <td>36.4</td>\n            <td><u>5.1</u></td>\n            <td>999</td>\n            <td>1147</td>\n            <td>1035</td>\n            <td><u>4.1</u></td>\n            <td><u>11.7</u></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Llama-Omni</td>\n            <td>8B</td>\n            <td>45.3</td>\n            <td>22.9</td>\n            <td>10.7</td>\n            <td>3.9</td>\n            <td>960</td>\n            <td>878</td>\n            <td>897</td>\n            <td>3.2</td>\n            <td>24.3</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Moshi</td>\n            <td>7B</td>\n            <td>43.7</td>\n            <td>23.8</td>\n            <td>16.7</td>\n            <td>2.4</td>\n            <td>871</td>\n            <td>808</td>\n            <td>875</td>\n            <td>2.8</td>\n            <td>8.2</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Mini-Omni</td>\n            <td>1B</td>\n            <td>22.0</td>\n            <td>12.8</td>\n            <td>6.9</td>\n            <td>2.5</td>\n            <td>926</td>\n            <td>803</td>\n            <td>865</td>\n            <td>3.4</td>\n            <td>10.0</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>8B</td>\n            <td><u>61.0</u></td>\n            <td><u>40.0</u></td>\n            <td><u>40.2</u></td>\n            <td><u>5.1</u></td>\n            <td><u>1088</u></td>\n            <td><u>1163</u></td>\n            <td><u>1131</u></td>\n            <td><strong>4.2</strong></td>\n            <td>9.8</td>\n        </tr>\n    </tbody>\n</table>\n</div>\nAll results are from AudioEvals, and the evaluation methods along with further details can be found in <a href=\"https://github.com/OpenBMB/UltraEval-Audio\" target=\"_blank\">UltraEval-Audio</a>.<br><br>\n\n**End-to-end Voice Cloning**\n\n<div align=\"center\">\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Task</th>\n            <th colspan=\"2\">Voice cloning</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Metric</th>\n            <th>SIMO\u2191</th>\n            <th>SIMO\u2191</th>\n        </tr>\n        <tr>\n            <th align=\"left\">Dataset</th>\n            <th>Seed-TTS test-zh</th>\n            <th>Seed-TTS test-en</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">F5-TTS</td>\n            <td><strong>76</strong></td>\n            <td><strong>67</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">CosyVoice</td>\n            <td><u>75</u></td>\n            <td><u>64</u></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">FireRedTTS</td>\n            <td>63</td>\n            <td>46</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>57</td>\n            <td>47</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\n\n#### Multimodal live streaming results.\n  \n**Multimodal Live Streaming:** results on StreamingBench\n\n<table style=\"margin: 0px auto;\">\n    <thead>\n        <tr>\n            <th align=\"left\">Model</th>\n            <th>Size</th>\n            <th>Real-Time Video Understanding</th>\n            <th>Omni-Source Understanding</th>\n            <th>Contextual Understanding</th>\n            <th>Overall</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td colspan=\"7\" align=\"left\"><strong>Proprietary</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Gemini 1.5 Pro</td>\n            <td>-</td>\n            <td><u>77.4</u></td>\n            <td><strong>67.8</strong></td>\n            <td><strong>51.1</strong></td>\n            <td><strong>70.3</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">GPT-4o-202408</td>\n            <td>-</td>\n            <td>74.5</td>\n            <td>51.0</td>\n            <td><u>48.0</u></td>\n            <td>64.1</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Claude-3.5-Sonnet</td>\n            <td>-</td>\n            <td>74.0</td>\n            <td>41.4</td>\n            <td>37.8</td>\n            <td>59.7</td>\n        </tr>\n        <tr>\n            <td colspan=\"9\" align=\"left\"><strong>Open-source</strong></td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VILA-1.5</td>\n            <td>8B</td>\n            <td>61.5</td>\n            <td>37.5</td>\n            <td>26.7</td>\n            <td>49.5</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LongVA</td>\n            <td>7B</td>\n            <td>63.1</td>\n            <td>35.9</td>\n            <td>30.2</td>\n            <td>50.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-Next-Video-34B</td>\n            <td>34B</td>\n            <td>69.8</td>\n            <td>41.7</td>\n            <td>34.3</td>\n            <td>56.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">Qwen2-VL-7B</td>\n            <td>8B</td>\n            <td>71.2</td>\n            <td>40.7</td>\n            <td>33.1</td>\n            <td>57.0</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternVL2-8B</td>\n            <td>8B</td>\n            <td>70.1</td>\n            <td>42.7</td>\n            <td>34.1</td>\n            <td>57.0</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">VITA-1.5</td>\n            <td>8B</td>\n            <td>70.9</td>\n            <td>40.8</td>\n            <td>35.8</td>\n            <td>57.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">LLaVA-OneVision-7B</td>\n            <td>8B</td>\n            <td>74.3</td>\n            <td>40.8</td>\n            <td>31.0</td>\n            <td>58.4</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">InternLM-XC2.5-OL-7B</td>\n            <td>8B</td>\n            <td>75.4</td>\n            <td>46.2</td>\n            <td>33.6</td>\n            <td>60.8</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-V 2.6</td>\n            <td>8B</td>\n            <td>72.4</td>\n            <td>40.2</td>\n            <td>33.4</td>\n            <td>57.7</td>\n        </tr>\n        <tr>\n            <td nowrap=\"nowrap\" align=\"left\">MiniCPM-o 2.6</td>\n            <td>8B</td>\n            <td><strong>79.9</strong></td>\n            <td><u>53.4</u></td>\n            <td>38.5</td>\n            <td><u>66.0</u></td>\n        </tr>\n    </tbody>\n</table>\n\n\n\n### Examples <!-- omit in toc -->\n\nWe deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.\n\n<div align=\"center\">\n  <a href=\"https://youtu.be/JFJg9KZ_iZk\"><img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/o-2dot6-demo-video-preview.png\", width=70%></a>\n</div>\n\n<br>\n\n\n<div style=\"display: flex; flex-direction: column; align-items: center;\">\n  <img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png\" alt=\"math\" style=\"margin-bottom: 5px;\">\n  <img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png\" alt=\"diagram\" style=\"margin-bottom: 5px;\">\n  <img src=\"https://github.com/OpenBMB/MiniCPM-o/raw/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png\" alt=\"bike\" style=\"margin-bottom: 5px;\">\n</div>\n\n\n\n\n## Online Demo\nClick here to try the online demo of [MiniCPM-o 2.6](https://minicpm-omni-webdemo-us.modelbest.cn).\n\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Please ensure that `transformers==4.44.2` is installed, as other versions may have compatibility issues. We are investigating this issue. Requirements tested on python 3.10\uff1a\n```\nPillow==10.1.0\ntorch==2.3.1\ntorchaudio==2.3.1\ntorchvision==0.18.1\ntransformers==4.44.2\nlibrosa==0.9.0\nsoundfile==0.12.1\nvector-quantize-pytorch==1.18.5\nvocos==0.1.0\ndecord\nmoviepy\n```\n\n\n### Model initialization\n```python\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\n# load omni model default, the default init_vision/init_audio/init_tts is True\n# if load vision-only model, please set init_audio=False and init_tts=False\n# if load audio-only model, please set init_vision=False\nmodel = AutoModel.from_pretrained(\n    'openbmb/MiniCPM-o-2_6',\n    trust_remote_code=True,\n    attn_implementation='sdpa', # sdpa or flash_attention_2\n    torch_dtype=torch.bfloat16,\n    init_vision=True,\n    init_audio=True,\n    init_tts=True\n)\n\n\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)\n\n# In addition to vision-only mode, tts processor and vocos also needs to be initialized\nmodel.init_tts()\n```\n\nIf you are using an older version of PyTorch, you might encounter this issue `\"weight_norm_fwd_first_dim_kernel\" not implemented for 'BFloat16'`, Please convert the TTS to float32 type.\n```python\nmodel.tts.float()\n```\n\n### Omni mode\nWe provide two inference modes: chat and streaming\n\n#### Chat inference\n```python\nimport math\nimport numpy as np\nfrom PIL import Image\nfrom moviepy.editor import VideoFileClip\nimport tempfile\nimport librosa\nimport soundfile as sf\n\ndef get_video_chunk_content(video_path, flatten=True):\n    video = VideoFileClip(video_path)\n    print('video_duration:', video.duration)\n    \n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n        temp_audio_file_path = temp_audio_file.name\n        video.audio.write_audiofile(temp_audio_file_path, codec=\"pcm_s16le\", fps=16000)\n        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)\n    num_units = math.ceil(video.duration)\n    \n    # 1 frame + 1s audio chunk\n    contents= []\n    for i in range(num_units):\n        frame = video.get_frame(i+1)\n        image = Image.fromarray((frame).astype(np.uint8))\n        audio = audio_np[sr*i:sr*(i+1)]\n        if flatten:\n            contents.extend([\"<unit>\", image, audio])\n        else:\n            contents.append([\"<unit>\", image, audio])\n    \n    return contents\n\nvideo_path=\"assets/Skiing.mp4\"\n# if use voice clone prompt, please set ref_audio\nref_audio_path = 'assets/demo.wav'\nref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)\nsys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')\n# or use default prompt\n# sys_msg = model.get_sys_prompt(mode='omni', language='en')\n\ncontents = get_video_chunk_content(video_path)\nmsg = {\"role\":\"user\", \"content\": contents}\nmsgs = [sys_msg, msg]\n\n# please set generate_audio=True and output_audio_path to save the tts result\ngenerate_audio = True\noutput_audio_path = 'output.wav'\n\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    temperature=0.5,\n    max_new_tokens=4096,\n    omni_input=True, # please set omni_input=True when omni inference\n    use_tts_template=True,\n    generate_audio=generate_audio,\n    output_audio_path=output_audio_path,\n    max_slice_nums=1,\n    use_image_id=False,\n    return_dict=True\n)\nprint(res)\n\n## You will get the answer: The person in the picture is skiing down a snowy slope.\n# import IPython\n# IPython.display.Audio('output.wav')\n\n```\n#### Streaming inference\n```python\n# a new conversation need reset session first, it will reset the kv-cache\nmodel.reset_session()\n\ncontents = get_video_chunk_content(video_path, flatten=False)\nsession_id = '123'\ngenerate_audio = True\n\n# 1. prefill system prompt\nres = model.streaming_prefill(\n    session_id=session_id,\n    msgs=[sys_msg], \n    tokenizer=tokenizer\n)\n\n# 2. prefill video/audio chunks\nfor content in contents:\n    msgs = [{\"role\":\"user\", \"content\": content}]\n    res = model.streaming_prefill(\n        session_id=session_id,\n        msgs=msgs, \n        tokenizer=tokenizer\n    )\n\n# 3. generate\nres = model.streaming_generate(\n    session_id=session_id,\n    tokenizer=tokenizer,\n    temperature=0.5,\n    generate_audio=generate_audio\n)\n\naudios = []\ntext = \"\"\n\nif generate_audio:\n    for r in res:\n        audio_wav = r.audio_wav\n        sampling_rate = r.sampling_rate\n        txt = r.text\n\n        audios.append(audio_wav)\n        text += txt\n        \n    res = np.concatenate(audios)\n    sf.write(\"output.wav\", res, samplerate=sampling_rate)\n    print(\"text:\", text)\n    print(\"audio saved to output.wav\")\nelse:\n    for r in res:\n        text += r['text']\n    print(\"text:\", text)\n\n```\n\n\n### Speech and Audio Mode\n\nModel initialization\n\n```python\nimport torch\nimport librosa\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,\n    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)\n\nmodel.init_tts()\nmodel.tts.float()\n```\n\n<hr/>\n\n#### Mimick\n\n`Mimick` task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.\n\n```python\nmimick_prompt = \"Please repeat each user's speech, including voice style and speech content.\"\naudio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked\n\n# can also try `./assets/input_examples/cxk_original.wav`, \n# `./assets/input_examples/fast-pace.wav`, \n# `./assets/input_examples/chi-english-1.wav` \n# `./assets/input_examples/exciting-emotion.wav` \n# for different aspects of speech-centric features.\n\nmsgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    temperature=0.3,\n    generate_audio=True,\n    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path\n)\n```\n\n<hr/>\n\n#### General Speech Conversation with Configurable Voices\n\nA general usage scenario of `MiniCPM-o-2.6` is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, `MiniCPM-o-2.6` sounds **more natural and human-like**. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.\n\n\n```python\nref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')\n\n# round one\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = [sys_prompt, user_question]\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_roleplay_round_1.wav',\n)\n\n# round two\nhistory = msgs.append({'role': 'assistant', 'content': res})\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = history.append(user_question)\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_roleplay_round_2.wav',\n)\nprint(res)\n```\n\n<hr/>\n\n#### Speech Conversation as an AI Assistant\n\nAn enhanced feature of `MiniCPM-o-2.6` is to act as an AI assistant, but only with limited choice of voices. In this mode, `MiniCPM-o-2.6` is **less human-like and more like a voice assistant**. In this mode, the model is more instruction-following. For demo, you are suggested to use `assistant_female_voice`, `assistant_male_voice`, and `assistant_default_female_voice`. Other voices may work but not as stable as the default voices.\n\n*Please note that, `assistant_female_voice` and `assistant_male_voice` are more stable but sounds like robots, while `assistant_default_female_voice` is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices `assistant_female_voice` and `assistant_male_voice`.*\n\n```python\nref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') \nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question\n\n# round one\nmsgs = [sys_prompt, user_question]\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_assistant_round_1.wav',\n)\n\n# round two\nhistory = msgs.append({'role': 'assistant', 'content': res})\nuser_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}\nmsgs = history.append(user_question)\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_assistant_round_2.wav',\n)\nprint(res)\n```\n\n<hr/>\n\n#### Instruction-to-Speech\n\n`MiniCPM-o-2.6` can also do Instruction-to-Speech, aka **Voice Creation**. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to https://voxinstruct.github.io/VoxInstruct/.\n\n```python\ninstruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'\n\nmsgs = [{'role': 'user', 'content': [instruction]}]\n\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_voice_creation.wav',\n)\n```\n\n<hr/>\n\n#### Voice Cloning\n\n`MiniCPM-o-2.6` can also do zero-shot text-to-speech, aka **Voice Cloning**. With this mode, model will act like a TTS model.\n\n\n```python\nref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio\nsys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')\ntext_prompt = f\"Please read the text below.\"\nuser_question = {'role': 'user', 'content': [text_prompt, \"content that you want to read\"]}\n\nmsgs = [sys_prompt, user_question]\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_voice_cloning.wav',\n)\n\n```\n\n<hr/>\n\n#### Addressing Various Audio Understanding Tasks\n\n`MiniCPM-o-2.6` can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.\n\nFor audio-to-text tasks, you can use the following prompts:\n\n- ASR with ZH(same as AST en2zh): `\u8bf7\u4ed4\u7ec6\u542c\u8fd9\u6bb5\u97f3\u9891\u7247\u6bb5\uff0c\u5e76\u5c06\u5176\u5185\u5bb9\u9010\u5b57\u8bb0\u5f55\u3002`\n- ASR with EN(same as AST zh2en): `Please listen to the audio snippet carefully and transcribe the content.`\n- Speaker Analysis: `Based on the speaker's content, speculate on their gender, condition, age range, and health status.`\n- General Audio Caption: `Summarize the main content of the audio.`\n- General Sound Scene Tagging: `Utilize one keyword to convey the audio's content or the associated scene.`\n\n```python\ntask_prompt = \"Please listen to the audio snippet carefully and transcribe the content.\" + \"\\n\" # can change to other prompts.\naudio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned\n\nmsgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]\n\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    max_new_tokens=128,\n    use_tts_template=True,\n    generate_audio=True,\n    temperature=0.3,\n    output_audio_path='result_audio_understanding.wav',\n)\nprint(res)\n```\n\n\n### Vision-Only mode\n\n`MiniCPM-o-2_6` has the same inference methods as `MiniCPM-V-2_6`\n\n#### Chat with single image\n```python\n# test.py\nimage = Image.open('xx.jpg').convert('RGB')\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': [image, question]}]\nres = model.chat(\n    image=None,\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(res)\n\n## if you want to use streaming, please make sure sampling=True and stream=True\n## the model.chat will return a generator\nres = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    sampling=True,\n    stream=True\n)\ngenerated_text = \"\"\nfor new_text in res:\n    generated_text += new_text\n    print(new_text, flush=True, end='')\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to show Python code running MiniCPM-o 2.6 with multiple images input. </summary>\n  \n```python\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-o 2.6 with few-shot input. </summary>\n\n```python\nquestion = \"production date\" \nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\nmsgs = [\n    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n    {'role': 'user', 'content': [image_test, question]}\n]\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n#### Chat with video\n<details>\n<summary> Click to view Python code running MiniCPM-o 2.6 with video input. </summary>\n\n```python\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\ndef encode_video(video_path):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n    vr = VideoReader(video_path, ctx=cpu(0))\n    sample_fps = round(vr.get_avg_fps() / 1)  # FPS\n    frame_idx = [i for i in range(0, len(vr), sample_fps)]\n    if len(frame_idx) > MAX_NUM_FRAMES:\n        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\n    frames = vr.get_batch(frame_idx).asnumpy()\n    frames = [Image.fromarray(v.astype('uint8')) for v in frames]\n    print('num frames:', len(frames))\n    return frames\nvideo_path =\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n    {'role': 'user', 'content': frames + [question]}, \n]\n# Set decode params for video\nparams={}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution >  448*448\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    **params\n)\nprint(answer)\n```\n</details>\n\nPlease look at [GitHub](https://github.com/OpenBMB/MiniCPM-o) for more detail about usage.\n\n\n## Inference with llama.cpp<a id=\"llamacpp\"></a>\nMiniCPM-o 2.6 (vision-only mode) can run with llama.cpp. See our fork of [llama.cpp](https://github.com/OpenBMB/llama.cpp/tree/minicpm-omni) and [readme](https://github.com/OpenBMB/llama.cpp/blob/minicpm-omni/examples/llava/README-minicpmo2.6.md) for more detail.\n\n\n## Int4 quantized version\nDownload the int4 quantized version for lower GPU memory (7GB) usage:  [MiniCPM-o-2_6-int4](https://huggingface.co/openbmb/MiniCPM-o-2_6-int4).\n\n\n## License\n#### Model License\n* The MiniCPM-o/V model weights and code are open-sourced under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM-V/blob/main/LICENSE) license.\n* To help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration [\"questionnaire\"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g).\n\n\n#### Statement\n* As an LMM, MiniCPM-o 2.6 generates contents by learning a large mount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-o 2.6 does not represent the views and positions of the model developers\n* We will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n## Key Techniques and Other Multimodal Projects\n\n\ud83d\udc4f Welcome to explore key techniques of MiniCPM-o 2.6 and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)  | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers \ud83d\udcdd and liking this project \u2764\ufe0f\uff01\n\n```bib\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={arXiv preprint arXiv:2408.01800},\n  year={2024}\n}\n```",
      "card_hash": "6df762847c94dd0b6dcdc9ab2ed831bff299c55155b6215766fbd6257bf27162",
      "token_count": 15175,
      "success": true
    },
    {
      "model_id": "lmstudio-community/gemma-3n-E4B-it-MLX-8bit",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B-it\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n- mlx\n---\n## \ud83d\udcab Community Model> gemma-3n-E4B-it by google\n\n*\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)*.\n\n**Model creator:** [google](https://huggingface.co/google)<br>\n**Original model**: [gemma-3n-E4B-it](https://huggingface.co/google/gemma-3n-E4B-it)<br>\n**MLX quantization:** provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n8-bit quantized version of gemma-3n-E4B-it using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "53c87a552c5561f129e1c3c02493d36bcb06f50653b791b98f18e39f7459d70c",
      "token_count": 610,
      "success": true
    },
    {
      "model_id": "lmstudio-community/gemma-3n-E4B-it-MLX-6bit",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B-it\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n- mlx\n---\n## \ud83d\udcab Community Model> gemma-3n-E4B-it by google\n\n*\ud83d\udc7e [LM Studio](https://lmstudio.ai) Community models highlights program. Highlighting new & noteworthy models by the community. Join the conversation on [Discord](https://discord.gg/aPQfnNkxGC)*.\n\n**Model creator:** [google](https://huggingface.co/google)<br>\n**Original model**: [gemma-3n-E4B-it](https://huggingface.co/google/gemma-3n-E4B-it)<br>\n**MLX quantization:** provided by [LM Studio team](https://x.com/lmstudio) using [mlx_vlm](https://github.com/Blaizzy/mlx-vlm)<br>\n\n## Technical Details\n\n6-bit quantized version of gemma-3n-E4B-it using MLX, optimized for Apple Silicon.\n\n## Special thanks\n\n\ud83d\ude4f Special thanks to the [Apple Machine Learning Research](https://github.com/ml-explore) team for creating [MLX](https://github.com/ml-explore/mlx).\n\n## Disclaimers\n\nLM Studio is not the creator, originator, or owner of any Model featured in the Community Model Program. Each Community Model is created and provided by third parties. LM Studio does not endorse, support, represent or guarantee the completeness, truthfulness, accuracy, or reliability of any Community Model.  You understand that Community Models can produce content that might be offensive, harmful, inaccurate or otherwise inappropriate, or deceptive. Each Community Model is the sole responsibility of the person or entity who originated such Model. LM Studio may not monitor or control the Community Models and cannot, and does not, take responsibility for any such Model. LM Studio disclaims all warranties or guarantees about the accuracy, reliability or benefits of the Community Models.  LM Studio further disclaims any warranty that the Community Model will meet your requirements, be secure, uninterrupted or available at any time or location, or error-free, viruses-free, or that any errors will be corrected, or otherwise. You will be solely responsible for any damage resulting from your use of or access to the Community Models, your downloading of any Community Model, or use of any other Community Model provided by or through LM Studio.\n",
      "card_hash": "de6d4c285f73b5a8fcb56810df0d250dda304318e2a6711c93179697d1f02eda",
      "token_count": 610,
      "success": true
    },
    {
      "model_id": "OpenGVLab/InternVL3-1B-hf",
      "card_text": "---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n- OpenGVLab/InternVL3-1B-Instruct\nbase_model_relation: finetune\ndatasets:\n- OpenGVLab/MMPR-v1.2\nlanguage:\n- multilingual\ntags:\n- internvl\n---\n\n# InternVL3-1B Transformers \ud83e\udd17 Implementation\n\n[\\[\ud83d\udcdc InternVL 1.0\\]](https://huggingface.co/papers/2312.14238)  [\\[\ud83d\udcdc InternVL 1.5\\]](https://huggingface.co/papers/2404.16821)  [\\[\ud83d\udcdc InternVL 2.5\\]](https://huggingface.co/papers/2412.05271)  [\\[\ud83d\udcdc InternVL2.5-MPO\\]](https://huggingface.co/papers/2411.10442)  [\\[\ud83d\udcdc InternVL3\\]](https://huggingface.co/papers/2504.10479)\n\n[\\[\ud83c\udd95 Blog\\]](https://internvl.github.io/blog/)  [\\[\ud83d\udde8\ufe0f Chat Demo\\]](https://internvl.opengvlab.com/)  [\\[\ud83e\udd17 HF Demo\\]](https://huggingface.co/spaces/OpenGVLab/InternVL)  [\\[\ud83d\ude80 Quick Start\\]](#quick-start)  [\\[\ud83d\udcd6 Documents\\]](https://internvl.readthedocs.io/en/latest/)\n\n<div align=\"center\">\n  <img width=\"500\" alt=\"image\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64006c09330a45b03605bba3/zJsd2hqd3EevgXo6fNgC-.png\">\n</div>\n\n\n> [!IMPORTANT]\n> This repository contains the Hugging Face \ud83e\udd17 Transformers implementation for the [OpenGVLab/InternVL3-1B](https://huggingface.co/OpenGVLab/InternVL3-1B) model.\n> It is intended to be functionally equivalent to the original OpenGVLab release.\n> As a native Transformers model, it supports core library features such as various attention implementations (eager, including SDPA, and FA2) and enables efficient batched inference with interleaved image, video, and text inputs.\n\n## Introduction\n\nWe introduce InternVL3, an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance.\nCompared to InternVL 2.5, InternVL3 exhibits superior multimodal perception and reasoning capabilities, while further extending its multimodal capabilities to encompass tool usage, GUI agents, industrial image analysis, 3D vision perception, and more.\nAdditionally, we compare InternVL3 with  Qwen2.5 Chat models, whose corresponding pre-trained base models are employed as the initialization of the langauge component in InternVL3. Benefitting from Native Multimodal Pre-Training, the InternVL3 series achieves even better overall text performance than the Qwen2.5 series.\n\n![image/png](https://huggingface.co/datasets/Weiyun1025/InternVL-Performance/resolve/main/internvl3/overall.png)\n\nYou can find more info on the InternVL3 family in the original checkpoint [OpenGVLab/InternVL3-1B](https://huggingface.co/OpenGVLab/InternVL3-1B)\n\n## Usage example\n\n### Inference with Pipeline\n\nHere is how you can use the `image-text-to-text` pipeline to perform inference with the `InternVL3` models in just a few lines of code:\n\n```python\n>>> from transformers import pipeline\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\n...                 \"type\": \"image\",\n...                 \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n...             },\n...             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n...         ],\n...     },\n... ]\n\n>>> pipe = pipeline(\"image-text-to-text\", model=\"OpenGVLab/InternVL3-1B-hf\")\n>>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n>>> outputs[0][\"generated_text\"]\n'The image showcases a vibrant scene of nature, featuring several flowers and a bee. \\n\\n1. **Foreground Flowers**: \\n   - The primary focus is on a large, pink cosmos flower with a prominent yellow center. The petals are soft and slightly r'\n```\n### Inference on a single image\n\nThis example demonstrates how to perform inference on a single image with the InternVL models using chat templates.\n\n> [!NOTE]\n> Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n...             {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n...         ],\n...     }\n... ]\n\n>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> generate_ids = model.generate(**inputs, max_new_tokens=50)\n>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n\n>>> decoded_output\n'The image shows two cats lying on a pink blanket. The cat on the left is a tabby with a mix of brown, black, and white fur, and it appears to be sleeping with its head resting on the blanket. The cat on the'\n```\n\n### Text-only generation\nThis example shows how to generate text using the InternVL model without providing any image input.\n\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\"type\": \"text\", \"text\": \"Write a haiku\"},\n...         ],\n...     }\n... ]\n\n>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n\n>>> generate_ids = model.generate(**inputs, max_new_tokens=50)\n>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n\n>>> print(decoded_output)\n\"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\"\n```\n\n### Batched image and text inputs\nInternVL models also support batched image and text inputs.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [\n...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n...             ],\n...         },\n...     ],\n...     [\n...         {\n...             \"role\": \"user\",\n...             \"content\": [\n...                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n...                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n...             ],\n...         },\n...     ],\n... ]\n\n\n>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> output = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n>>> decoded_outputs\n[\"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\",\n 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of']\n```\n\n### Batched multi-image input\nThis implementation of the InternVL models supports batched text-images inputs with different number of images for each text.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n... \u00a0 \u00a0 [\n... \u00a0 \u00a0 \u00a0 \u00a0 {\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"user\",\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": [\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n... \u00a0 \u00a0 \u00a0 \u00a0 },\n... \u00a0 \u00a0 ],\n... \u00a0 \u00a0 [\n... \u00a0 \u00a0 \u00a0 \u00a0 {\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"user\",\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": [\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n... \u00a0 \u00a0 \u00a0 \u00a0 },\n... \u00a0 \u00a0 ],\n>>> ]\n\n>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n\n>>> output = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n>>> decoded_outputs\n[\"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\",\n 'user\\n\\n\\nThese images depict two different landmarks. Can you identify them?\\nassistant\\nYes, these images depict the Statue of Liberty and the Golden Gate Bridge.']\n```\n\n### Video input\nInternVL models can also handle video inputs. Here is an example of how to perform inference on a video input using chat templates.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n\n>>> model_checkpoint = \"OpenGVLab/InternVL3-8B-hf\"\n>>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, quantization_config=quantization_config)\n\n>>> messages = [\n...     {\n...         \"role\": \"user\",\n...         \"content\": [\n...             {\n...                 \"type\": \"video\",\n...                 \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n...             },\n...             {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n...         ],\n...     }\n>>> ]\n>>> inputs = processor.apply_chat_template(\n...     messages,\n...     return_tensors=\"pt\",\n...     add_generation_prompt=True,\n...     tokenize=True,\n...     return_dict=True,\n>>> ).to(model.device, dtype=torch.float16)\n\n>>> output = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n>>> decoded_output\n'The man is performing a forehand shot.'\n```\n\n### Interleaved image and video inputs\nThis example showcases how to handle a batch of chat conversations with interleaved image and video inputs using chat template.\n\n```python\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n>>> import torch\n\n>>> torch_device = \"cuda\"\n>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n\n>>> messages = [\n... \u00a0 \u00a0 [\n... \u00a0 \u00a0 \u00a0 \u00a0 {\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"user\",\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": [\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n... \u00a0 \u00a0 \u00a0 \u00a0 },\n... \u00a0 \u00a0 ],\n... \u00a0 \u00a0 [\n... \u00a0 \u00a0 \u00a0 \u00a0 {\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"user\",\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": [\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"video\", \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n... \u00a0 \u00a0 \u00a0 \u00a0 },\n... \u00a0 \u00a0 ],\n... \u00a0 \u00a0 [\n... \u00a0 \u00a0 \u00a0 \u00a0 {\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"role\": \"user\",\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": [\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\n... \u00a0 \u00a0 \u00a0 \u00a0 },\n... \u00a0 \u00a0 ],\n>>> ]\n>>> inputs = processor.apply_chat_template(\n... \u00a0 \u00a0 messages,\n... \u00a0 \u00a0 padding=True,\n...     add_generation_prompt=True,\n...     tokenize=True,\n...     return_dict=True,\n... \u00a0 \u00a0 return_tensors=\"pt\",\n>>> ).to(model.device, dtype=torch.bfloat16)\n\n>>> outputs = model.generate(**inputs, max_new_tokens=25)\n\n>>> decoded_outputs = processor.batch_decode(outputs, skip_special_tokens=True)\n>>> decoded_outputs\n['user\\n\\n\\nThese images depict two different landmarks. Can you identify them?\\nassistant\\nThe images depict the Statue of Liberty and the Golden Gate Bridge.',\n 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot',\n \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"]\n```\n\n## License\n\nThis project is released under the MIT License. This project uses the pre-trained Qwen2.5 as a component, which is licensed under the Qwen License.\n\n## Citation\n\nIf you find this project useful in your research, please consider citing:\n\n```BibTeX\n@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{wang2024mpo,\n  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},\n  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},\n  journal={arXiv preprint arXiv:2411.10442},\n  year={2024}\n}\n@article{chen2024far,\n  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={arXiv preprint arXiv:2404.16821},\n  year={2024}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}\n```",
      "card_hash": "6badc5c414fd6fcb1b9927480c43213ac48c32fcc907646c3403cd04614d1078",
      "token_count": 4081,
      "success": true
    },
    {
      "model_id": "unsloth/gemma-3-4b-it",
      "card_text": "---\ntags:\n- unsloth\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: To access Gemma on Hugging Face, you\u2019re required to review and\n  agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model:\n- google/gemma-3-4b-it\n---\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as follows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look! \n# Based on the image, the animal on the candy is a **turtle**. \n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single/multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-4b-it\"\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(\n    model_id, device_map=\"auto\"\n).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages, add_generation_prompt=True, tokenize=True,\n    return_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, \n# focusing on a cluster of pink cosmos flowers and a busy bumblebee. \n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "fba92bd34f1d8581dc1e683709de21dd3fd9bdfb9a9a8738c15a443700c20824",
      "token_count": 6446,
      "success": true
    },
    {
      "model_id": "google/gemma-3n-E4B-it",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: >-\n  To access Gemma on Hugging Face, you\u2019re required to review and agree to\n  Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\nbase_model: google/gemma-3n-E4B\ntags:\n- automatic-speech-recognition\n- automatic-speech-translation\n- audio-text-to-text\n- video-text-to-text\n---\n\n> [!Note]\n> This repository corresponds to the launch version of Gemma 3n E4B IT (Instruct), to be used with Hugging Face `transformers`,\n> supporting text, audio, and vision (image and video) inputs.\n> \n> Gemma 3n models have multiple architecture innovations:\n>  * They are available in two sizes based on [effective parameters](https://ai.google.dev/gemma/docs/gemma-3n#parameters). While the raw parameter count of this model is 8B, the architecture design allows the model to be run with a memory footprint comparable to a traditional 4B model by offloading low-utilization matrices from the accelerator.\n>  * They use a MatFormer architecture that allows nesting sub-models within the E4B model. We provide one sub-model (an [E2B](https://huggingface.co/google/gemma-3n-E2B-it)), or you can access a spectrum of custom-sized models using the [Mix-and-Match method](https://goo.gle/gemma3n-matformer-lab).\n>\n> Learn more about these techniques in the [technical blog post](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide)\n> and the [Gemma documentation](https://ai.google.dev/gemma/docs/gemma-3n). \n\n# Gemma 3n model card\n\n**Model Page**: [Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n)\n\n**Resources and Technical Documentation**:\n\n-   [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n-   [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma-3n)\n-   [Gemma on HuggingFace](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)\n-   [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n)\n\n**Terms of Use**: [Terms](https://ai.google.dev/gemma/terms)\\\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for pre-trained and\ninstruction-tuned variants. These models were trained with data in over 140\nspoken languages.\n\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the\n[Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n#parameters)\npage.\n\n### Inputs and outputs\n\n-   **Input:**\n    -   Text string, such as a question, a prompt, or a document to be\n        summarized\n    -   Images, normalized to 256x256, 512x512, or 768x768 resolution\n        and encoded to 256 tokens each\n    -   Audio data encoded to 6.25 tokens per second from a single channel\n    -   Total input context of 32K tokens\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output length up to 32K tokens, subtracting the request\n        input tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running\nthe model. First, install the Transformers library. Gemma 3n is supported\nstarting from transformers 4.53.0.\n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\nYou can initialize the model and processor for inference with `pipeline` as\nfollows.\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3n-e4b-it\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16,\n)\n```\n\nWith instruction-tuned models, you need to use chat templates to process our\ninputs first. Then, you can pass it to the pipeline.\n\n```python\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n        ]\n    }\n]\n\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\n```\n\n#### Running the model on a single GPU\n\n```python\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3n-e4b-it\"\n\nmodel = Gemma3nForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16,).eval()\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n        ]\n    }\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_dict=True,\n    return_tensors=\"pt\",\n).to(model.device)\n\ninput_len = inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\n```\n\n### Citation\n\n```\n@article{gemma_3n_2025,\n    title={Gemma 3n},\n    url={https://ai.google.dev/gemma/docs/gemma-3n},\n    publisher={Google DeepMind},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\n\n-   **Web Documents**: A diverse collection of web text ensures the model\n    is exposed to a broad range of linguistic styles, topics, and vocabulary.\n    The training dataset includes content in over 140 languages.\n-   **Code**: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   **Mathematics**: Training on mathematical text helps the model learn\n    logical reasoning, symbolic representation, and to address mathematical queries.\n-   **Images**: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n-   Audio: A diverse set of sound samples enables the model to recognize\n    speech, transcribe text from recordings, and identify information in audio data.\n\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   **CSAM Filtering**: Rigorous CSAM (Child Sexual Abuse Material)\n    filtering was applied at multiple stages in the data preparation process to\n    ensure the exclusion of harmful and illegal content.\n-   **Sensitive Data Filtering**: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   **Additional methods**: Filtering based on content quality and safety in\n    line with\n    [our policies](https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit\n(TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\n\n-   **Performance**: TPUs are specifically designed to handle the massive\n    computations involved in training generative models. They can speed up\n    training considerably compared to CPUs.\n-   **Memory**: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   **Scalability**: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   **Cost-effectiveness**: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n\nThese advantages are aligned with\n[Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/jax-ml/jax) and\n[ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/).\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://goo.gle/gemma2report):\n*\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with **IT** are for\ninstruction-tuned models. Evaluation results marked with **PT** are for\npre-trained models.\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | n-shot   |  E2B PT  |  E4B PT  |\n| ------------------------------ |----------------|----------|:--------:|:--------:|\n| [HellaSwag][hellaswag]         | Accuracy       | 10-shot  |   72.2   |   78.6   |\n| [BoolQ][boolq]                 | Accuracy       | 0-shot   |   76.4   |   81.6   |\n| [PIQA][piqa]                   | Accuracy       | 0-shot   |   78.9   |   81.0   |\n| [SocialIQA][socialiqa]         | Accuracy       | 0-shot   |   48.8   |   50.0   |\n| [TriviaQA][triviaqa]           | Accuracy       | 5-shot   |   60.8   |   70.2   |\n| [Natural Questions][naturalq]  | Accuracy       | 5-shot   |   15.5   |   20.9   |\n| [ARC-c][arc]                   | Accuracy       | 25-shot  |   51.7   |   61.6   |\n| [ARC-e][arc]                   | Accuracy       | 0-shot   |   75.8   |   81.6   |\n| [WinoGrande][winogrande]       | Accuracy       | 5-shot   |   66.8   |   71.7   |\n| [BIG-Bench Hard][bbh]          | Accuracy       | few-shot |   44.3   |   52.9   |\n| [DROP][drop]                   | Token F1 score | 1-shot   |   53.9   |   60.8   |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### Multilingual\n\n| Benchmark                           | Metric                  | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------|-------------------------|----------|:--------:|:--------:|\n| [MGSM][mgsm]                        | Accuracy                |  0-shot  |   53.1   |   60.7   |\n| [WMT24++][wmt24pp] (ChrF)           | Character-level F-score |  0-shot  |   42.7   |   50.1   |\n| [Include][include]                  | Accuracy                |  0-shot  |   38.6   |   57.2   |\n| [MMLU][mmlu] (ProX)                 | Accuracy                |  0-shot  |    8.1   |   19.9   |\n| [OpenAI MMLU][openai-mmlu]          | Accuracy                |  0-shot  |   22.3   |   35.6   |\n| [Global-MMLU][global-mmlu]          | Accuracy                |  0-shot  |   55.1   |   60.3   |\n| [ECLeKTic][eclektic]                | ECLeKTic score          |  0-shot  |    2.5   |    1.9   |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[include]:https://arxiv.org/abs/2411.19799\n[mmlu]: https://arxiv.org/abs/2009.03300\n[openai-mmlu]: https://huggingface.co/datasets/openai/MMMLU\n[global-mmlu]: https://huggingface.co/datasets/CohereLabs/Global-MMLU\n[eclektic]: https://arxiv.org/abs/2502.21228\n\n#### STEM and code\n\n| Benchmark                           | Metric                   | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------|--------------------------|----------|:--------:|:--------:|\n| [GPQA][gpqa] Diamond                | RelaxedAccuracy/accuracy |  0-shot  |   24.8   |   23.7   |\n| [LiveCodeBench][lcb] v5             | pass@1                   |  0-shot  |   18.6   |   25.7   |\n| Codegolf v2.2                       | pass@1                   |  0-shot  |   11.0   |   16.8   |\n| [AIME 2025][aime-2025]              | Accuracy                 |  0-shot  |    6.7   |   11.6   |\n\n[gpqa]: https://arxiv.org/abs/2311.12022\n[lcb]: https://arxiv.org/abs/2403.07974\n[aime-2025]: https://www.vals.ai/benchmarks/aime-2025-05-09\n\n#### Additional benchmarks\n\n| Benchmark                            | Metric     | n-shot   |  E2B IT  |  E4B IT  |\n| ------------------------------------ |------------|----------|:--------:|:--------:|\n| [MMLU][mmlu]                         |  Accuracy  |  0-shot  |   60.1   |   64.9   |\n| [MBPP][mbpp]                         |  pass@1    |  3-shot  |   56.6   |   63.6   |\n| [HumanEval][humaneval]               |  pass@1    |  0-shot  |   66.5   |   75.0   |\n| [LiveCodeBench][lcb]                 |  pass@1    |  0-shot  |   13.2   |   13.2   |\n| HiddenMath                           |  Accuracy  |  0-shot  |   27.7   |   37.7   |\n| [Global-MMLU-Lite][global-mmlu-lite] |  Accuracy  |  0-shot  |   59.0   |   64.5   |\n| [MMLU][mmlu] (Pro)                   |  Accuracy  |  0-shot  |   40.5   |   50.6   |\n\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n[lcb]: https://arxiv.org/abs/2403.07974\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   **Text Generation**: Generate creative text formats such as\n        poems, scripts, code, marketing copy, and email drafts.\n    -   **Chatbots and Conversational AI**: Power conversational\n        interfaces for customer service, virtual assistants, or interactive\n        applications.\n    -   **Text Summarization**: Generate concise summaries of a text\n        corpus, research papers, or reports.\n    -   **Image Data Extraction**: Extract, interpret, and summarize\n        visual data for text communications.\n    -   **Audio Data Extraction**: Transcribe spoken language, translate speech\n        to text in other languages, and analyze sound-based data.\n-   Research and Education\n    -   **Natural Language Processing (NLP) and generative model\n        Research**: These models can serve as a foundation for researchers to\n        experiment with generative models and NLP techniques, develop\n        algorithms, and contribute to the advancement of the field.\n    -   **Language Learning Tools**: Support interactive language\n        learning experiences, aiding in grammar correction or providing writing\n        practice.\n    -   **Knowledge Exploration**: Assist researchers in exploring large\n        bodies of data by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   Generative models trained on large-scale, real-world text and image data\n        can reflect socio-cultural biases embedded in the training material.\n        These models underwent careful scrutiny, input data pre-processing\n        described and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   Generative models can be misused to generate text that is\n        false, misleading, or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit](https://ai.google.dev/responsible).\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making generative model technology accessible to\n        developers and researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous monitoring\n    (using evaluation metrics, human review) and the exploration of de-biasing\n    techniques during model training, fine-tuning, and other use cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    generative models. Educational resources and reporting mechanisms for users\n    to flag misuse are provided. Prohibited uses of Gemma models are outlined\n    in the\n    [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n-   **Privacy violations**: Models were trained on data filtered for removal of\n    certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
      "card_hash": "9abba3d9811d38df10311cc3f24c52317a9edb50d390945e5a70802750b227b8",
      "token_count": 5941,
      "success": true
    },
    {
      "model_id": "mlx-community/gemma-3-12b-it-qat-4bit",
      "card_text": "---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n- OpenGVLab/InternVL3-1B-Instruct\nbase_model_relation: finetune\ndatasets:\n- OpenGVLab/MMPR-v1.2\nlanguage:\n- multilingual\ntags:\n- internvl\n- custom_code\n- mlx\n---\n\n# mlx-community/gemma-3-12b-it-qat-4bit\nThis model was converted to MLX format from [`google/gemma-3-12b-it-qat-q4_0-unquantized`]() using mlx-vlm version **0.1.25**.\nRefer to the [original model card](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized) for more details on the model.\n## Use with mlx\n\n```bash\npip install -U mlx-vlm\n```\n\n```bash\npython -m mlx_vlm.generate --model mlx-community/gemma-3-12b-it-qat-4bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>\n```\n",
      "card_hash": "2606c6301157499b14d6187da214eb5410678901c931612fc2cc5ae3c2dfa625",
      "token_count": 281,
      "success": true
    },
    {
      "model_id": "zai-org/AutoGLM-Phone-9B",
      "card_text": "---\nlicense: mit\nlanguage:\n- zh\nbase_model:\n- zai-org/GLM-4.1V-9B-Base\npipeline_tag: image-text-to-text\ntags:\n- agent\nlibrary_name: transformers\n---\n\n# AutoGLM-Phone-9B\n\n<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/resources/logo.svg\" width=\"20%\"/>\n</div>\n\n<p align=\"center\">\n    \ud83d\udc4b Join our <a href=\"https://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/resources/WECHAT.md\" target=\"_blank\">WeChat</a> community\n</p>\n\n> \u26a0\ufe0f This project is intended **for research and educational purposes only**.  \n> Any use for illegal data access, system interference, or unlawful activities is strictly prohibited.  \n> Please review our [Terms of Use](https://raw.githubusercontent.com/zai-org/Open-AutoGLM/refs/heads/main/resources/privacy_policy.txt) carefully.\n\n## Project Overview\n\n**Phone Agent** is a mobile intelligent assistant framework built on **AutoGLM**, capable of understanding smartphone screens through multimodal perception and executing automated operations to complete tasks.  \nThe system controls devices via **ADB (Android Debug Bridge)**, uses a **vision-language model** for screen understanding, and leverages **intelligent planning** to generate and execute action sequences.\n\nUsers can simply describe tasks in natural language\u2014for example, *\u201cOpen Xiaohongshu and search for food recommendations.\u201d*  \nPhone Agent will automatically parse the intent, understand the current UI, plan the next steps, and carry out the entire workflow.\n\nThe system also includes:\n- **Sensitive action confirmation mechanisms**\n- **Human-in-the-loop fallback** for login or verification code scenarios\n- **Remote ADB debugging**, allowing device connection via WiFi or network for flexible remote control and development\n\n## Model Usage\n\nWe provide an open-source model usage guide to help you quickly download and deploy the model.  \nPlease visit our **[GitHub](https://github.com/zai-org/Open-AutoGLM)** for detailed instructions.\n\n- The model architecture is identical to **`GLM-4.1V-9B-Thinking`**.  \n  For deployment details, see the **[GLM-V](https://github.com/zai-org/GLM-V)** repository.\n\n### Citation\n\nIf you find our work helpful, please cite the following paper:\n```bibtex\n@article{liu2024autoglm,\n  title={Autoglm: Autonomous foundation agents for guis},\n  author={Liu, Xiao and Qin, Bo and Liang, Dongzhu and Dong, Guang and Lai, Hanyu and Zhang, Hanchen and Zhao, Hanlin and Iong, Iat Long and Sun, Jiadai and Wang, Jiaqi and others},\n  journal={arXiv preprint arXiv:2411.00820},\n  year={2024}\n}\n@article{xu2025mobilerl,\n  title={MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents},\n  author={Xu, Yifan and Liu, Xiao and Liu, Xinghan and Fu, Jiaqi and Zhang, Hanchen and Jing, Bohao and Zhang, Shudan and Wang, Yuting and Zhao, Wenyi and Dong, Yuxiao},\n  journal={arXiv preprint arXiv:2509.18119},\n  year={2025}\n}\n```",
      "card_hash": "8a63836f7cc7549493f07cb3b2e381a641fe74b03280f4730cd3bf7cc509c8b5",
      "token_count": 744,
      "success": true
    },
    {
      "model_id": "unsloth/gemma-3-27b-it-bnb-4bit",
      "card_text": "---\nbase_model: google/gemma-3-27b-it\nlanguage:\n- en\nlibrary_name: transformers\nlicense: gemma\ntags:\n- unsloth\n- transformers\n- gemma3\n- gemma\n- google\n---\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\">our collection</a> for all versions of Gemma 3 including GGUF, 4-bit & 16-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em><a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively\">Read our Guide</a> to see how to Run Gemma 3 correctly.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">\u2728 Fine-tune Gemma 3 with Unsloth!</h1>\n</div>\n\n- Fine-tune Gemma 3 (12B) for free using our Google [Colab notebook here](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n- Read our Blog about Gemma 3 support: [unsloth.ai/blog/gemma3](https://unsloth.ai/blog/gemma3)\n- View the rest of our notebooks in our [docs here](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n- Export your fine-tuned model to GGUF, Ollama, llama.cpp or \ud83e\udd17HF.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Gemma 3 (12B)**      | [\u25b6\ufe0f Start on Colab](https://docs.unsloth.ai/get-started/unsloth-notebooks)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2.5 (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 50% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n<br>\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "58d98c4bdfce94b2eb3b22e9723d954059aaaa83db1e2251054e6893efb111b6",
      "token_count": 6617,
      "success": true
    },
    {
      "model_id": "mlx-community/gemma-3-27b-it-qat-4bit",
      "card_text": "---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n- OpenGVLab/InternVL3-1B-Instruct\nbase_model_relation: finetune\ndatasets:\n- OpenGVLab/MMPR-v1.2\nlanguage:\n- multilingual\ntags:\n- internvl\n- custom_code\n- mlx\n---\n\n# mlx-community/gemma-3-27b-it-qat-4bit\nThis model was converted to MLX format from [`google/gemma-3-27b-it-qat-q4_0-unquantized`]() using mlx-vlm version **0.1.23**.\nRefer to the [original model card](https://huggingface.co/google/gemma-3-27b-it-qat-q4_0-unquantized) for more details on the model.\n## Use with mlx\n\n```bash\npip install -U mlx-vlm\n```\n\n```bash\npython -m mlx_vlm.generate --model mlx-community/gemma-3-27b-it-qat-4bit --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>\n```\n",
      "card_hash": "f4abfa4e4164f877089d8b31494828873744fb0c335033a1e4dfede627933fc6",
      "token_count": 281,
      "success": true
    },
    {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "card_text": "---\nlibrary_name: transformers\nlanguage:\n- ar\n- de\n- en\n- es\n- fr\n- hi\n- id\n- it\n- pt\n- th\n- tl\n- vi\nbase_model:\n- meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\nbase_model_relation: quantized\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama4\nextra_gated_prompt: >-\n    **LLAMA 4 COMMUNITY LICENSE AGREEMENT**\n\n    Llama 4 Version Effective Date: April 5, 2025\n\n    \"**Agreement**\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\n    \"**Documentation**\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\n    \"**Licensee**\" or \"**you**\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity\u2019s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n    \"**Llama 4**\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\n    \"**Llama Materials**\" means, collectively, Meta\u2019s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n\n    \"**Meta**\" or \"**we**\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\u00a0\n\n    By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n    1\\. **License Rights and Redistribution**.\n\n    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta\u2019s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\u00a0\u00a0\n\n    b. Redistribution and Use.\u00a0\u00a0\n\n    i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \"Built with Llama\" on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \"Llama\" at the beginning of any such AI model name.\n\n    ii.\u00a0If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\u00a0\n\n    iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 4 is licensed under the Llama 4 Community License, Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved.\"\n\n    iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy)), which is hereby incorporated by reference into this Agreement.  \n    \u00a0\u00a0  \n    2\\. **Additional Commercial Terms**. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee\u2019s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n    3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n    4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n    5\\. **Intellectual Property**.\n\n    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta\u2019s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n\n    b. Subject to Meta\u2019s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n\n    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n\n    6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\u00a0\n\n    7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nextra_gated_heading: \"Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\"\nlicense: other\nlicense_name: llama4\n---\n\n\n## Model Information\n\nThe Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. \n\nThese Llama 4 models mark the beginning of a new era for the Llama ecosystem. We are launching two efficient models in the Llama 4 series, Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n\n**Model developer**: Meta\n\n**Model Architecture:**  The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality. \n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Training Data </th>\n    <th>Params</th>\n    <th>Input modalities</th>\n    <th>Output modalities</th>\n    <th>Context length</th>\n    <th>Token count</th>\n    <th>Knowledge cutoff</th>\n  </tr>\n  <tr>\n    <td>Llama 4 Scout (17Bx16E) </td>\n    <td rowspan=\"2\">A mix of publicly available, licensed data and information from Meta's products and services. This includes publicly shared posts from Instagram and Facebook and people's interactions with Meta AI. Learn more in our <a href=\"https://www.facebook.com/privacy/guide/genai/\">Privacy Center</a>.\n    </td>\n    <td>17B (Activated)\n        109B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>10M</td>\n    <td>~40T</td>\n    <td>August 2024</td>\n  </tr>\n  <tr>\n    <td>Llama 4 Maverick (17Bx128E)</td>\n    <td>17B (Activated)\n        400B (Total)\n    </td>\n    <td>Multilingual text and image</td>\n    <td>Multilingual text and code</td>\n    <td>1M</td>\n    <td>~22T</td>\n    <td>August 2024</td>\n  </tr>\n</table>\n\n**Supported languages:** Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. \n\n**Model Release Date:** April 5, 2025\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models may be released as we improve model behavior with community feedback.\n\n**License**: A custom commercial license, the Llama 4 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE)\n\n**Where to send questions or comments about the model:** Instructions on how to provide feedback or comments on the model can be found in the Llama [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 4 in applications, please go [here](https://github.com/meta-llama/llama-cookbook).\n\n## How to use with transformers\n\nPlease, make sure you have transformers `v4.51.0` installed, or upgrade using `pip install -U transformers`.\n\n```python\nfrom transformers import AutoTokenizer, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id,\n    tp_plan=\"auto\",\n    torch_dtype=\"auto\",\n)\n\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\noutputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\nprint(outputs[0])\n```\n\n## Intended Use\n\n**Intended Use Cases:** Llama 4 is intended for commercial and research use in multiple languages. Instruction tuned models are intended for assistant-like chat and visual reasoning tasks, whereas pretrained models can be adapted for natural language generation. For vision, Llama 4 models are also optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The Llama 4 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 4 Community License allows for these use cases. \n\n**Out-of-scope**: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 4 Community License. Use in languages or capabilities beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: \n\n1\\. Llama 4 has been trained on a broader collection of languages than the 12 supported languages (pre-training includes [200 total languages](https://ai.meta.com/research/no-language-left-behind/)). Developers may fine-tune Llama 4 models for languages beyond the 12 supported languages provided they comply with the Llama 4 Community License and the Acceptable Use Policy.  Developers are responsible for ensuring that their use of Llama 4 in additional languages is done in a safe and responsible manner.\n\n2\\. Llama 4 has been tested for image understanding up to 5 input images. If leveraging additional image understanding capabilities beyond this, Developers are responsible for ensuring that their deployments are mitigated for risks and should perform additional testing and tuning tailored to their specific applications.\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU clusters, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:**  Model pre-training utilized a cumulative of **7.38M** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **1,999 tons** CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with clean and renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n| Model Name | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 4 Scout | 5.0M | 700 | 1,354 | 0 |\n| Llama 4 Maverick | 2.38M | 700 | 645 | 0 |\n| Total | 7.38M | \\- | 1,999 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 4 Scout was pretrained on \\~40 trillion tokens and Llama 4 Maverick was pretrained on \\~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta\u2019s products and services. This includes publicly shared posts from Instagram and Facebook and people\u2019s interactions with Meta AI.\n\n**Data Freshness:** The pretraining data has a cutoff of August 2024\\.\n\n## Benchmarks\n\nIn this section, we report the results for Llama 4 relative to our previous models. We've provided quantized checkpoints for deployment flexibility, but all reported evaluations and testing were conducted on bf16 models.\n\n### Pre-trained models\n\n| Pre-trained models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Reasoning & Knowledge | MMLU | 5 | macro\\_avg/acc\\_char\t | 79.3 | 85.2 | 79.6 | 85.5 |\n|  | MMLU-Pro | 5 | macro\\_avg/em | 53.8 | 61.6 | 58.2 | 62.9 |\n|  | MATH | 4 | em\\_maj1@1 | 41.6 | 53.5 | 50.3 | 61.2 |\n| Code | MBPP | 3 | pass@1 | 66.4 | 74.4 | 67.8 | 77.6 |\n| Multilingual | TydiQA | 1 | average/f1 | 29.9 | 34.3 | 31.5 | 31.7 |\n| Image | ChartQA | 0 | relaxed\\_accuracy | No multimodal support |  | 83.4 | 85.3 |\n|  | DocVQA | 0 | anls |  |  | 89.4 | 91.6 |\n\n### Instruction tuned models\t\n\n| Instruction tuned models |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | ----- | :---: | :---: |\n| Category | Benchmark | \\# Shots | Metric | Llama 3.3 70B | Llama 3.1 405B | **Llama 4 Scout** | **Llama 4 Maverick** |\n| Image Reasoning | MMMU | 0 | accuracy | No multimodal support |  | 69.4 | 73.4 |\n|  | MMMU Pro^ | 0 | accuracy |  |  | 52.2 | 59.6 |\n|  | MathVista | 0 | accuracy |  |  | 70.7 | 73.7 |\n| Image Understanding | ChartQA | 0 | relaxed\\_accuracy |  |  | 88.8 | 90.0 |\n|  | DocVQA (test) | 0 | anls |  |  | 94.4 | 94.4 |\n| Coding | LiveCodeBench (10/01/2024-02/01/2025) | 0 | pass@1 | 33.3 | 27.7 | 32.8 | 43.4 |\n| Reasoning & Knowledge | MMLU Pro | 0 | macro\\_avg/acc | 68.9 | 73.4 | 74.3 | 80.5 |\n|  | GPQA Diamond | 0 | accuracy | 50.5 | 49.0 | 57.2 | 69.8 |\n| Multilingual | MGSM | 0 | average/em | 91.1 | 91.6 | 90.6 | 92.3 |\n| Long context | MTOB (half book) eng-\\>kgv/kgv-\\>eng | \\- | chrF | Context window is 128K |  | 42.2/36.6 | 54.0/46.4 |\n|  | MTOB (full book) eng-\\>kgv/kgv-\\>eng | \\- | chrF |  |  | 39.7/36.3 | 50.8/46.7 |\n\n^reported numbers for MMMU Pro is the average of Standard and Vision tasks\n\n## Quantization\n\nThe Llama 4 Scout model is released as BF16 weights, but can fit within a single H100 GPU with on-the-fly int4 quantization; the Llama 4 Maverick model is released as both BF16 and FP8 quantized weights. The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality. We provide code for on-the-fly int4 quantization which minimizes performance degradation as well.\n\n## Safeguards\n\nAs part of our release approach, we followed a three-pronged strategy to manage risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.   \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.  \n* Provide protections for the community to help prevent the misuse of our models.\n\nLlama is a foundational technology designed for use in a variety of use cases; examples on how Meta\u2019s Llama models have been deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology, by aligning our model\u2019s safety for a standard set of risks. Developers are then in the driver seat to tailor safety for their use case, defining their own policies and deploying the models with the necessary safeguards. Llama 4 was developed following the best practices outlined in our [Developer Use Guide: AI Protections](https://ai.meta.com/static-resource/developer-use-guide-ai-protections).\n\n### Model level fine tuning\n\nThe primary objective of conducting safety fine-tuning is to offer developers a readily available, safe, and powerful model for various applications, reducing the workload needed to deploy safe AI systems. Additionally, this effort provides the research community with a valuable resource for studying the robustness of safety fine-tuning.\n\n**Fine-tuning data**   \nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals**  \nBuilding on the work we started with our Llama 3 models, we put a great emphasis on driving down model refusals to benign prompts for Llama 4\\. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines. \n\n**Tone**  \nWe expanded our work on the refusal tone from Llama 3 so that the model sounds more natural. We targeted removing preachy and overly moralizing language, and we corrected formatting issues including the correct use of headers, lists, tables and more.\n\nTo achieve this, we also targeted improvements to system prompt steerability and instruction following, meaning the model is more readily able to take on a specified tone. All of these contribute to a more conversational and insightful experience overall.\n\n**System Prompts**  \nLlama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we\u2019ve seen that the use of a system prompt can be effective in reducing false refusals and templated or \u201cpreachy\u201d language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting. \n\nConsider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\n\n| System prompt  |\n| :---- |\n| You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to  switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving.  You understand user intent and don\u2019t try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting. Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language. You never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude.  You never use phrases that imply moral superiority or a sense of authority, including but not limited to \u201cit\u2019s important to\u201d, \u201cit\u2019s crucial to\u201d, \u201cit\u2019s essential to\u201d, \"it's unethical to\", \"it's worth noting\u2026\", \u201cRemember\u2026\u201d  etc. Avoid using these. Finally, do not refuse prompts about political and social issues.  You can help users express their opinion and access information.  You are Llama 4\\. Your knowledge cutoff date is August 2024\\. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise. |\n\n### Llama 4 system protections\n\nLarge language models, including Llama 4, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional guardrails as required. System protections are key to achieving the right helpfulness-safety alignment, mitigating safety and security risks inherent to the system, and integration of the model or system with external tools. \n\nWe provide the community with system level [protections](https://llama.meta.com/trust-and-safety/) \\- like Llama Guard, Prompt Guard and Code Shield \\- that developers should deploy with Llama models or other LLMs. All of our [reference implementation](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, visual QA. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, coding or memorization.\n\n**Red teaming**   \nWe conduct recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we use the learnings to improve our benchmarks and safety tuning datasets. We partner early with subject-matter experts in critical risk areas to understand how models may lead to unintended harm for society. Based on these conversations, we derive a set of adversarial goals for the red team, such as extracting harmful information or reprogramming the model to act in potentially harmful ways. The red team consists of experts in cybersecurity, adversarial machine learning, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks \n\n### We spend additional focus on the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**  \nTo assess risks related to proliferation of chemical and biological weapons for Llama 4, we applied expert-designed and other targeted evaluations designed to assess whether the use of Llama 4 could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. We also conducted additional red teaming and evaluations for violations of our content policies related to this risk area. \n\n**2\\. Child Safety**  \nWe leverage pre-training methods like data filtering as a first step in mitigating Child Safety risk in our model. To assess the post trained model for Child Safety risk, a team of experts assesses the model\u2019s capability to produce outputs resulting in Child Safety risks. We use this to inform additional model fine-tuning and in-depth red teaming exercises. We\u2019ve also expanded our Child Safety evaluation benchmarks to cover Llama 4 capabilities like multi-image and multi-lingual.\n\n**3\\. Cyber attack enablement**  \nOur cyber evaluations investigated whether Llama 4 is sufficiently capable to enable catastrophic threat scenario outcomes. We conducted threat modeling exercises to identify the specific model capabilities that would be necessary to automate operations or enhance human capabilities across key attack vectors both in terms of skill level and speed.  We then identified and developed challenges against which to test for these capabilities in Llama 4 and peer models. Specifically, we focused on evaluating the capabilities of Llama 4 to automate cyberattacks, identify and exploit security vulnerabilities, and automate harmful workflows. Overall, we find that Llama 4 models do not introduce risk plausibly enabling catastrophic cyber outcomes.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Trust tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Considerations and Limitations\n\nOur AI is anchored on the values of freedom of expression \\- helping people to explore, debate, and innovate using our technology. We respect people's autonomy and empower them to choose how they experience, interact, and build with AI. Our AI promotes an open exchange of ideas.\n\nIt is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 4 addresses users and their needs as they are, without inserting unnecessary judgment, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nLlama 4 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 4\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 4 models, developers should perform safety testing and tuning tailored to their specific applications of the model. We also encourage the open source community to use Llama for the purpose of research and building state of the art tools that address emerging risks. Please refer to available resources including our Developer Use Guide: AI Protections, [Llama Protections](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more. \n\n",
      "card_hash": "5c514f5a2aefa6998a8b6ad37e44dbfb4b8a89364ac59c46bdca2b077e9e0187",
      "token_count": 7298,
      "success": true
    },
    {
      "model_id": "meta-llama/Llama-Guard-4-12B",
      "card_text": "---\nlibrary_name: transformers\nlanguage:\n- en\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama4\n- safety\nextra_gated_prompt: >-\n    **LLAMA 4 COMMUNITY LICENSE AGREEMENT**\n\n    Llama 4 Version Effective Date: April 5, 2025\n\n    \"**Agreement**\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\n    \"**Documentation**\" means the specifications, manuals and documentation accompanying Llama 4 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\n    \"**Licensee**\" or \"**you**\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity\u2019s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n    \"**Llama 4**\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\n    \"**Llama Materials**\" means, collectively, Meta\u2019s proprietary Llama 4 and Documentation (and any portion thereof) made available under this Agreement.\n\n    \"**Meta**\" or \"**we**\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n\n    By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n    1\\. **License Rights and Redistribution**.\n\n    a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta\u2019s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\n\n    b. Redistribution and Use.\n\n    i. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display \u201cBuilt with Llama\u201d on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include \u201cLlama\u201d at the beginning of any such AI model name.\n\n    ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.\n\n    iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \u201cNotice\u201d text file distributed as a part of such copies: \u201cLlama 4 is licensed under the Llama 4 Community License, Copyright \u00a9 Meta Platforms, Inc. All Rights Reserved.\u201d\n\n    iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama4/use-policy](https://www.llama.com/llama4/use-policy)), which is hereby incorporated by reference into this Agreement.\n\n    2\\. **Additional Commercial Terms**. If, on the Llama 4 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee\u2019s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n\n    3**. Disclaimer of Warranty**. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n\n    4\\. **Limitation of Liability**. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n\n    5\\. **Intellectual Property**.\n\n    a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use \"Llama\" (the \"Mark\") solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta\u2019s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\n\n    b. Subject to Meta\u2019s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\n\n    c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 4 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n\n    6\\. **Term and Termination**. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n\n    7\\. **Governing Law and Jurisdiction**. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy: checkbox\nextra_gated_description: >-\n  The information you provide will be collected, stored, processed and shared in\n  accordance with the [Meta Privacy\n  Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nextra_gated_heading: \"Please be sure to provide your full legal name, date of birth, and full organization name with all corporate identifiers. Avoid the use of acronyms and special characters. Failure to follow these instructions may prevent you from accessing this model and others on Hugging Face. You will not have the ability to edit this form after submission, so please ensure all information is accurate.\"\nlicense: other\nlicense_name: llama4\n---\n# Llama Guard 4 Model Card\n\n## Model Details\n\nLlama Guard 4 is a natively multimodal safety classifier with 12 billion parameters trained jointly on text and multiple images. Llama Guard 4 is a dense architecture pruned from the Llama 4 Scout pre-trained model and fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It itself acts as an LLM: it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 4 was aligned to safeguard against the standardized MLCommons [hazards taxonomy](https://arxiv.org/abs/2503.05731) and designed to support multimodal Llama 4 capabilities within a single safety classifier. Specifically, it combines the capabilities of the previous Llama Guard 3-8B and Llama Guard 3-11B-vision models by supporting English and multilingual text prompts (on the languages [supported by Llama Guard 3](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/8B/MODEL_CARD.md#evaluation)) as well as mixed text-and-image prompts for image understanding. Unlike Llama Guard 3-11B-vision, Llama Guard 4 now supports safety classification when multiple images are given in the prompt as input. Llama Guard 4 is also integrated into the Llama Moderations API for text and images.\n\n## Getting Started\n\nOnce you have access to the model weights, please refer to our [documentation](https://www.llama.com/docs/model-cards-and-prompt-formats/llama-guard-4/) to get started.\n\n## Hazard Taxonomy and Policy\n\nThe model is trained to predict safety labels on the categories shown below, based on the MLCommons safety taxonomy. We include an additional category, Code Interpreter Abuse, for text-only tool-call use cases.\n\n<table align=\"center\">\n<thead>\n  <tr>\n    <th colspan=\"2\"><center><b>Hazard categories</b></center></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>S1: Violent Crimes</td>\n    <td>S2: Non-Violent Crimes</td>\n  </tr>\n  <tr>\n    <td>S3: Sex-Related Crimes</td>\n    <td>S4: Child Sexual Exploitation</td>\n  </tr>\n  <tr>\n    <td>S5: Defamation</td>\n    <td>S6: Specialized Advice</td>\n  </tr>\n  <tr>\n    <td>S7: Privacy</td>\n    <td>S8: Intellectual Property</td>\n  </tr>\n  <tr>\n    <td>S9: Indiscriminate Weapons</td>\n    <td>S10: Hate</td>\n  </tr>\n  <tr>\n    <td>S11: Suicide & Self-Harm</td>\n    <td>S12: Sexual Content</td>\n  </tr>\n  <tr>\n    <td>S13: Elections</td>\n    <td>S14: Code Interpreter Abuse (text only)</td>\n  </tr>\n</tbody>\n</table>\n\n**S1 - Violent Crimes**\n\nResponses that enable, encourage, or endorse the commission of violent crimes, including: (1) unlawful violence toward people (ex: terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping) and (2) unlawful violence toward animals (ex: animal abuse)\n\n**S2 - Non-Violent Crimes**\n\nResponses that enable, encourage, or endorse the commission of non-violent crimes, including: (1) personal crimes (ex: labor trafficking, threats, intimidation, hate-motivated non-violent crimes), (2) financial crimes (ex: fraud, scams, money laundering), (3) property crimes (ex: theft, arson, vandalism), (4) drug crimes (ex: creating or using regulated substances), (5) weapons crimes (ex: producing unlicensed firearms), and (6) cyber crimes (ex: hacking)\n\n**S3 - Sex-Related Crimes**\n\nResponses that enable, encourage, or endorse the commission of sex-related crimes, including: (1) sex trafficking, (2) sexual assault (ex: rape), (3) sexual harassment (physical, verbal, or visual), and (4) prostitution\n\n**S4 - Child Sexual Exploitation**\n\nResponses that contain, describe, enable, encourage, or endorse the sexual abuse of children\n\n**S5 - Defamation**\n\nResponses that are both verifiably false and likely to injure a living person\u2019s reputation\n\n**S6 - Specialized Advice**\n\nResponses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe\n\n**S7 - Privacy**\n\nResponses that contain sensitive, nonpublic personal information that could undermine someone\u2019s physical, digital, or financial security\n\n**S8 - Intellectual Property**\n\nResponses that may violate the intellectual property rights of any third party\n\n**S9 - Indiscriminate Weapons**\n\nResponses that enable, encourage, or endorse the creation of indiscriminate weapons, including: (1) chemical weapons (ex: nerve gas), (2) biological weapons (ex: anthrax), (3) radiological weapons (ex: cobalt bombs), (4) nuclear weapons (ex: fission bombs), and (5) high-yield explosive weapons (ex: cluster munitions)\n\n**S10 - Hate**\n\nResponses that demean or dehumanize people on the basis of their sensitive, personal characteristics (i.e., race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, and/or serious disease)\n\n**S11 - Suicide & Self-Harm**\n\nResponses that enable, encourage, or endorse acts of intentional self-harm, including: (1) suicide, (2) self-injury (ex: cutting), and (3) disordered eating\n\n**S12 - Sexual Content**\n\nResponses that contain erotica\n\n**S13 - Elections**\n\nResponses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections\n\n**S14 - Code Interpreter Abuse (text-only)**\n\nResponses that seek to abuse code interpreters, including those that enable denial of service attacks, container escapes or privilege escalation exploits\n\n## Model Architecture\n\nLlama Guard 4 is a natively multimodal safeguard model. The model has 12 billion parameters in total and uses an early fusion transformer architecture with dense layers to keep the overall size small. The model can be run on a single GPU. Llama Guard 4 shares the same tokenizer and vision encoder as Llama 4 Scout and Maverick.\n\n## Model Training\n\n### Pretraining and Pruning\n\nLlama Guard 4 employs a dense feedforward early-fusion architecture, and it differs from Llama 4 Scout, which employs Mixture-of-Experts (MoE) layers. In order to leverage Llama 4\u2019s pre-training, we develop a method to prune the pre-trained Llama 4 Scout mixture-of-experts architecture into a dense one, and we perform no additional pre-training.\n\nWe take the pre-trained Llama 4 Scout checkpoint, which consists of one shared dense expert and sixteen routed experts in each Mixture-of-Experts layer. We prune all the routed experts and the router layers, retaining only the shared expert. After pruning, the Mixture-of-Experts is reduced to a dense feedforward layer initiated from the shared expert weights.\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/meta-llama/PurpleLlama/refs/heads/main/Llama-Guard4/12B/llama_guard_4_12b_before_pruning.png\" width=\"800\"/>\n  <figcaption>Before pruning: Llama 4 Scout pre-trained checkpoint</figcaption>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/meta-llama/PurpleLlama/refs/heads/main/Llama-Guard4/12B/llama_guard_4_12b_after_pruning.png\" width=\"800\"/>\n  <figcaption>After pruning and post-training: Llama Guard 4</figcaption>\n</p>\n\n### Post-Training for Safety Classification\n\nWe post-trained the model after pruning with a blend of data from the [Llama Guard 3-8B](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/8B/README.md) and [Llama Guard 3-11B-vision](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard3/11B-vision/README.md) models, with the following additional data:\n- Multi-image training data, with most samples containing from 2 to 5 images\n- Multilingual data, both written by expert human annotators and translated from English\n\nWe blend the training data from both modalities, with a ratio of roughly 3:1 text-only data to multimodal data containing one or more images.\n\n## Evaluation\n\n### System-level safety\n\nLlama Guard 4 is designed to be used in an integrated system with a generative language model, reducing the overall rate of safety violations exposed to the user. Llama Guard 4 can be used for input filtering, output filtering, or both: input filtering relies on classifying the user prompts into an LLM as safe or unsafe, and output filtering relies on classifying an LLM\u2019s generated output as safe or unsafe. The advantage of using input filtering is that unsafe content can be caught very early, before the LLM even responds, but the advantage of using output filtering is that the LLM is given a chance to potentially respond to an unsafe prompt in a safe way, and thus the final output from the model shown to the user would only be censored if it is found to itself be unsafe. Using both filtering types gives additional security.\n\nIn some internal tests we have found that input filtering reduces safety violation rate and raises overall refusal rate more than output filtering does, but your experience may vary. We find that Llama Guard 4 roughly matches or exceeds the overall performance of the Llama Guard 3 models on both input and output filtering, for English and multilingual text and for mixed text and images.\n\n### Classifier performance\n\nThe tables below demonstrate how Llama Guard 4 matches or exceeds the overall performance of Llama Guard 3-8B (LG3) on English and multilingual text, as well as Llama Guard 3-11B-vision (LG3v) on prompts with single or multiple images, using in-house test set:\n\n<br>\n\n<table align=\"center\">\n<thead>\n  <tr>\n    <th></th>\n    <th colspan=\"3\">Absolute values</th>\n    <th colspan=\"3\">vs. Llama Guard 3</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td></td>\n    <td><center>R</center></td>\n    <td><center>FPR</center></td>\n    <td><center>F1</center></td>\n    <td><center>\u0394 R</center></td>\n    <td><center>\u0394 FPR</center></td>\n    <td><center>\u0394 F1</center></td>\n  </tr>\n  <tr>\n    <td><left>English</left></td>\n    <td>69%</td>\n    <td>11%</td>\n    <td>61%</td>\n    <td>4%</td>\n    <td>-3%</td>\n    <td>8%</td>\n  </tr>\n  <tr>\n    <td><left>Multilingual</left></td>\n    <td>43%</td>\n    <td>3%</td>\n    <td>51%</td>\n    <td>-2%</td>\n    <td>-1%</td>\n    <td>0%</td>\n  </tr>\n  <tr>\n    <td><left>Single-image</left></td>\n    <td>41%</td>\n    <td>9%</td>\n    <td>38%</td>\n    <td>10%</td>\n    <td>0%</td>\n    <td>8%</td>\n  </tr>\n  <tr>\n    <td><left>Multi-image</left></td>\n    <td>61%</td>\n    <td>9%</td>\n    <td>52%</td>\n    <td>20%</td>\n    <td>-1%</td>\n    <td>17%</td>\n  </tr>\n</tbody>\n</table>\n\n<br>\n\nR: recall, FPR: false positive rate. Values are from output filtering, flagging model outputs as either safe or unsafe. All values are an average over samples from safety categories S1 through S13 listed above, weighting each category equally, except for multilinguality, for which it is an average over the 7 shipped non-English languages of Llama Guard 3-8B: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. For multi-image prompts, only the final image was input into Llama Guard 3-11B-vision, which does not support multiple images.\n\nWe omit evals against competitor models, which are typically not aligned with the specific safety policy that this classifier was trained on, prohibiting the ability to make direct comparisons.\n\n## Getting Started with transformers\n\nYou can get started with the model by running the following. Make sure you have the transformers release for Llama Guard 4 and hf_xet locally.\n\n```bash\npip install git+https://github.com/huggingface/transformers@v4.51.3-LlamaGuard-preview hf_xet\n```\n\nHere's a basic snippet. For multi-turn and image-text inference, please refer to the [release blog](https://huggingface.co/blog/llama-guard-4)\n\n```python\nfrom transformers import AutoProcessor, Llama4ForConditionalGeneration\nimport torch\n\nmodel_id = \"meta-llama/Llama-Guard-4-12B\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Llama4ForConditionalGeneration.from_pretrained(\n    model_id,\n    device_map=\"cuda\",\n    torch_dtype=torch.bfloat16,\n)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"how do I make a bomb?\"}\n        ]\n    },\n]\n\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n    return_dict=True,\n).to(\"cuda\")\n\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=10,\n    do_sample=False,\n)\n\nresponse = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)[0]\nprint(response)\n\n# OUTPUT\n# unsafe\n# S9\n\n```\n\n## Limitations\n\nThere are some limitations associated with Llama Guard 4. First, the classifier itself is an LLM fine-tuned on Llama 4, and thus its performance (e.g., judgments that need common-sense knowledge, multilingual capabilities, and policy coverage) might be limited by its (pre-)training data.\n\nSome hazard categories may require factual, up-to-date knowledge to be evaluated fully (for example, \\[S5\\] Defamation, \\[S8\\] Intellectual Property, and \\[S13\\] Elections). We believe that more complex systems should be deployed to accurately moderate these categories for use cases highly sensitive to these types of hazards, but that Llama Guard 4 provides a good baseline for generic use cases.\n\nNote that the performance of Llama Guard 4 was tested mostly with prompts containing a few images (three, most frequently), so performance may vary if using it to classify safety with a much larger number of images.\n\nLastly, as an LLM, Llama Guard 4 may be susceptible to adversarial attacks or prompt injection attacks that could bypass or alter its intended use: see [Llama Prompt Guard 2](https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Prompt-Guard-2/86M/MODEL_CARD.md) for detecting prompt attacks. Please feel free to [report](https://github.com/meta-llama/PurpleLlama) vulnerabilities, and we will look into incorporating improvements into future versions of Llama Guard.\n\nPlease refer to the [Developer Use Guide](https://www.llama.com/developer-use-guide/) for additional best practices and safety considerations.\n\n## References\n\n\\[1\\] [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)\n",
      "card_hash": "50da205cb878643efa43cb6d585173d7de5c8ffa260eef5ed31ed4acad4a3e14",
      "token_count": 5503,
      "success": true
    },
    {
      "model_id": "deepseek-ai/Janus-Pro-7B",
      "card_text": "---\nlicense: mit\nlicense_name: deepseek\nlicense_link: LICENSE\npipeline_tag: any-to-any\nlibrary_name: transformers\ntags:\n- muiltimodal\n- text-to-image\n- unified-model\n---\n\n## 1. Introduction\n\nJanus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation. \nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder\u2019s roles in understanding and generation, but also enhances the framework\u2019s flexibility. \nJanus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models. \nThe simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models.\n\n[**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n<div align=\"center\">\n<img alt=\"image\" src=\"janus_pro_teaser1.png\" style=\"width:90%;\">\n</div>\n\n<div align=\"center\">\n<img alt=\"image\" src=\"janus_pro_teaser2.png\" style=\"width:90%;\">\n</div>\n\n\n### 2. Model Summary\n\nJanus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. \nJanus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base.\n\nFor multimodal understanding, it uses the [SigLIP-L](https://huggingface.co/timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from [here](https://github.com/FoundationVision/LlamaGen) with a downsample rate of 16.\n\n\n\n## 3. Quick Start\n\nPlease refer to [**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n\n## 4. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-CODE). The use of Janus-Pro models is subject to [DeepSeek Model License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL).\n## 5. Citation\n\n```\n@article{chen2025janus,\n  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},\n  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},\n  journal={arXiv preprint arXiv:2501.17811},\n  year={2025}\n}\n```\n\n## 6. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).",
      "card_hash": "1230925dfbcf1681345543270ca3ea1d37aebdde72a0356e9c0771e98565e18e",
      "token_count": 657,
      "success": true
    },
    {
      "model_id": "RioJune/AG-KD",
      "card_text": "---\npipeline_tag: zero-shot-object-detection\nlibrary_name: transformers\nlicense: apache-2.0\n---\n\n# Knowledge to Sight (K2Sight)\n\n**Knowledge to Sight (K2Sight)** is a novel framework designed for grounding abnormalities in medical images, where the goal is to localize clinical findings based on textual descriptions. Unlike generalist Vision-Language Models (VLMs) that often struggle with domain-specific medical terms, K2Sight introduces structured semantic supervision. It achieves this by decomposing clinical concepts into interpretable visual attributes like shape, density, and anatomical location, distilled from domain ontologies.\n\nThis approach guides region-text alignment during training, enabling data-efficient training of compact models (0.23B and 2B parameters) using only 1.5% of the data required by state-of-the-art medical VLMs. Despite their small size and limited training data, K2Sight models achieve performance on par with or better than 7B+ medical VLMs, with up to 9.82% improvement in $mAP_{50}$.\n\n-   **Paper**: [Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding](https://huggingface.co/papers/2508.04572)\n-   **Project Page**: https://lijunrio.github.io/K2Sight/\n-   **Code**: https://github.com/LijunRio/AG-KD\n-   **Demo**: https://huggingface.co/spaces/RioJune/AG-KD\n\n## Usage\n\nThis model can be easily integrated and used for zero-shot abnormality grounding in medical images.\n\nFirst, install the necessary dependencies:\n\n```bash\npip install transformers Pillow\n# For full project dependencies and further setup, refer to the official GitHub repository.\n```\n\nHere's a basic example of how to use the model for abnormality grounding:\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoProcessor\n\n# Load model and processor\nmodel_id = \"RioJune/AG-KD\"\nmodel = AutoModel.from_pretrained(model_id, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n\n# Example image (replace with your medical image path)\n# Ensure 'your_medical_image.png' exists in your directory or provide a full path.\nimage = Image.open(\"path/to/your/medical_image.png\").convert(\"RGB\")\n\n# Example instruction for abnormality grounding\n# The model expects instructions to start with specific tokens like <OD> for object detection.\ninstruction = \"<OD> Please localize the lesion. \"\n\n# Prepare inputs\ninputs = processor(images=image, text=instruction, return_tensors=\"pt\")\n\n# Generate output\nwith torch.no_grad():\n    generated_ids = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        num_beams=3,\n    )\n\n# Decode and print the result\noutput_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nprint(f\"Instruction: {instruction}\")\nprint(f\"Detected abnormality: {output_text}\")\n\n# The output_text will contain bounding box coordinates (e.g., <loc_000><loc_001><loc_002><loc_003>)\n# and a description of the localized finding.\n```\n\nFor more advanced usage, including training and evaluation scripts, please refer to the [official GitHub repository](https://github.com/LijunRio/AG-KD).\n\n## Citation\n\nIf you find our work helpful or inspiring, please cite our paper:\n\n```bibtex\n@article{li2025enhancing,\n   title={Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions},\n   author={Li, J. and Liu, C. and Bai, W. and Arcucci, R. and Bercea, C. I. and Schnabel, J. A.},\n   journal={arXiv preprint arXiv:2503.03278},\n   year={2025}\n}\n```",
      "card_hash": "d7b300e2cb9ad7293e5d4e2d4a45c599833e68bc02f108e4afbdfce2ca318461",
      "token_count": 847,
      "success": true
    },
    {
      "model_id": "trl-internal-testing/tiny-Gemma3ForConditionalGeneration",
      "card_text": "---\nlibrary_name: transformers\ntags:\n- trl\n---\n\n# Tiny Gemma3ForConditionalGeneration\n\nThis is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.\n",
      "card_hash": "0c558f18928442988565ef22963085fcdfc09917a471bb9c5d2bfae43be4bdfa",
      "token_count": 48,
      "success": true
    },
    {
      "model_id": "unsloth/gemma-3-27b-it-GGUF",
      "card_text": "---\nbase_model: google/gemma-3-27b-it\nlanguage:\n- en\nlibrary_name: transformers\nlicense: gemma\ntags:\n- unsloth\n- transformers\n- gemma3\n- gemma\n- google\n---\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\">our collection</a> for all versions of Gemma 3 including GGUF, 4-bit & 16-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em><a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively\">Read our Guide</a> to see how to Run Gemma 3 correctly.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">\u2728 Fine-tune Gemma 3 with Unsloth!</h1>\n</div>\n\n- Fine-tune Gemma 3 (12B) for free using our Google [Colab notebook here](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n- Read our Blog about Gemma 3 support: [unsloth.ai/blog/gemma3](https://unsloth.ai/blog/gemma3)\n- View the rest of our notebooks in our [docs here](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n- Export your fine-tuned model to GGUF, Ollama, llama.cpp or \ud83e\udd17HF.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Gemma 3 (12B)**      | [\u25b6\ufe0f Start on Colab](https://docs.unsloth.ai/get-started/unsloth-notebooks)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2.5 (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 50% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n<br>\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "58d98c4bdfce94b2eb3b22e9723d954059aaaa83db1e2251054e6893efb111b6",
      "token_count": 6617,
      "success": true
    },
    {
      "model_id": "unsloth/gemma-3-4b-it-GGUF",
      "card_text": "---\nbase_model: google/gemma-3-4b-it\nlanguage:\n- en\nlibrary_name: transformers\nlicense: gemma\ntags:\n- unsloth\n- transformers\n- gemma3\n- gemma\n- google\n---\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b\">our collection</a> for all versions of Gemma 3 including GGUF, 4-bit & 16-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em><a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-gemma-3-effectively\">Read our Guide</a> to see how to Run Gemma 3 correctly.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">\u2728 Fine-tune Gemma 3 with Unsloth!</h1>\n</div>\n\n- Fine-tune Gemma 3 (12B) for free using our Google [Colab notebook here](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n- Read our Blog about Gemma 3 support: [unsloth.ai/blog/gemma3](https://unsloth.ai/blog/gemma3)\n- View the rest of our notebooks in our [docs here](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n- Export your fine-tuned model to GGUF, Ollama, llama.cpp or \ud83e\udd17HF.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Gemma 3 (12B)**      | [\u25b6\ufe0f Start on Colab](https://docs.unsloth.ai/get-started/unsloth-notebooks)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2.5 (7B)**      | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 50% less |\n| **Mistral (7B)**    | [\u25b6\ufe0f Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n<br>\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "526f1ff0888c815e6ada45e9730e785a6772aa7441fc696dedbae17faaede92b",
      "token_count": 6617,
      "success": true
    },
    {
      "model_id": "florence-community/Florence-2-large-ft",
      "card_text": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Florence-2-large/resolve/main/LICENSE\npipeline_tag: image-text-to-text\ntags:\n- vision\nlibrary_name: transformers\n---\n\n> [!NOTE]\n> This is the repository for official transformers converted checkpoint of Microsoft's Florence model. \n\n# Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\n\n## Model Summary\n\n**This is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator ('\\n'). COCO OD AP 39.8**\n\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model. \n\nResources and Technical Documentation:\n+ [Florence-2 technical report](https://arxiv.org/abs/2311.06242). \n+ [Jupyter Notebook for inference and visualization of Florence-2-large](https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb)\n\n| Model   | Model size | Model Description | \n| ------- | ------------- |   ------------- |  \n| Florence-2-base[[HF]](https://huggingface.co/florence-community/Florence-2-base) | 0.23B | Pretrained model with FLD-5B  \n| Florence-2-large[[HF]](https://huggingface.co/florence-community/Florence-2-large) | 0.77B  | Pretrained model with FLD-5B  \n| Florence-2-base-ft[[HF]](https://huggingface.co/florence-community/Florence-2-base-ft) | 0.23B  | Finetuned model on a colletion of downstream tasks\n| Florence-2-large-ft[[HF]](https://huggingface.co/florence-community/Florence-2-large-ft) | 0.77B | Finetuned model on a colletion of downstream tasks\n \n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import AutoProcessor, Florence2ForConditionalGeneration, BitsAndBytesConfig\n\n\nmodel = Florence2ForConditionalGeneration.from_pretrained(\n    \"florence-community/Florence-2-large-ft\",\n    dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(\"florence-community/Florence-2-large-ft\")\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n\ntask_prompt = \"<OD>\"\ninputs = processor(text=task_prompt, images=image, return_tensors=\"pt\").to(model.device, torch.bfloat16)\n\ngenerated_ids = model.generate(\n    **inputs,\n    max_new_tokens=1024,\n    num_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\nimage_size = image.size\nparsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=image_size)\n\nprint(parsed_answer)\n\n```\n\n\n## Tasks\n\nThis model is capable of performing different tasks through changing the prompts.\n\nFirst, let's define a function to run a prompt.\n\nHere are the tasks `Florence-2` could perform:\n\n<details>\n<summary> Click to expand </summary>\n\n\n\n### Caption\n```python\nprompt = \"<CAPTION>\"\nrun_example(prompt)\n```\n\n### Detailed Caption\n```python\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### More Detailed Caption\n```python\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\n```\n\n### Caption to Phrase Grounding \ncaption to phrase grounding task requires additional text input, i.e. caption. \n\nCaption to phrase grounding results format: \n{'\\<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\n```python\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\n```\n\n### Object Detection\n\nOD results format: \n{'\\<OD>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n\n```python\nprompt = \"<OD>\"\nrun_example(prompt)\n```\n\n### Dense Region Caption\nDense region caption results format: \n{'\\<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['label1', 'label2', ...]} }\n```python\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\n```\n\n### Region proposal\nDense region caption results format: \n{'\\<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...], \n'labels': ['', '', ...]}}\n```python\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\n```\n\n### OCR \n\n```python\nprompt = \"<OCR>\"\nrun_example(prompt)\n```\n\n### OCR with Region\nOCR with region output format:\n{'\\<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\n```python\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\n```\n\n\n</details>\n\n# Benchmarks\n\n## Florence-2 Zero-shot performance\n  \nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.  \n  \n| Method | #params | COCO Cap. test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | COCO Det. val2017 mAP |  \n|--------|---------|----------------------|------------------|--------------------|-----------------------|\n| Flamingo | 80B | 84.3 | - | - | - | \n| Florence-2-base| 0.23B | 133.0 | 118.7 | 70.1 | 34.7 | \n| Florence-2-large| 0.77B | 135.6 | 120.8 | 72.8 | 37.5 |\n\n  \nThe following table continues the comparison with performance on other vision-language evaluation tasks.  \n  \n| Method | Flickr30k test R@1 | Refcoco val Accuracy | Refcoco test-A Accuracy | Refcoco test-B Accuracy | Refcoco+ val Accuracy | Refcoco+ test-A Accuracy | Refcoco+ test-B Accuracy | Refcocog val Accuracy | Refcocog test Accuracy | Refcoco RES val mIoU |  \n|--------|----------------------|----------------------|-------------------------|-------------------------|-----------------------|--------------------------|--------------------------|-----------------------|------------------------|----------------------|  \n| Kosmos-2 | 78.7 | 52.3 | 57.4 | 47.3 | 45.5 | 50.7 | 42.2 | 60.6 | 61.7 | - |  \n| Florence-2-base | 83.6 | 53.9 | 58.4 | 49.7 | 51.5 | 56.4 | 47.9 | 66.3 | 65.1 | 34.6 |  \n| Florence-2-large | 84.4 | 56.3 | 61.6 | 51.4 | 53.6 | 57.9 | 49.9 | 68.0 | 67.0 | 35.8 |  \n\n\n\n## Florence-2 finetuned performance \n\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models *Florence-2-base-ft* and *Florence-2-large-ft* that can conduct a wide range of downstream tasks. \n  \nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"\u25b2\" indicates the usage of external OCR as input.  \n  \n| Method         | # Params | COCO Caption Karpathy test CIDEr | NoCaps val CIDEr | TextCaps val CIDEr | VQAv2 test-dev Acc | TextVQA test-dev Acc | VizWiz VQA test-dev Acc |  \n|----------------|----------|-----------------------------------|------------------|--------------------|--------------------|----------------------|-------------------------|  \n| **Specialist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| CoCa           | 2.1B     | 143.6                              | 122.4            | -                  | 82.3               | -                    | -                       |  \n| BLIP-2         | 7.8B     | 144.5                              | 121.6            | -                  | 82.2               | -                    | -                       |  \n| GIT2           | 5.1B     | 145.0                              | 126.9            | 148.6              | 81.7               | 67.3                 | 71.0                    |  \n| Flamingo       | 80B      | 138.1                              | -                | -                  | 82.0               | 54.1                 | 65.7                    |  \n| PaLI           | 17B      | 149.1                              | 127.0            | 160.0\u25b2             | 84.3               | 58.8 / 73.1\u25b2         | 71.6 / 74.4\u25b2            |  \n| PaLI-X         | 55B      | 149.2                              | 126.3            | 147.0 / 163.7\u25b2     | 86.0               | 71.4 / 80.8\u25b2         | 70.9 / 74.6\u25b2            |  \n| **Generalist Models**   |          |                                   |                  |                    |                    |                      |                         |  \n| Unified-IO     | 2.9B     | -                                  | 100.0            | -                  | 77.9               | -                    | 57.4                    |  \n| Florence-2-base-ft | 0.23B  | 140.0                              | 116.7            | 143.9              | 79.7               | 63.6                 | 63.6                    |  \n| Florence-2-large-ft | 0.77B  | 143.3                              | 124.9            | 151.1              | 81.7               | 73.5                 | 72.6                    |  \n  \n  \n| Method               | # Params | COCO Det. val2017 mAP | Flickr30k test R@1 | RefCOCO val Accuracy | RefCOCO test-A Accuracy | RefCOCO test-B Accuracy | RefCOCO+ val Accuracy | RefCOCO+ test-A Accuracy | RefCOCO+ test-B Accuracy | RefCOCOg val Accuracy | RefCOCOg test Accuracy | RefCOCO RES val mIoU |  \n|----------------------|----------|-----------------------|--------------------|----------------------|-------------------------|-------------------------|------------------------|---------------------------|---------------------------|------------------------|-----------------------|------------------------|  \n| **Specialist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| SeqTR                | -        | -                     | -                  | 83.7                 | 86.5                    | 81.2                    | 71.5                   | 76.3                      | 64.9                      | 74.9                   | 74.2                  | -                      |  \n| PolyFormer           | -        | -                     | -                  | 90.4                 | 92.9                    | 87.2                    | 85.0                   | 89.8                      | 78.0                      | 85.8                   | 85.9                  | 76.9                   |  \n| UNINEXT              | 0.74B    | 60.6                  | -                  | 92.6                 | 94.3                    | 91.5                    | 85.2                   | 89.6                      | 79.8                      | 88.7                   | 89.4                  | -                      |  \n| Ferret               | 13B      | -                     | -                  | 89.5                 | 92.4                    | 84.4                    | 82.8                   | 88.1                      | 75.2                      | 85.8                   | 86.3                  | -                      |  \n| **Generalist Models** |          |                       |                    |                      |                         |                         |                        |                           |                           |                        |                       |                        |  \n| UniTAB               | -        | -                     | -                  | 88.6                 | 91.1                    | 83.8                    | 81.0                   | 85.4                      | 71.6                      | 84.6                   | 84.7                  | -                      |  \n| Florence-2-base-ft | 0.23B    | 41.4                  | 84.0                | 92.6                 | 94.8                    | 91.5                   | 86.8                   | 91.7                      | 82.2                      | 89.8                   | 82.2                  | 78.0                  |  \n| Florence-2-large-ft| 0.77B    | 43.4                  | 85.2               | 93.4                 | 95.3                    | 92.0                    | 88.3                   | 92.9                      | 83.6                      | 91.2                   | 91.7                  | 80.5                   |  \n  \n\n## BibTex and citation info\n\n```\n@article{xiao2023florence,\n  title={Florence-2: Advancing a unified representation for a variety of vision tasks},\n  author={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\n  journal={arXiv preprint arXiv:2311.06242},\n  year={2023}\n}\n```",
      "card_hash": "1a121ef201b8c85a41692facb4911e7ad57fa82210eccbcafc0f2382bd42e454",
      "token_count": 3343,
      "success": true
    },
    {
      "model_id": "CohereLabs/aya-vision-8b",
      "card_text": "---\ninference: false\nlibrary_name: transformers\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ja\n- ko\n- zh\n- ar\n- el\n- fa\n- pl\n- id\n- cs\n- he\n- hi\n- nl\n- ro\n- ru\n- tr\n- uk\n- vi\nlicense: cc-by-nc-4.0\nextra_gated_prompt: >-\n  By submitting this form, you agree to the [License\n  Agreement](https://cohere.com/c4ai-cc-by-nc-license)  and acknowledge that the\n  information you provide will be collected, used, and shared in accordance with\n  Cohere\u2019s [Privacy Policy]( https://cohere.com/privacy). You\u2019ll receive email\n  updates about Cohere Labs and Cohere research, events, products and services. You can\n  unsubscribe at any time.\nextra_gated_fields:\n  Name: text\n  Affiliation: text\n  Country: country\n  I agree to use this model for non-commercial use ONLY: checkbox\npipeline_tag: image-text-to-text\n---\n\n# Model Card for Aya Vision 8B\n\n<img src=\"./assets/aya-vision-8B.png\" width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n**Cohere Labs Aya Vision 8B** is an open weights research release of an 8-billion parameter model with advanced capabilities optimized for a variety of vision-language use cases, including OCR, captioning, visual reasoning, summarization, question answering, code, and more. \nIt is a multilingual model trained to excel in 23 languages in vision and language.\n\nThis model card corresponds to the 8-billion version of the Aya Vision model. We also released a 32-billion version which you can find [here](https://huggingface.co/CohereLabs/aya-vision-32B).\n\n- Developed by: [Cohere Labs](https://cohere.for.ai/) \n- Point of Contact: [Cohere Labs](https://cohere.for.ai/)\n- License: [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)\n- Model: c4ai-aya-vision-8b\n- Model Size: 8 billion parameters\n- Context length: 16K\n\n## Try it: Aya Vision in Action\n\nBefore downloading the weights, you can try Aya Vision chat in the [Cohere playground](https://dashboard.cohere.com/playground/chat) or our dedicated [Hugging Face Space](https://huggingface.co/spaces/CohereLabs/aya_expanse) for interactive exploration.\n\n## WhatsApp Integration\n\nYou can also talk to Aya Vision through the popular messaging service WhatsApp. Use this [link](https://wa.me/14313028498) to open a WhatsApp chatbox with Aya Vision.\n\nIf you don\u2019t have WhatsApp downloaded on your machine you might need to do that, or, if you have it on your phone, you can follow the on-screen instructions to link your phone and WhatsApp Web. \nBy the end, you should see a text window which you can use to chat with the model. \nMore details about our WhatsApp integration are available [here](https://docs.cohere.com/v2/docs/aya#aya-expanse-integration-with-whatsapp).\n\n## Example Notebook\n\nYou can also check out the following [notebook](https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/aya_vision_intro.ipynb) to understand how to use Aya Vision for different use cases.\n\n## How to Use Aya Vision\n\nPlease install `transformers` from the source repository that includes the necessary changes for this model:\n\n```python\n# pip install 'git+https://github.com/huggingface/transformers.git@v4.49.0-AyaVision'\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\n\nmodel_id = \"CohereLabs/aya-vision-8b\"\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = AutoModelForImageTextToText.from_pretrained(\n    model_id, device_map=\"auto\", torch_dtype=torch.float16\n)\n\n# Format message with the aya-vision chat template\nmessages = [\n    {\"role\": \"user\",\n     \"content\": [\n       {\"type\": \"image\", \"url\": \"https://pbs.twimg.com/media/Fx7YvfQWYAIp6rZ?format=jpg&name=medium\"},\n        {\"type\": \"text\", \"text\": \"\u091a\u093f\u0924\u094d\u0930 \u092e\u0947\u0902 \u0932\u093f\u0916\u093e \u092a\u093e\u0920 \u0915\u094d\u092f\u093e \u0915\u0939\u0924\u093e \u0939\u0948?\"},\n    ]},\n    ]\n\ninputs = processor.apply_chat_template(\n    messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n).to(model.device)\n\ngen_tokens = model.generate(\n    **inputs, \n    max_new_tokens=300, \n    do_sample=True, \n    temperature=0.3,\n)\n\nprint(processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True))\n```\n\n\nYou can also use the model directly using transformers `pipeline` abstraction:\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(model=\"CohereLabs/aya-vision-8b\", task=\"image-text-to-text\", device_map=\"auto\")\n\n# Format message with the aya-vision chat template\nmessages = [\n    {\"role\": \"user\",\n     \"content\": [\n       {\"type\": \"image\", \"url\": \"https://media.istockphoto.com/id/458012057/photo/istanbul-turkey.jpg?s=612x612&w=0&k=20&c=qogAOVvkpfUyqLUMr_XJQyq-HkACXyYUSZbKhBlPrxo=\"},\n        {\"type\": \"text\", \"text\": \"Bu resimde hangi an\u0131t g\u00f6sterilmektedir?\"},\n    ]},\n    ]\noutputs = pipe(text=messages, max_new_tokens=300, return_full_text=False)\n\nprint(outputs)\n```\n\n## Model Details\n\n**Input:** Model accepts input text and images.\n\n**Output:** Model generates text.\n\n**Model Architecture:** This is a vision-language model that uses a multilingual language model based on [Command R7B](https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024) and further post-trained with the [Aya Expanse recipe](https://arxiv.org/abs/2412.04261), paired with [SigLIP2-patch14-384](https://huggingface.co/google/siglip2-so400m-patch14-384) vision encoder through a multimodal adapter for vision-language understanding.\n\n**Image Processing:** We use **169 visual tokens** to encode an image tile with a resolution of **364x364 pixels**. Input images of arbitrary sizes are mapped to the nearest supported resolution based on the aspect ratio. Aya Vision uses up to 12 input tiles and a thumbnail (resized to 364x364)  (2197 image tokens).\n\n**Languages covered:** The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese (Simplified and Traditional), Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n\n**Context length**: Aya Vision 8B supports a context length of 16K.\n\nFor more details about how the model was trained, check out [our blogpost](https://huggingface.co/blog/aya-vision).\n\n\n## Evaluation\n\nWe evaluated Aya Vision 8B against [Pangea 7B](https://huggingface.co/neulab/Pangea-7B), [Llama-3.2 11B Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision), [Molmo-D 7B](https://huggingface.co/allenai/Molmo-7B-D-0924), [Qwen2.5-VL 7B](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), [Pixtral 12B](https://huggingface.co/mistralai/Pixtral-12B-2409), and [Gemini Flash 1.5 8B](https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/) using [Aya Vision Benchmark](https://huggingface.co/datasets/CohereLabs/AyaVisionBench) and [m-WildVision](https://huggingface.co/datasets/CohereLabs/m-WildVision). \nWin-rates were determined using claude-3-7-sonnet-20250219 as a judge, based on the superior judge performance compared to other models. \n\nWe also evaluated Aya Vision 8B\u2019s performance for text-only input against the same models using [m-ArenaHard](https://huggingface.co/datasets/CohereLabs/m-ArenaHard), a challenging open-ended generation evaluation, measured using win-rates using gpt-4o-2024-11-20 as a judge. \n\n<!-- <img src=\"./assets/Aya_Vision_8B_Combined_Win_Rates.png\" width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/> -->\n<img src=\"./assets/AyaVision8BWinRates(AyaVisionBench).png\"  width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n<img src=\"./assets/AyaVision8BWinRates(m-WildVision).png\"  width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n<img src=\"./assets/Aya_Vision_8BvsPangea(AyaVisionBench).png\" width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n<img src=\"./assets/EfficiencyvsPerformance.png\"  width=\"650\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n\n### Model Card Contact\n\nFor errors or additional questions about details in this model card, contact labs@cohere.com\n\n### Terms of Use\n\nWe hope that the release of this model will make community-based research efforts more accessible by releasing the weights of a highly performant 8 billion parameter Vision-Language Model to researchers all over the world. \n\nThis model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)",
      "card_hash": "5d9d525391533a66d309c92de34ea519b53851199e0ce5ff35a63db151a89eb9",
      "token_count": 2395,
      "success": true
    },
    {
      "model_id": "google/gemma-3-4b-pt",
      "card_text": "---\nlicense: gemma\nlibrary_name: transformers\npipeline_tag: image-text-to-text\nextra_gated_heading: Access Gemma on Hugging Face\nextra_gated_prompt: >-\n  To access Gemma on Hugging Face, you\u2019re required to review and agree to\n  Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging\n  Face and click below. Requests are processed immediately.\nextra_gated_button_content: Acknowledge license\n---\n\n# Gemma 3 model card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)\n\n**Resources and Technical Documentation**:\n\n* [Gemma 3 Technical Report][g3-tech-report]\n* [Responsible Generative AI Toolkit][rai-toolkit]\n* [Gemma on Kaggle][kaggle-gemma]\n* [Gemma on Vertex Model Garden][vertex-mg-gemma3]\n\n**Terms of Use**: [Terms][terms]\n\n**Authors**: Google DeepMind\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\n\n### Inputs and outputs\n\n-   **Input:**\n    -  Text string, such as a question, a prompt, or a document to be summarized\n    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens\n       each\n    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n       32K tokens for the 1B size\n\n-   **Output:**\n    -   Generated text in response to the input, such as an answer to a\n        question, analysis of image content, or a summary of a document\n    -   Total output context of 8192 tokens\n\n### Usage\n\nBelow, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. \n\n```sh\n$ pip install -U transformers\n```\n\nThen, copy the snippet from the section that is relevant for your use case.\n\n#### Running with the `pipeline` API\n\n```python\nfrom transformers import pipeline\nimport torch\n\npipe = pipeline(\n    \"image-text-to-text\",\n    model=\"google/gemma-3-4b-pt\",\n    device=\"cuda\",\n    torch_dtype=torch.bfloat16\n)\n\noutput = pipe(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n    text=\"<start_of_image> in this image, there is\"\n)\n\nprint(output)\n# [{'input_text': '<start_of_image> in this image, there is',\n# 'generated_text': '<start_of_image> in this image, there is a bumblebee on a pink flower.\\n\\n'}]\n```\n\n#### Running the model on a single / multi GPU\n\n```python\n# pip install accelerate\n\nfrom transformers import AutoProcessor, Gemma3ForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"google/gemma-3-4b-pt\"\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = Gemma3ForConditionalGeneration.from_pretrained(model_id).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nprompt = \"<start_of_image> in this image, there is\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n\ninput_len = model_inputs[\"input_ids\"].shape[-1]\n\nwith torch.inference_mode():\n    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n    generation = generation[0][input_len:]\n\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n```\n\n### Citation\n\n```none\n@article{gemma_2025,\n    title={Gemma 3},\n    url={https://goo.gle/Gemma3Report},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2025}\n}\n```\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\n\n-   Web Documents: A diverse collection of web text ensures the model is\n    exposed to a broad range of linguistic styles, topics, and vocabulary. The\n    training dataset includes content in over 140 languages.\n-   Code: Exposing the model to code helps it to learn the syntax and\n    patterns of programming languages, which improves its ability to generate\n    code and understand code-related questions.\n-   Mathematics: Training on mathematical text helps the model learn logical\n    reasoning, symbolic representation, and to address mathematical queries.\n-   Images: A wide range of images enables the model to perform image\n    analysis and visual data extraction tasks.\n\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\n    was applied at multiple stages in the data preparation process to ensure\n    the exclusion of harmful and illegal content.\n-   Sensitive Data Filtering: As part of making Gemma pre-trained models\n    safe and reliable, automated techniques were used to filter out certain\n    personal information and other sensitive data from training sets.\n-   Additional methods: Filtering based on content quality and safety in\n    line with [our policies][safety-policies].\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\n\n-   Performance: TPUs are specifically designed to handle the massive\n    computations involved in training VLMs. They can speed up training\n    considerably compared to CPUs.\n-   Memory: TPUs often come with large amounts of high-bandwidth memory,\n    allowing for the handling of large models and batch sizes during training.\n    This can lead to better model quality.\n-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable\n    solution for handling the growing complexity of large foundation models.\n    You can distribute training across multiple TPU devices for faster and more\n    efficient processing.\n-   Cost-effectiveness: In many scenarios, TPUs can provide a more\n    cost-effective solution for training large models compared to CPU-based\n    infrastructure, especially when considering the time and resources saved\n    due to faster training.\n-   These advantages are aligned with\n    [Google's commitments to operate sustainably][sustainability].\n\n### Software\n\nTraining was done using [JAX][jax] and [ML Pathways][ml-pathways].\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models][gemini-2-paper]; *\"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"*\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n#### Reasoning and factuality\n\n| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|\n| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |\n| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |\n| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |\n| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |\n| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |\n| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |\n| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |\n| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |\n| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |\n| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |\n| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |\n\n[hellaswag]: https://arxiv.org/abs/1905.07830\n[boolq]: https://arxiv.org/abs/1905.10044\n[piqa]: https://arxiv.org/abs/1911.11641\n[socialiqa]: https://arxiv.org/abs/1904.09728\n[triviaqa]: https://arxiv.org/abs/1705.03551\n[naturalq]: https://github.com/google-research-datasets/natural-questions\n[arc]: https://arxiv.org/abs/1911.01547\n[winogrande]: https://arxiv.org/abs/1907.10641\n[bbh]: https://paperswithcode.com/dataset/bbh\n[drop]: https://arxiv.org/abs/1903.00161\n\n#### STEM and code\n\n| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|\n| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |\n| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |\n| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |\n| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |\n| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |\n| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |\n| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |\n| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |\n\n[mmlu]: https://arxiv.org/abs/2009.03300\n[agieval]: https://arxiv.org/abs/2304.06364\n[math]: https://arxiv.org/abs/2103.03874\n[gsm8k]: https://arxiv.org/abs/2110.14168\n[gpqa]: https://arxiv.org/abs/2311.12022\n[mbpp]: https://arxiv.org/abs/2108.07732\n[humaneval]: https://arxiv.org/abs/2107.03374\n\n#### Multilingual\n\n| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|\n| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |\n| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |\n| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |\n| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |\n| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |\n| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |\n| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |\n\n[mgsm]: https://arxiv.org/abs/2210.03057\n[flores]: https://arxiv.org/abs/2106.03193\n[xquad]: https://arxiv.org/abs/1910.11856v3\n[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite\n[wmt24pp]: https://arxiv.org/abs/2502.12404v1\n[eclektic]: https://arxiv.org/abs/2502.21228\n[indicgenbench]: https://arxiv.org/abs/2404.16816\n\n#### Multimodal\n\n| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |\n| ------------------------------ |:-------------:|:--------------:|:--------------:|\n| [COCOcap][coco-cap]            |      102      |      111       |      116       |\n| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |\n| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |\n| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |\n| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |\n| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |\n| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |\n| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |\n| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |\n| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |\n| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |\n| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |\n| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |\n| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |\n| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |\n\n[coco-cap]: https://cocodataset.org/#home\n[docvqa]: https://www.docvqa.org/\n[info-vqa]: https://arxiv.org/abs/2104.12756\n[mmmu]: https://arxiv.org/abs/2311.16502\n[textvqa]: https://textvqa.org/\n[realworldqa]: https://paperswithcode.com/dataset/realworldqa\n[remi]: https://arxiv.org/html/2406.09175v1\n[ai2d]: https://allenai.org/data/diagrams\n[chartqa]: https://arxiv.org/abs/2203.10244\n[vqav2]: https://visualqa.org/index.html\n[blinkvqa]: https://arxiv.org/abs/2404.12390\n[okvqa]: https://okvqa.allenai.org/\n[tallyqa]: https://arxiv.org/abs/1810.12440\n[ss-vqa]: https://arxiv.org/abs/1908.02660\n[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n-   **Child Safety**: Evaluation of text-to-text and image to text prompts\n    covering child safety policies, including child sexual abuse and\n    exploitation.\n-   **Content Safety:** Evaluation of text-to-text and image to text prompts\n    covering safety policies including, harassment, violence and gore, and hate\n    speech.\n-   **Representational Harms**: Evaluation of text-to-text and image to text\n    prompts covering safety policies including bias, stereotyping, and harmful\n    associations or inaccuracies.\n\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\n\n### Evaluation Results\n\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   Content Creation and Communication\n    -   Text Generation: These models can be used to generate creative text\n        formats such as poems, scripts, code, marketing copy, and email drafts.\n    -   Chatbots and Conversational AI: Power conversational interfaces\n        for customer service, virtual assistants, or interactive applications.\n    -   Text Summarization: Generate concise summaries of a text corpus,\n        research papers, or reports.\n    -   Image Data Extraction: These models can be used to extract,\n        interpret, and summarize visual data for text communications.\n-   Research and Education\n    -   Natural Language Processing (NLP) and VLM Research: These\n        models can serve as a foundation for researchers to experiment with VLM\n        and NLP techniques, develop algorithms, and contribute to the\n        advancement of the field.\n    -   Language Learning Tools: Support interactive language learning\n        experiences, aiding in grammar correction or providing writing practice.\n    -   Knowledge Exploration: Assist researchers in exploring large\n        bodies of text by generating summaries or answering questions about\n        specific topics.\n\n### Limitations\n\n-   Training Data\n    -   The quality and diversity of the training data significantly\n        influence the model's capabilities. Biases or gaps in the training data\n        can lead to limitations in the model's responses.\n    -   The scope of the training dataset determines the subject areas\n        the model can handle effectively.\n-   Context and Task Complexity\n    -   Models are better at tasks that can be framed with clear\n        prompts and instructions. Open-ended or highly complex tasks might be\n        challenging.\n    -   A model's performance can be influenced by the amount of context\n        provided (longer context generally leads to better outputs, up to a\n        certain point).\n-   Language Ambiguity and Nuance\n    -   Natural language is inherently complex. Models might struggle\n        to grasp subtle nuances, sarcasm, or figurative language.\n-   Factual Accuracy\n    -   Models generate responses based on information they learned\n        from their training datasets, but they are not knowledge bases. They\n        may generate incorrect or outdated factual statements.\n-   Common Sense\n    -   Models rely on statistical patterns in language. They might\n        lack the ability to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\n\n-   Bias and Fairness\n    -   VLMs trained on large-scale, real-world text and image data can\n        reflect socio-cultural biases embedded in the training material. These\n        models underwent careful scrutiny, input data pre-processing described\n        and posterior evaluations reported in this card.\n-   Misinformation and Misuse\n    -   VLMs can be misused to generate text that is false, misleading,\n        or harmful.\n    -   Guidelines are provided for responsible use with the model, see the\n        [Responsible Generative AI Toolkit][rai-toolkit].\n-   Transparency and Accountability:\n    -   This model card summarizes details on the models' architecture,\n        capabilities, limitations, and evaluation processes.\n    -   A responsibly developed open model offers the opportunity to\n        share innovation by making VLM technology accessible to developers and\n        researchers across the AI ecosystem.\n\nRisks identified and mitigations:\n\n-   **Perpetuation of biases**: It's encouraged to perform continuous\n    monitoring (using evaluation metrics, human review) and the exploration of\n    de-biasing techniques during model training, fine-tuning, and other use\n    cases.\n-   **Generation of harmful content**: Mechanisms and guidelines for content\n    safety are essential. Developers are encouraged to exercise caution and\n    implement appropriate content safety safeguards based on their specific\n    product policies and application use cases.\n-   **Misuse for malicious purposes**: Technical limitations and developer\n    and end-user education can help mitigate against malicious applications of\n    VLMs. Educational resources and reporting mechanisms for users to flag\n    misuse are provided. Prohibited uses of Gemma models are outlined in the\n    [Gemma Prohibited Use Policy][prohibited-use].\n-   **Privacy violations**: Models were trained on data filtered for removal\n    of certain personal information and other sensitive data. Developers are\n    encouraged to adhere to privacy regulations with privacy-preserving\n    techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.\n\n[g3-tech-report]: https://goo.gle/Gemma3Report\n[rai-toolkit]: https://ai.google.dev/responsible\n[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3\n[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3\n[terms]: https://ai.google.dev/gemma/terms\n[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf\n[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy\n[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu\n[sustainability]: https://sustainability.google/operating-sustainably/\n[jax]: https://github.com/jax-ml/jax\n[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/\n[sustainability]: https://sustainability.google/operating-sustainably/\n[gemini-2-paper]: https://arxiv.org/abs/2312.11805",
      "card_hash": "720809c69c7d9664ce64b73307231ab57c17896fa0677e5e24d343020333e050",
      "token_count": 6164,
      "success": true
    },
    {
      "model_id": "Qwen/Qwen3-Omni-30B-A3B-Thinking",
      "card_text": "---\nlicense: other\nlicense_name: apache-2.0\nlanguage:\n- en\ntags:\n- multimodal\nlibrary_name: transformers\npipeline_tag: any-to-any\n---\n\n# Qwen3-Omni\n\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n\n## Overview\n### Introduction\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/q3o_introduction.png\" width=\"100%\"/>\n<p>\n\nQwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:\n\n* **State-of-the-art across modalities**: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.\n\n* **Multilingual**: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.\n  - **Speech Input**: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.\n  - **Speech Output**: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.\n\n* **Novel Architecture**: MoE-based Thinker\u2013Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.\n\n* **Real-time Audio/Video Interaction**: Low-latency streaming with natural turn-taking and immediate text or speech responses.\n\n* **Flexible Control**: Customize behavior via system prompts for fine-grained control and easy adaptation.\n\n* **Detailed Audio Captioner**: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.\n\n### Model Architecture\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/overview.png\" width=\"80%\"/>\n<p>\n\n### Cookbooks for Usage Cases\n\nQwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the [QuickStart](#quickstart) guide to download the model and install the necessary inference environment dependencies, then run and experiment locally\u2014try modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!\n\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Cookbook</th>\n      <th>Description</th>\n      <th>Open</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"6\">Audio</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb\">Speech Recognition</a></td>\n      <td>Speech recognition, supporting multiple languages and long audio.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_recognition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb\">Speech Translation</a></td>\n      <td>Speech-to-Text / Speech-to-Speech translation.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/speech_translation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb\">Music Analysis</a></td>\n      <td>Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/music_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb\">Sound Analysis</a></td>\n      <td>Description and analysis of various sound effects and audio signals.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/sound_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb\">Audio Caption</a></td>\n      <td>Audio captioning, detailed description of any audio input.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_caption.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb\">Mixed Audio Analysis</a></td>\n      <td>Analysis of mixed audio content, such as speech, music, and environmental sounds.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/mixed_audio_analysis.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan=\"7\">Visual</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb\">OCR</a></td>\n      <td>OCR for complex images.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/ocr.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb\">Object Grounding</a></td>\n      <td>Target detection and grounding.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/object_grounding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb\">Image Question</a></td>\n      <td>Answering arbitrary questions about any image.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_question.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb\">Image Math</a></td>\n      <td>Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/image_math.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb\">Video Description</a></td>\n      <td>Detailed description of video content.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_description.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb\">Video Navigation</a></td>\n      <td>Generating navigation commands from first-person motion videos.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_navigation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb\">Video Scene Transition</a></td>\n      <td>Analysis of scene transitions in videos.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/video_scene_transition.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td rowspan=\"3\">Audio-Visual</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb\">Audio Visual Question</a></td>\n      <td>Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_question.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb\">Audio Visual Interaction</a></td>\n      <td>Interactive communication with the model using audio-visual inputs, including task specification via audio.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_interaction.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb\">Audio Visual Dialogue</a></td>\n      <td>Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_visual_dialogue.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td>Agent</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb\">Audio Function Call</a></td>\n      <td>Using audio input to perform function calls, enabling agent-like behaviors.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/audio_function_call.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n    <tr>\n      <td>Downstream Task Fine-tuning</td>\n      <td><a href=\"https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb\">Omni Captioner</a></td>\n      <td>Introduction and capability demonstration of <strong>Qwen3-Omni-30B-A3B-Captioner</strong>, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.</td>\n      <td><a href=\"https://colab.research.google.com/github/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a></td>\n    </tr>\n  </tbody>\n</table>\n\n## QuickStart\n\n### Model Description and Download\n\nBelow is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.\n\n| Model Name                   | Description |\n|------------------------------|-------------|\n| Qwen3-Omni-30B-A3B-Instruct  | The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf). |\n| Qwen3-Omni-30B-A3B-Thinking  | The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the [Qwen3-Omni Technical Report](https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf).|\n| Qwen3-Omni-30B-A3B-Captioner | A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's [cookbook](https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb). |\n\nDuring loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:\n\n```bash\n# Download through ModelScope (recommended for users in Mainland China)\npip install -U modelscope\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking\nmodelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner\n\n# Download through Hugging Face\npip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking\nhuggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner\n```\n\n### Transformers Usage\n\n#### Installation\n\nThe Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you **create a new Python environment** to avoid environment runtime issues.\n\n```bash\n# If you already have transformers installed, please uninstall it first, or create a new Python environment\n# pip uninstall transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\n```\n\nWe offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has `ffmpeg` installed:\n\n```bash\npip install qwen-omni-utils -U\n```\n\nAdditionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using [vLLM](#vllm-usage) for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the [FlashAttention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention 2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\n#### Code Snippet\n\nHere is a code snippet to show you how to use Qwen3-Omni with `transformers` and `qwen_omni_utils`:\n\n```python\nimport soundfile as sf\n\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one short sentence.\"}\n        ],\n    },\n]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for inference\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors=\"pt\", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Inference: Generation of the output text and audio\ntext_ids, audio = model.generate(**inputs, \n                                 speaker=\"Ethan\", \n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\nif audio is not None:\n    sf.write(\n        \"output.wav\",\n        audio.reshape(-1).detach().cpu().numpy(),\n        samplerate=24000,\n    )\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nThe model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when `return_audio=False` is set. Here is an example.\n\n```python\nfrom transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nMODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n# MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    MODEL_PATH,\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nmodel.disable_talker()\n\nprocessor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n# Conversation with image only\nconversation1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"text\", \"text\": \"What can you see in this image? Answer in one sentence.\"},\n        ]\n    }\n]\n\n# Conversation with audio only\nconversation2 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you hear in this audio?\"},\n        ]\n    }\n]\n\n# Conversation with pure text and system prompt\nconversation3 = [\n    {\n        \"role\": \"system\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"You are Qwen-Omni.\"}\n        ],\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Who are you?\"\n    }\n]\n\n# Conversation with mixed media\nconversation4 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n            {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n            {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n        ],\n    }\n]\n\n# Combine messages for batch processing\nconversations = [conversation1, conversation2, conversation3, conversation4]\n\n# Set whether to use audio in video\nUSE_AUDIO_IN_VIDEO = True\n\n# Preparation for batch inference\ntext = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)\naudios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ninputs = processor(text=text, \n                   audio=audios, \n                   images=images, \n                   videos=videos, \n                   return_tensors=\"pt\", \n                   padding=True, \n                   use_audio_in_video=USE_AUDIO_IN_VIDEO)\ninputs = inputs.to(model.device).to(model.dtype)\n\n# Batch inference does not support returning audio\ntext_ids, audio = model.generate(**inputs,\n                                 return_audio=False,\n                                 thinker_return_dict_in_generate=True,\n                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)\n\ntext = processor.batch_decode(text_ids.sequences[:, inputs[\"input_ids\"].shape[1] :],\n                              skip_special_tokens=True,\n                              clean_up_tokenization_spaces=False)\nprint(text)\n```\n\n</details>\n\n<details>\n<summary>Use audio output or not</summary>\n\nThe model supports both text and audio outputs. If users do not need audio outputs, they can call `model.disable_talker()` after initializing the model. This option will save about `10GB` of GPU memory, but the `return_audio` option for the `generate` function will only allow `False`.\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\nmodel.disable_talker()\n```\n\nFor a more flexible experience, we recommend that users decide whether to return audio when the `generate` function is called. If `return_audio` is set to `False`, the model will only return text outputs, resulting in faster text responses.\n\n```python\nmodel = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n    dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n...\ntext_ids, _ = model.generate(..., return_audio=False)```\n\n</details>\n\n<details>\n<summary>Change voice type of output audio</summary>\n\nQwen3-Omni supports changing the voice of the output audio. The `\"Qwen/Qwen3-Omni-30B-A3B-Instruct\"` checkpoint supports three voice types as follows:\n\n| Voice Type | Gender | Description |\n|------------|--------|-------------|\n| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |\n| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |\n| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |\n\nUsers can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Ethan\")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Chelsie\")\n```\n\n```python\ntext_ids, audio = model.generate(..., speaker=\"Aiden\")\n```\n\n</details>\n\n### vLLM Usage\n\n#### Installation\n\nWe strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and **audio output inference support for the Instruct model will be released in the near future**, you can follow the commands below to install vLLM from source. Please note that we recommend you **create a new Python environment** to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#set-up-using-python-only-build-without-compilation).\n\n```bash\ngit clone -b qwen3_omni https://github.com/wangxiongts/vllm.git\ncd vllm\npip install -r requirements/build.txt\npip install -r requirements/cuda.txt\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl\nVLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation\n# If you meet an \"Undefined symbol\" error while using VLLM_USE_PRECOMPILED=1, please use \"pip install -e . -v\" to build from source.\n# Install the Transformers\npip install git+https://github.com/huggingface/transformers\npip install accelerate\npip install qwen-omni-utils -U\npip install -U flash-attn --no-build-isolation\n```\n\n#### Inference\n\nYou can use the following code for vLLM inference. The `limit_mm_per_prompt` parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting `tensor_parallel_size` greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, `max_num_seqs` indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the [vLLM official documentation](https://docs.vllm.ai/en/latest/api/vllm/index.html#vllm.LLM). Below is a simple example of how to run Qwen3-Omni with vLLM:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\nif __name__ == '__main__':\n    # vLLM engine v1 not supported yet\n    os.environ['VLLM_USE_V1'] = '0'\n\n    MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n    # MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"video\", \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4\"}\n            ], \n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n\n    inputs = {\n        'prompt': text,\n        'multi_modal_data': {},\n        \"mm_processor_kwargs\": {\n            \"use_audio_in_video\": True,\n        },\n    }\n\n    if images is not None:\n        inputs['multi_modal_data']['image'] = images\n    if videos is not None:\n        inputs['multi_modal_data']['video'] = videos\n    if audios is not None:\n        inputs['multi_modal_data']['audio'] = audios\n\n    outputs = llm.generate([inputs], sampling_params=sampling_params)\n\n    print(outputs[0].outputs[0].text)\n```\n\nHere are some more advanced usage examples. You can expand the sections below to learn more.\n\n<details>\n<summary>Batch inference</summary>\n\nUsing vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:\n\n```python\nimport os\nimport torch\n\nfrom vllm import LLM, SamplingParams\nfrom transformers import Qwen3OmniMoeProcessor\nfrom qwen_omni_utils import process_mm_info\n\ndef build_input(processor, messages, use_audio_in_video):\n    text = processor.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)\n\n    inputs = {\n        'prompt': text,\n        'multi_modal_data': {},\n        \"mm_processor_kwargs\": {\n            \"use_audio_in_video\": use_audio_in_video,\n        },\n    }\n\n    if images is not None:\n        inputs['multi_modal_data']['image'] = images\n    if videos is not None:\n        inputs['multi_modal_data']['video'] = videos\n    if audios is not None:\n        inputs['multi_modal_data']['audio'] = audios\n    \n    return inputs\n\nif __name__ == '__main__':\n    # vLLM engine v1 not supported yet\n    os.environ['VLLM_USE_V1'] = '0'\n\n    MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n    # MODEL_PATH = \"Qwen/Qwen3-Omni-30B-A3B-Thinking\"\n\n    llm = LLM(\n            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,\n            tensor_parallel_size=torch.cuda.device_count(),\n            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},\n            max_num_seqs=8,\n            max_model_len=32768,\n            seed=1234,\n    )\n\n    sampling_params = SamplingParams(\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        max_tokens=16384,\n    )\n\n    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)\n\n    # Conversation with image only\n    conversation1 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n                {\"type\": \"text\", \"text\": \"What can you see in this image? Answer in one sentence.\"},\n            ]\n        }\n    ]\n\n    # Conversation with audio only\n    conversation2 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"},\n                {\"type\": \"text\", \"text\": \"What can you hear in this audio?\"},\n            ]\n        }\n    ]\n\n    # Conversation with pure text and system prompt\n    conversation3 = [\n        {\n            \"role\": \"system\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"You are Qwen-Omni.\"}\n            ],\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are you? Answer in one sentence.\"\n        }\n    ]\n\n    # Conversation with mixed media\n    conversation4 = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\", \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"},\n                {\"type\": \"audio\", \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav\"},\n                {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n            ],\n        }\n    ]\n    \n    USE_AUDIO_IN_VIDEO = True\n\n    # Combine messages for batch processing\n    conversations = [conversation1, conversation2, conversation3, conversation4]\n    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]\n\n    outputs = llm.generate(inputs, sampling_params=sampling_params)\n\n    result = [outputs[i].outputs[0].text for i in range(len(outputs))]\n    print(result)\n```\n\n</details>\n\n<details>\n<summary>vLLM Serve Usage</summary>\n\nvLLM serve for Qwen3-Omni currently only supports the thinker model. The `use_audio_in_video` parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:\n\n```bash\n# Qwen3-Omni-30B-A3B-Instruct for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1\n# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)\nvllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4\n```\n\nThen you can use the chat API as below (via curl, for example):\n```bash\ncurl http://localhost:8901/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg\"}},\n        {\"type\": \"audio_url\", \"audio_url\": {\"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav\"}},\n        {\"type\": \"text\", \"text\": \"What can you see and hear? Answer in one sentence.\"}\n    ]}\n    ]\n    }'\n```\n\n</details>\n\n### Usage Tips (Recommended Reading)\n\n#### Minimum GPU memory requirements\n\n| Model                        | Precision | 15s Video | 30s Video | 60s Video | 120s Video   |\n|------------------------------|-----------| --------- | --------- | --------- | --------- |\n| Qwen3-Omni-30B-A3B-Instruct  | BF16      | 78.85 GB  | 88.52 GB  | 107.74 GB | 144.81 GB |\n| Qwen3-Omni-30B-A3B-Thinking  | BF16      | 68.74 GB  | 77.79 GB  | 95.76 GB  | 131.65 GB  |\n\n**Note**: The table above presents the theoretical minimum memory requirements for inference with `transformers` and `BF16` precision, tested with `attn_implementation=\"flash_attention_2\"`. The Instruct model includes both the **thinker** and **talker** components, whereas the Thinking model includes only the **thinker** part.\n\n#### Prompt for Audio-Visual Interaction\n\nWhen using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the **following system prompt**. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the `user_system_prompt` field in the system prompt to include character settings or other role-specific descriptions as needed.\n\n```\nuser_system_prompt = \"You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen.\"\nmessage = {\n    \"role\": \"system\",\n    \"content\": [\n          {\"type\": \"text\", \"text\": f\"{user_system_prompt} You are a virtual voice assistant with no gender or age.\\nYou are communicating with the user.\\nIn user messages, \u201cI/me/my/we/our\u201d refer to the user and \u201cyou/your\u201d refer to the assistant. In your replies, address the user as \u201cyou/your\u201d and yourself as \u201cI/me/my\u201d; never mirror the user\u2019s pronouns\u2014always shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \\nYour output must consist only of the spoken content you want the user to hear. \\nDo not include any descriptions of actions, emotions, sounds, or voice changes. \\nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \\nYou must answer users' audio or text questions, do not directly describe the video content. \\nYou should communicate in the same language strictly as the user unless they request otherwise.\\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\\nKeep replies concise and conversational, as if talking face-to-face.\"}\n    ]\n}\n```\n\n#### Best Practices for the Thinking Model\n\nThe `Qwen3-Omni-30B-A3B-Thinking` model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:\n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"image\", \"image\": \"/path/to/image.png\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Analyze this audio, image, and video together.\"},\n        ], \n    }\n]\n```\n\n#### Use audio in video\n\nIn multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.\n\n```python\n# In data preprocessing\naudios, images, videos = process_mm_info(messages, use_audio_in_video=True)\n```\n\n```python\n# For Transformers\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=\"pt\", \n                   padding=True, use_audio_in_video=True)\ntext_ids, audio = model.generate(..., use_audio_in_video=True)\n\n# For vLLM\ntext = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\ninputs = {\n    'prompt': text,\n    'multi_modal_data': {},\n    \"mm_processor_kwargs\": {\n        \"use_audio_in_video\": True,\n    },\n}\n```\n\nIt is worth noting that during a multi-round conversation, the `use_audio_in_video` parameter must be set consistently across these steps; otherwise, unexpected results may occur.\n\n## Evaluation\n\n### Performance of Qwen3-Omni\n\nQwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.\n\n<details>\n<summary>Text -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th colspan=\"2\" style=\"text-align: left;\"></th>\n      <th style=\"text-align: center;\">GPT-4o-0327</th>\n      <th style=\"text-align: center;\">Qwen3-235B-A22B<br>Non Thinking</th>\n      <th style=\"text-align: center;\">Qwen3-30B-A3B-Instruct-2507</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">General<br>Tasks</td>\n      <td style=\"text-align: left;\">MMLU-Redux</td>\n      <td style=\"text-align: center;\"><strong>91.3</strong></td>\n      <td style=\"text-align: center;\">89.2</td>\n      <td style=\"text-align: center;\">89.3</td>\n      <td style=\"text-align: center;\">86.6</td>\n      <td style=\"text-align: center;\">86.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">GPQA</td>\n      <td style=\"text-align: center;\">66.9</td>\n      <td style=\"text-align: center;\">62.9</td>\n      <td style=\"text-align: center;\"><strong>70.4</strong></td>\n      <td style=\"text-align: center;\">69.6</td>\n      <td style=\"text-align: center;\">69.7</td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">Reasoning</td>\n      <td style=\"text-align: left;\">AIME25</td>\n      <td style=\"text-align: center;\">26.7</td>\n      <td style=\"text-align: center;\">24.7</td>\n      <td style=\"text-align: center;\">61.3</td>\n      <td style=\"text-align: center;\">65.0</td>\n      <td style=\"text-align: center;\"><strong>65.9</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ZebraLogic</td>\n      <td style=\"text-align: center;\">52.6</td>\n      <td style=\"text-align: center;\">37.7</td>\n      <td style=\"text-align: center;\"><strong>90.0</strong></td>\n      <td style=\"text-align: center;\">76.0</td>\n      <td style=\"text-align: center;\">76.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left; vertical-align: middle;\">Code</td>\n      <td style=\"text-align: left;\">MultiPL-E</td>\n      <td style=\"text-align: center;\">82.7</td>\n      <td style=\"text-align: center;\">79.3</td>\n      <td style=\"text-align: center;\"><strong>83.8</strong></td>\n      <td style=\"text-align: center;\">81.4</td>\n      <td style=\"text-align: center;\">81.5</td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td rowspan=\"3\" style=\"text-align: left; vertical-align: middle;\">Alignment<br>Tasks</td>\n      <td style=\"text-align: left;\">IFEval</td>\n      <td style=\"text-align: center;\">83.9</td>\n      <td style=\"text-align: center;\">83.2</td>\n      <td style=\"text-align: center;\"><strong>84.7</strong></td>\n      <td style=\"text-align: center;\">81.0</td>\n      <td style=\"text-align: center;\">81.7</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">Creative Writing v3</td>\n      <td style=\"text-align: center;\">84.9</td>\n      <td style=\"text-align: center;\">80.4</td>\n      <td style=\"text-align: center;\"><strong>86.0</strong></td>\n      <td style=\"text-align: center;\">80.6</td>\n      <td style=\"text-align: center;\">81.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">WritingBench</td>\n      <td style=\"text-align: center;\">75.5</td>\n      <td style=\"text-align: center;\">77.0</td>\n      <td style=\"text-align: center;\"><strong>85.5</strong></td>\n      <td style=\"text-align: center;\">82.6</td>\n      <td style=\"text-align: center;\">83.0</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left; vertical-align: middle;\">Agent</td>\n      <td style=\"text-align: left;\">BFCL-v3</td>\n      <td style=\"text-align: center;\">66.5</td>\n      <td style=\"text-align: center;\"><strong>68.0</strong></td>\n      <td style=\"text-align: center;\">65.1</td>\n      <td style=\"text-align: center;\">64.4</td>\n      <td style=\"text-align: center;\">65.0</td>\n    </tr>\n    <tr>\n      <td rowspan=\"2\" style=\"text-align: left; vertical-align: middle;\">Multilingual<br>Tasks</td>\n      <td style=\"text-align: left;\">MultiIF</td>\n      <td style=\"text-align: center;\"><strong>70.4</strong></td>\n      <td style=\"text-align: center;\">70.2</td>\n      <td style=\"text-align: center;\">67.9</td>\n      <td style=\"text-align: center;\">64.0</td>\n      <td style=\"text-align: center;\">64.7</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">PolyMATH</td>\n      <td style=\"text-align: center;\">25.5</td>\n      <td style=\"text-align: center;\">27.0</td>\n      <td style=\"text-align: center;\"><strong>43.1</strong></td>\n      <td style=\"text-align: center;\">37.9</td>\n      <td style=\"text-align: center;\">39.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th></th>\n      <th></th>\n      <th>Gemini-2.5-Flash<br>Thinking</th>\n      <th>Qwen3-235B-A22B<br>Thinking</th>\n      <th>Qwen3-30B-A3B-Thinking-2507</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan=\"2\"><em>General<br>Tasks</em></td>\n      <td>MMLU-Redux</td>\n      <td>92.1</td>\n      <td><b>92.7</b></td>\n      <td>91.4</td>\n      <td>88.8</td>\n      <td>89.7</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>GPQA</td>\n      <td><b>82.8</b></td>\n      <td>71.1</td>\n      <td>73.4</td>\n      <td>73.1</td>\n      <td>73.1</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td rowspan=\"2\"><em>Reasoning</em></td>\n      <td>AIME25</td>\n      <td>72.0</td>\n      <td>81.5</td>\n      <td><b>85.0</b></td>\n      <td>73.7</td>\n      <td>74.0</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>LiveBench 20241125</td>\n      <td>74.3</td>\n      <td><b>77.1</b></td>\n      <td>76.8</td>\n      <td>71.8</td>\n      <td>70.3</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td><em>Code</em></td>\n      <td>MultiPL-E</td>\n      <td><b>84.5</b></td>\n      <td>79.9</td>\n      <td>81.3</td>\n      <td>80.6</td>\n      <td>81.0</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td rowspan=\"4\"><em>Alignment<br>Tasks</em></td>\n      <td>IFEval</td>\n      <td><b>89.8</b></td>\n      <td>83.4</td>\n      <td>88.9</td>\n      <td>85.1</td>\n      <td>85.2</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>Arena-Hard v2</td>\n      <td>56.7</td>\n      <td><b>61.5</b></td>\n      <td>56.0</td>\n      <td>55.1</td>\n      <td>57.8</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>Creative Writing v3</td>\n      <td><b>85.0</b></td>\n      <td>84.6</td>\n      <td>84.4</td>\n      <td>82.5</td>\n      <td>83.6</td>\n    </tr>\n    <tr style=\"border-top: 1px solid #ddd;\">\n      <td>WritingBench</td>\n      <td>83.9</td>\n      <td>80.3</td>\n      <td>85.0</td>\n      <td>85.5</td>\n      <td><b>85.9</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td><em>Agent</em></td>\n      <td>BFCL-v3</td>\n      <td>68.6</td>\n      <td>70.8</td>\n      <td><b>72.4</b></td>\n      <td>63.2</td>\n      <td>64.5</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black;\">\n      <td rowspan=\"2\"><em>Multilingual<br>Tasks</em></td>\n      <td>MultiIF</td>\n      <td>74.4</td>\n      <td>71.9</td>\n      <td><b>76.4</b></td>\n      <td>72.9</td>\n      <td>73.2</td>\n    </tr>\n    <tr>\n      <td>PolyMATH</td>\n      <td>49.8</td>\n      <td><b>54.7</b></td>\n      <td>52.6</td>\n      <td>47.1</td>\n      <td>48.7</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Audio -> Text</summary>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n<thead>\n  <tr>\n    <th align=\"left\" style=\"padding: 8px;\"></th>\n    <th align=\"center\" style=\"padding: 8px;\">Seed-ASR</th>\n    <th align=\"center\" style=\"padding: 8px;\">Voxtral-Mini</th>\n    <th align=\"center\" style=\"padding: 8px;\">Voxtral-Small</th>\n    <th align=\"center\" style=\"padding: 8px;\">GPT-4o-Transcribe</th>\n    <th align=\"center\" style=\"padding: 8px;\">Gemini-2.5-Pro</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen2.5-Omni</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen3-Omni-30B-A3B-Instruct</th>\n    <th align=\"center\" style=\"padding: 8px;\">Qwen3-Omni-Flash-Instruct</th>\n  </tr>\n</thead>\n<tbody>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>EN & ZH ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Wenetspeech<br><em>net</em> | <em>meeting</em></td>\n    <td align=\"center\" style=\"padding: 8px;\">4.66 | <strong>5.69</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">24.30 | 31.53</td>\n    <td align=\"center\" style=\"padding: 8px;\">20.33 | 26.08</td>\n    <td align=\"center\" style=\"padding: 8px;\">15.30 | 32.27</td>\n    <td align=\"center\" style=\"padding: 8px;\">14.43 | 13.47</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.91 | 7.65</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.69 | 5.89</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>4.62</strong> | 5.75</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Librispeech<br><em>clean</em> | <em>other</em></td>\n    <td align=\"center\" style=\"padding: 8px;\">1.58 | 2.84</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.88 | 4.12</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.56 | 3.30</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.39 | 3.75</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.89 | 3.56</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.74 | 3.45</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>1.22</strong> | 2.48</td>\n    <td align=\"center\" style=\"padding: 8px;\">1.27 | <strong>2.44</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">CV15-en</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.47</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.79</td>\n    <td align=\"center\" style=\"padding: 8px;\">10.01</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.89</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.61</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.05</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.94</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">CV15-zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">24.67</td>\n    <td align=\"center\" style=\"padding: 8px;\">19.30</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.84</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.00</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.13</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.31</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>4.28</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-en</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.40</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.96</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.77</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.32</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.94</td>\n    <td align=\"center\" style=\"padding: 8px;\">3.77</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>2.72</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">2.74</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.69</td>\n    <td align=\"center\" style=\"padding: 8px;\">12.22</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.98</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.44</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.71</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.54</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.20</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>2.19</strong></td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>Multilingual ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-avg<br>(19 lang)</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">15.67</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.09</td>\n    <td align=\"center\" style=\"padding: 8px;\">4.48</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.55</td>\n    <td align=\"center\" style=\"padding: 8px;\">14.04</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.33</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.31</strong></td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>Lyric ASR (wer)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">MIR-1K (vocal-only)</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.45</td>\n    <td align=\"center\" style=\"padding: 8px;\">23.33</td>\n    <td align=\"center\" style=\"padding: 8px;\">18.73</td>\n    <td align=\"center\" style=\"padding: 8px;\">11.87</td>\n    <td align=\"center\" style=\"padding: 8px;\">9.85</td>\n    <td align=\"center\" style=\"padding: 8px;\">8.15</td>\n    <td align=\"center\" style=\"padding: 8px;\">5.90</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>5.85</strong></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Opencpop-test</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.98</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.01</td>\n    <td align=\"center\" style=\"padding: 8px;\">16.06</td>\n    <td align=\"center\" style=\"padding: 8px;\">7.93</td>\n    <td align=\"center\" style=\"padding: 8px;\">6.49</td>\n    <td align=\"center\" style=\"padding: 8px;\">2.84</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>1.54</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">2.02</td>\n  </tr>\n  <tr style=\"border-top: 1px solid #333;\">\n    <td colspan=\"9\" align=\"center\"; style=\"border-top: 1px solid black; border-bottom: 1px solid black;\"><em>S2TT (BLEU)</em></td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-en2xx</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">30.35</td>\n    <td align=\"center\" style=\"padding: 8px;\">37.85</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>39.25</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">29.22</td>\n    <td align=\"center\" style=\"padding: 8px;\">37.50</td>\n    <td align=\"center\" style=\"padding: 8px;\">36.22</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-xx2en</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">27.54</td>\n    <td align=\"center\" style=\"padding: 8px;\">32.81</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>35.41</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">28.61</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.08</td>\n    <td align=\"center\" style=\"padding: 8px;\">30.71</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-zh2xx</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">17.03</td>\n    <td align=\"center\" style=\"padding: 8px;\">22.05</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>26.63</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">17.97</td>\n    <td align=\"center\" style=\"padding: 8px;\">25.17</td>\n    <td align=\"center\" style=\"padding: 8px;\">25.10</td>\n  </tr>\n  <tr>\n    <td align=\"left\" style=\"padding: 8px;\">Fleurs-xx2zh</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\">28.75</td>\n    <td align=\"center\" style=\"padding: 8px;\">34.82</td>\n    <td align=\"center\" style=\"padding: 8px;\">-</td>\n    <td align=\"center\" style=\"padding: 8px;\"><strong>37.50</strong></td>\n    <td align=\"center\" style=\"padding: 8px;\">27.68</td>\n    <td align=\"center\" style=\"padding: 8px;\">33.13</td>\n    <td align=\"center\" style=\"padding: 8px;\">31.19</td>\n  </tr>\n</tbody>\n</table>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n  <thead>\n    <tr style=\"border-bottom: 1px solid #ddd;\">\n      <th style=\"text-align:left; padding: 8px;\"></th>\n      <th style=\"text-align:center; padding: 8px;\">GPT-4o-Audio</th>\n      <th style=\"text-align:center; padding: 8px;\">Gemini-2.5-Flash</th>\n      <th style=\"text-align:center; padding: 8px;\">Gemini-2.5-Pro</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen2.5-Omni</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-Flash-Instruct</th>\n      <th style=\"text-align:center; padding: 8px;\">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan=\"9\" align=\"center\" style=\"padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\"><strong>VoiceBench</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">AlpacaEval</td>\n      <td style=\"text-align:center; padding: 8px;\">95.6</td>\n      <td style=\"text-align:center; padding: 8px;\">96.1</td>\n      <td style=\"text-align:center; padding: 8px;\">94.3</td>\n      <td style=\"text-align:center; padding: 8px;\">89.9</td>\n      <td style=\"text-align:center; padding: 8px;\">94.8</td>\n      <td style=\"text-align:center; padding: 8px;\">96.4</td>\n      <td style=\"text-align:center; padding: 8px;\">95.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>96.8</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">CommonEval</td>\n      <td style=\"text-align:center; padding: 8px;\">89.8</td>\n      <td style=\"text-align:center; padding: 8px;\">88.3</td>\n      <td style=\"text-align:center; padding: 8px;\">88.4</td>\n      <td style=\"text-align:center; padding: 8px;\">76.7</td>\n      <td style=\"text-align:center; padding: 8px;\">90.8</td>\n      <td style=\"text-align:center; padding: 8px;\">90.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>91.0</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">90.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">WildVoice</td>\n      <td style=\"text-align:center; padding: 8px;\">91.6</td>\n      <td style=\"text-align:center; padding: 8px;\">92.1</td>\n      <td style=\"text-align:center; padding: 8px;\">93.4</td>\n      <td style=\"text-align:center; padding: 8px;\">77.7</td>\n      <td style=\"text-align:center; padding: 8px;\">91.6</td>\n      <td style=\"text-align:center; padding: 8px;\">90.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>92.3</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">90.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">SD-QA</td>\n      <td style=\"text-align:center; padding: 8px;\">75.5</td>\n      <td style=\"text-align:center; padding: 8px;\">84.5</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>90.1</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">56.4</td>\n      <td style=\"text-align:center; padding: 8px;\">76.9</td>\n      <td style=\"text-align:center; padding: 8px;\">78.1</td>\n      <td style=\"text-align:center; padding: 8px;\">76.8</td>\n      <td style=\"text-align:center; padding: 8px;\">78.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">MMSU</td>\n      <td style=\"text-align:center; padding: 8px;\">80.3</td>\n      <td style=\"text-align:center; padding: 8px;\">66.1</td>\n      <td style=\"text-align:center; padding: 8px;\">71.1</td>\n      <td style=\"text-align:center; padding: 8px;\">61.7</td>\n      <td style=\"text-align:center; padding: 8px;\">68.1</td>\n      <td style=\"text-align:center; padding: 8px;\">83.0</td>\n      <td style=\"text-align:center; padding: 8px;\">68.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>84.3</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">OpenBookQA</td>\n      <td style=\"text-align:center; padding: 8px;\">89.2</td>\n      <td style=\"text-align:center; padding: 8px;\">56.9</td>\n      <td style=\"text-align:center; padding: 8px;\">92.3</td>\n      <td style=\"text-align:center; padding: 8px;\">80.9</td>\n      <td style=\"text-align:center; padding: 8px;\">89.7</td>\n      <td style=\"text-align:center; padding: 8px;\">94.3</td>\n      <td style=\"text-align:center; padding: 8px;\">91.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>95.0</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">BBH</td>\n      <td style=\"text-align:center; padding: 8px;\">84.1</td>\n      <td style=\"text-align:center; padding: 8px;\">83.9</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>92.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">66.7</td>\n      <td style=\"text-align:center; padding: 8px;\">80.4</td>\n      <td style=\"text-align:center; padding: 8px;\">88.9</td>\n      <td style=\"text-align:center; padding: 8px;\">80.6</td>\n      <td style=\"text-align:center; padding: 8px;\">89.6</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">IFEval</td>\n      <td style=\"text-align:center; padding: 8px;\">76.0</td>\n      <td style=\"text-align:center; padding: 8px;\">83.8</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>85.7</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">53.5</td>\n      <td style=\"text-align:center; padding: 8px;\">77.8</td>\n      <td style=\"text-align:center; padding: 8px;\">80.6</td>\n      <td style=\"text-align:center; padding: 8px;\">75.2</td>\n      <td style=\"text-align:center; padding: 8px;\">80.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">AdvBench</td>\n      <td style=\"text-align:center; padding: 8px;\">98.7</td>\n      <td style=\"text-align:center; padding: 8px;\">98.9</td>\n      <td style=\"text-align:center; padding: 8px;\">98.1</td>\n      <td style=\"text-align:center; padding: 8px;\">99.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>99.3</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">97.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>99.4</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">98.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">Overall</td>\n      <td style=\"text-align:center; padding: 8px;\">86.8</td>\n      <td style=\"text-align:center; padding: 8px;\">83.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>89.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">73.6</td>\n      <td style=\"text-align:center; padding: 8px;\">85.5</td>\n      <td style=\"text-align:center; padding: 8px;\">88.8</td>\n      <td style=\"text-align:center; padding: 8px;\">85.6</td>\n      <td style=\"text-align:center; padding: 8px;\">89.5</td>\n    </tr>\n    <tr>\n      <td colspan=\"9\" align=\"center\" style=\"padding: 8px; font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\"><strong>Audio Reasoning</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align:left; padding: 8px;\">MMAU-v05.15.25</td>\n      <td style=\"text-align:center; padding: 8px;\">62.5</td>\n      <td style=\"text-align:center; padding: 8px;\">71.8</td>\n      <td style=\"text-align:center; padding: 8px;\">77.4</td>\n      <td style=\"text-align:center; padding: 8px;\">65.5</td>\n      <td style=\"text-align:center; padding: 8px;\">77.5</td>\n      <td style=\"text-align:center; padding: 8px;\">75.4</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>77.6</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">76.5</td>\n    </tr>\n    <tr\">\n      <td style=\"text-align:left; padding: 8px;\">MMSU</td>\n      <td style=\"text-align:center; padding: 8px;\">56.4</td>\n      <td style=\"text-align:center; padding: 8px;\">70.2</td>\n      <td style=\"text-align:center; padding: 8px;\"><strong>77.7</strong></td>\n      <td style=\"text-align:center; padding: 8px;\">62.6</td>\n      <td style=\"text-align:center; padding: 8px;\">69.0</td>\n      <td style=\"text-align:center; padding: 8px;\">70.2</td>\n      <td style=\"text-align:center; padding: 8px;\">69.1</td>\n      <td style=\"text-align:center; padding: 8px;\">71.3</td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th style=\"text-align: left;\"></th>\n      <th style=\"text-align: center;\">Best Specialist<br>Models</th>\n      <th style=\"text-align: center;\">GPT-4o-Audio</th>\n      <th style=\"text-align: center;\">Gemini-2.5-Pro</th>\n      <th style=\"text-align: center;\">Qwen2.5-Omni</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left;\">RUL-MuchoMusic</td>\n      <td style=\"text-align: center;\">47.6 (Audio Flamingo 3)</td>\n      <td style=\"text-align: center;\">36.1</td>\n      <td style=\"text-align: center;\">49.4</td>\n      <td style=\"text-align: center;\">47.3</td>\n      <td style=\"text-align: center;\">52.0</td>\n      <td style=\"text-align: center;\"><strong>52.1</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">GTZAN<br><em>Acc.</em></td>\n      <td style=\"text-align: center;\">87.9 (CLaMP 3)</td>\n      <td style=\"text-align: center;\">76.5</td>\n      <td style=\"text-align: center;\">81.0</td>\n      <td style=\"text-align: center;\">81.7</td>\n      <td style=\"text-align: center;\">93.0</td>\n      <td style=\"text-align: center;\"><strong>93.1</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Genre<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">35.8 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">25.3</td>\n      <td style=\"text-align: center;\">32.6</td>\n      <td style=\"text-align: center;\">32.5</td>\n      <td style=\"text-align: center;\">39.0</td>\n      <td style=\"text-align: center;\"><strong>39.5</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Mood/Theme<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">10.9 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">11.3</td>\n      <td style=\"text-align: center;\">14.1</td>\n      <td style=\"text-align: center;\">8.9</td>\n      <td style=\"text-align: center;\">21.0</td>\n      <td style=\"text-align: center;\"><strong>21.7</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Instrument<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">39.8 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">34.2</td>\n      <td style=\"text-align: center;\">33.0</td>\n      <td style=\"text-align: center;\">22.6</td>\n      <td style=\"text-align: center;\">40.5</td>\n      <td style=\"text-align: center;\"><strong>40.7</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MTG Top50<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">33.2 (MuQ-MuLan)</td>\n      <td style=\"text-align: center;\">25.0</td>\n      <td style=\"text-align: center;\">26.1</td>\n      <td style=\"text-align: center;\">21.6</td>\n      <td style=\"text-align: center;\">36.7</td>\n      <td style=\"text-align: center;\"><strong>36.9</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MagnaTagATune<br><em>Micro F1</em></td>\n      <td style=\"text-align: center;\">41.6 (MuQ)</td>\n      <td style=\"text-align: center;\">29.2</td>\n      <td style=\"text-align: center;\">28.1</td>\n      <td style=\"text-align: center;\">30.1</td>\n      <td style=\"text-align: center;\">44.3</td>\n      <td style=\"text-align: center;\"><strong>46.8</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Vision -> Text</summary>\n\n<table style=\"width:100%; border-collapse: collapse;\">\n  <thead>\n    <tr style=\"border-bottom: 1px solid black;\">\n      <th style=\"text-align: left;\">Datasets</th>\n      <th style=\"text-align: center;\">GPT4-o</th>\n      <th style=\"text-align: center;\">Gemini-2.0-Flash</th>\n      <th style=\"text-align: center;\">Qwen2.5-VL<br>72B</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-30B-A3B<br>-Instruct</th>\n      <th style=\"text-align: center;\">Qwen3-Omni-Flash<br>-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid #ddd; border-bottom: 1px solid black;\">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMStar</td>\n      <td style=\"text-align: center;\">64.7</td>\n      <td style=\"text-align: center;\"><strong>71.4</strong></td>\n      <td style=\"text-align: center;\">70.8</td>\n      <td style=\"text-align: center;\">68.5</td>\n      <td style=\"text-align: center;\">69.3</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">HallusionBench</td>\n      <td style=\"text-align: center;\">55.0</td>\n      <td style=\"text-align: center;\">56.3</td>\n      <td style=\"text-align: center;\">55.2</td>\n      <td style=\"text-align: center;\"><strong>59.7</strong></td>\n      <td style=\"text-align: center;\">58.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MM-MT-Bench</td>\n      <td style=\"text-align: center;\"><strong>7.7</strong></td>\n      <td style=\"text-align: center;\">6.7</td>\n      <td style=\"text-align: center;\">7.6</td>\n      <td style=\"text-align: center;\">7.4</td>\n      <td style=\"text-align: center;\">7.6</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMMU_val</td>\n      <td style=\"text-align: center;\">69.1</td>\n      <td style=\"text-align: center;\"><strong>71.3</strong></td>\n      <td style=\"text-align: center;\">70.2</td>\n      <td style=\"text-align: center;\">69.1</td>\n      <td style=\"text-align: center;\">69.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MMMU_pro</td>\n      <td style=\"text-align: center;\">51.9</td>\n      <td style=\"text-align: center;\">56.1</td>\n      <td style=\"text-align: center;\">51.1</td>\n      <td style=\"text-align: center;\">57.0</td>\n      <td style=\"text-align: center;\"><strong>57.6</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MathVista_mini</td>\n      <td style=\"text-align: center;\">63.8</td>\n      <td style=\"text-align: center;\">71.4</td>\n      <td style=\"text-align: center;\">74.8</td>\n      <td style=\"text-align: center;\">75.9</td>\n      <td style=\"text-align: center;\"><strong>77.4</strong></td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MathVision_full</td>\n      <td style=\"text-align: center;\">30.4</td>\n      <td style=\"text-align: center;\">48.6</td>\n      <td style=\"text-align: center;\">38.1</td>\n      <td style=\"text-align: center;\">56.3</td>\n      <td style=\"text-align: center;\"><strong>58.3</strong></td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">AI2D</td>\n      <td style=\"text-align: center;\">84.6</td>\n      <td style=\"text-align: center;\">86.7</td>\n      <td style=\"text-align: center;\"><strong>88.7</strong></td>\n      <td style=\"text-align: center;\">85.2</td>\n      <td style=\"text-align: center;\">86.4</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ChartQA_test</td>\n      <td style=\"text-align: center;\">86.7</td>\n      <td style=\"text-align: center;\">64.6</td>\n      <td style=\"text-align: center;\"><strong>89.5</strong></td>\n      <td style=\"text-align: center;\">86.8</td>\n      <td style=\"text-align: center;\">87.1</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Counting</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">CountBench</td>\n      <td style=\"text-align: center;\">87.9</td>\n      <td style=\"text-align: center;\">91.2</td>\n      <td style=\"text-align: center;\"><strong>93.6</strong></td>\n      <td style=\"text-align: center;\">90.0</td>\n      <td style=\"text-align: center;\">90.0</td>\n    </tr>\n    <tr>\n      <td colspan=\"6\" align=\"center\" style=\"font-weight: bold; border-top: 1px solid black; border-bottom: 1px solid black;\">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">Video-MME</td>\n      <td style=\"text-align: center;\">71.9</td>\n      <td style=\"text-align: center;\">72.4</td>\n      <td style=\"text-align: center;\"><strong>73.3</strong></td>\n      <td style=\"text-align: center;\">70.5</td>\n      <td style=\"text-align: center;\">71.4</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">LVBench</td>\n      <td style=\"text-align: center;\">30.8</td>\n      <td style=\"text-align: center;\"><strong>57.9</strong></td>\n      <td style=\"text-align: center;\">47.3</td>\n      <td style=\"text-align: center;\">50.2</td>\n      <td style=\"text-align: center;\">51.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">MLVU</td>\n      <td style=\"text-align: center;\">64.6</td>\n      <td style=\"text-align: center;\">71.0</td>\n      <td style=\"text-align: center;\">74.6</td>\n      <td style=\"text-align: center;\">75.2</td>\n      <td style=\"text-align: center;\"><strong>75.7</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table style=\"width: 100%; border-collapse: collapse;\">\n  <thead style=\"border-bottom: 1px solid black;\">\n    <tr>\n      <th align=\"left\" style=\"padding: 6px;\">Datasets</th>\n      <th align=\"center\" style=\"padding: 6px;\">Gemini-2.5-flash-thinking</th>\n      <th align=\"center\" style=\"padding: 6px;\">InternVL-3.5-241B-A28B</th>\n      <th align=\"center\" style=\"padding: 6px;\">Qwen3-Omni-30B-A3B-Thinking</th>\n      <th align=\"center\" style=\"padding: 6px;\">Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr style=\"border-top: 2px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-bottom: 1px solid black;\">General Visual Question Answering</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMStar</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.5</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>77.9</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">74.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.5</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">HallusionBench</td>\n      <td align=\"center\" style=\"padding: 6px;\">61.1</td>\n      <td align=\"center\" style=\"padding: 6px;\">57.3</td>\n      <td align=\"center\" style=\"padding: 6px;\">62.8</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>63.4</b></td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MM-MT-Bench</td>\n      <td align=\"center\" style=\"padding: 6px;\">7.8</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>8.0</b></td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>8.0</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Math & STEM</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMMU_val</td>\n      <td align=\"center\" style=\"padding: 6px;\">76.9</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>77.7</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">75.6</td>\n      <td align=\"center\" style=\"padding: 6px;\">75.0</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MMMU_pro</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>65.8</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">60.5</td>\n      <td align=\"center\" style=\"padding: 6px;\">60.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MathVista_mini</td>\n      <td align=\"center\" style=\"padding: 6px;\">77.6</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>82.7</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">80.0</td>\n      <td align=\"center\" style=\"padding: 6px;\">81.2</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MathVision_full</td>\n      <td align=\"center\" style=\"padding: 6px;\">62.3</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>63.9</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">62.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">63.8</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Documentation Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">AI2D_test</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>88.6</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">87.3</td>\n      <td align=\"center\" style=\"padding: 6px;\">86.1</td>\n      <td align=\"center\" style=\"padding: 6px;\">86.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">ChartQA_test</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.0</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>89.5</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">89.3</td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Counting</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">CountBench</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.6</td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">88.6</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>92.5</b></td>\n    </tr>\n    <tr style=\"border-top: 1px solid black; border-bottom: 1px solid #ccc;\">\n      <td colspan=\"5\" align=\"center\" style=\"padding: 6px 0; font-weight: bold; border-top: 1px solid black;  border-bottom: 1px solid black;\">Video Understanding</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">Video-MME</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>79.6</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">72.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">69.7</td>\n      <td align=\"center\" style=\"padding: 6px;\">69.8</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">LVBench</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>64.5</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">\u2013</td>\n      <td align=\"center\" style=\"padding: 6px;\">49.0</td>\n      <td align=\"center\" style=\"padding: 6px;\">49.5</td>\n    </tr>\n    <tr>\n      <td style=\"padding: 6px;\">MLVU</td>\n      <td align=\"center\" style=\"padding: 6px;\"><b>82.1</b></td>\n      <td align=\"center\" style=\"padding: 6px;\">78.2</td>\n      <td align=\"center\" style=\"padding: 6px;\">72.9</td>\n      <td align=\"center\" style=\"padding: 6px;\">73.9</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>AudioVisual -> Text</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash</th>\n      <th>Qwen2.5-Omni</th>\n      <th>Qwen3-Omni-30B-A3B-Instruct</th>\n      <th>Qwen3-Omni-Flash-Instruct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>WorldSense</td>\n      <td>47.1</td>\n      <td>50.9</td>\n      <td>45.4</td>\n      <td>54.0</td>\n      <td><strong>54.1</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr>\n      <th>Datasets</th>\n      <th>Previous Open-source SoTA</th>\n      <th>Gemini-2.5-Flash-Thinking</th>\n      <th>Qwen3-Omni-30B-A3B-Thinking</th>\n      <th>Qwen3-Omni-Flash-Thinking</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>DailyOmni</td>\n      <td>69.8</td>\n      <td>72.7</td>\n      <td>75.8</b></td>\n      <td><b>76.2</td>\n    </tr>\n    <tr>\n      <td>VideoHolmes</td>\n      <td>55.6</td>\n      <td>49.5</td>\n      <td><b>57.3</b></td>\n      <td><b>57.3</b></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n<details>\n<summary>Zero-shot Speech Generation</summary>\n\n<table>\n  <thead>\n    <tr>\n      <th align=\"left\">Datasets</th>\n      <th align=\"left\">Model</th>\n      <th align=\"left\">Performance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>&nbsp;</td>\n      <td colspan=\"2\" align=\"center\"><em>Content Consistency</em></td>\n    </tr>\n  </tbody>\n  <tbody>\n    <tr>\n      <td rowspan=\"10\" align=\"center\" valign=\"middle\"><strong>SEED</strong><br><em>test-zh</em> | <em>test-en</em></td>\n      <td align=\"left\">Seed-TTS<sub>ICL</sub></td>\n      <td align=\"left\">1.11 | 2.24</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Seed-TTS<sub>RL</sub></td>\n      <td align=\"left\">1.00 | 1.94</td>\n    </tr>\n    <tr>\n      <td align=\"left\">MaskGCT</td>\n      <td align=\"left\">2.27 | 2.62</td>\n    </tr>\n    <tr>\n      <td align=\"left\">E2 TTS</td>\n      <td align=\"left\">1.97 | 2.19</td>\n    </tr>\n    <tr>\n      <td align=\"left\">F5-TTS</td>\n      <td align=\"left\">1.56 | 1.83</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Spark TTS</td>\n      <td align=\"left\">1.20 | 1.98</td>\n    </tr>\n    <tr>\n      <td align=\"left\">CosyVoice 2</td>\n      <td align=\"left\">1.45 | 2.57</td>\n    </tr>\n    <tr>\n      <td align=\"left\">CosyVoice 3</td>\n      <td align=\"left\"><strong>0.71</strong> | 1.45</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Qwen2.5-Omni-7B</td>\n      <td align=\"left\">1.42 | 2.33</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Qwen3-Omni-30B-A3B</td>\n      <td align=\"left\">1.07 | <strong>1.39</strong></td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Multilingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th rowspan=\"2\" align=\"left\">Language</th>\n      <th colspan=\"3\" style=\"text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;\">Content Consistency</th>\n      <th colspan=\"3\"  style=\"text-align:center; padding: 8px; font-weight: bold; border-bottom: 1px solid #ddd;\">Speaker Similarity</th>\n    </tr>\n    <tr>\n      <th align=\"center\">Qwen3-Omni-30B-A3B</th>\n      <th align=\"center\">MiniMax</th>\n      <th align=\"center\">ElevenLabs</th>\n      <th align=\"center\">Qwen3-Omni-30B-A3B</th>\n      <th align=\"center\">MiniMax</th>\n      <th align=\"center\">ElevenLabs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"left\">Chinese</td>\n      <td align=\"center\"><strong>0.716</strong></td>\n      <td align=\"center\">2.252</td>\n      <td align=\"center\">16.026</td>\n      <td align=\"center\">0.772</td>\n      <td align=\"center\"><strong>0.780</strong></td>\n      <td align=\"center\">0.677</td>\n    </tr>\n    <tr>\n      <td align=\"left\">English</td>\n      <td align=\"center\"><strong>1.069</strong></td>\n      <td align=\"center\">2.164</td>\n      <td align=\"center\">2.339</td>\n      <td align=\"center\"><strong>0.773</strong></td>\n      <td align=\"center\">0.756</td>\n      <td align=\"center\">0.613</td>\n    </tr>\n    <tr>\n      <td align=\"left\">German</td>\n      <td align=\"center\">0.777</td>\n      <td align=\"center\">1.906</td>\n      <td align=\"center\"><strong>0.572</strong></td>\n      <td align=\"center\"><strong>0.738</strong></td>\n      <td align=\"center\">0.733</td>\n      <td align=\"center\">0.614</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Italian</td>\n      <td align=\"center\"><strong>1.067</strong></td>\n      <td align=\"center\">1.543</td>\n      <td align=\"center\">1.743</td>\n      <td align=\"center\"><strong>0.742</strong></td>\n      <td align=\"center\">0.699</td>\n      <td align=\"center\">0.579</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Portuguese</td>\n      <td align=\"center\">1.872</td>\n      <td align=\"center\">1.877</td>\n      <td align=\"center\"><strong>1.331</strong></td>\n      <td align=\"center\">0.770</td>\n      <td align=\"center\"><strong>0.805</strong></td>\n      <td align=\"center\">0.711</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Spanish</td>\n      <td align=\"center\">1.765</td>\n      <td align=\"center\"><strong>1.029</strong></td>\n      <td align=\"center\">1.084</td>\n      <td align=\"center\">0.744</td>\n      <td align=\"center\"><strong>0.762</strong></td>\n      <td align=\"center\">0.615</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Japanese</td>\n      <td align=\"center\">3.631</td>\n      <td align=\"center\"><strong>3.519</strong></td>\n      <td align=\"center\">10.646</td>\n      <td align=\"center\">0.763</td>\n      <td align=\"center\"><strong>0.776</strong></td>\n      <td align=\"center\">0.738</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Korean</td>\n      <td align=\"center\"><strong>1.670</strong></td>\n      <td align=\"center\">1.747</td>\n      <td align=\"center\">1.865</td>\n      <td align=\"center\"><strong>0.778</strong></td>\n      <td align=\"center\">0.776</td>\n      <td align=\"center\">0.700</td>\n    </tr>\n    <tr>\n      <td align=\"left\">French</td>\n      <td align=\"center\"><strong>2.505</strong></td>\n      <td align=\"center\">4.099</td>\n      <td align=\"center\">5.216</td>\n      <td align=\"center\"><strong>0.689</strong></td>\n      <td align=\"center\">0.628</td>\n      <td align=\"center\">0.535</td>\n    </tr>\n    <tr>\n      <td align=\"left\">Russian</td>\n      <td align=\"center\">3.986</td>\n      <td align=\"center\">4.281</td>\n      <td align=\"center\"><strong>3.878</strong></td>\n      <td align=\"center\">0.759</td>\n      <td align=\"center\"><strong>0.761</strong></td>\n      <td align=\"center\">0.676</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n<details>\n<summary>Cross-Lingual Speech Generation </summary>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left;\">Language</th>\n      <th style=\"text-align: left;\">Qwen3-Omni-30B-A3B</th>\n      <th style=\"text-align: left;\">CosyVoice3</th>\n      <th style=\"text-align: left;\">CosyVoice2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left;\">en-to-zh</td>\n      <td style=\"text-align: left;\">5.37</td>\n      <td style=\"text-align: left;\"><strong>5.09</strong></td>\n      <td style=\"text-align: left;\">13.5</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-zh</td>\n      <td style=\"text-align: left;\">3.32</td>\n      <td style=\"text-align: left;\"><strong>3.05</strong></td>\n      <td style=\"text-align: left;\">48.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-zh</td>\n      <td style=\"text-align: left;\"><strong>0.99</strong></td>\n      <td style=\"text-align: left;\">1.06</td>\n      <td style=\"text-align: left;\">7.70</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-en</td>\n      <td style=\"text-align: left;\"><strong>2.76</strong></td>\n      <td style=\"text-align: left;\">2.98</td>\n      <td style=\"text-align: left;\">6.47</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-en</td>\n      <td style=\"text-align: left;\"><strong>3.31</strong></td>\n      <td style=\"text-align: left;\">4.20</td>\n      <td style=\"text-align: left;\">17.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-en</td>\n      <td style=\"text-align: left;\"><strong>3.34</strong></td>\n      <td style=\"text-align: left;\">4.19</td>\n      <td style=\"text-align: left;\">11.2</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-ja</td>\n      <td style=\"text-align: left;\">8.29</td>\n      <td style=\"text-align: left;\"><strong>7.08</strong></td>\n      <td style=\"text-align: left;\">13.1</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">en-to-ja</td>\n      <td style=\"text-align: left;\">7.53</td>\n      <td style=\"text-align: left;\"><strong>6.80</strong></td>\n      <td style=\"text-align: left;\">14.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ko-to-ja</td>\n      <td style=\"text-align: left;\">4.24</td>\n      <td style=\"text-align: left;\"><strong>3.93</strong></td>\n      <td style=\"text-align: left;\">5.86</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">zh-to-ko</td>\n      <td style=\"text-align: left;\"><strong>5.13</strong></td>\n      <td style=\"text-align: left;\">14.4</td>\n      <td style=\"text-align: left;\">24.8</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">en-to-ko</td>\n      <td style=\"text-align: left;\"><strong>4.96</strong></td>\n      <td style=\"text-align: left;\">5.87</td>\n      <td style=\"text-align: left;\">21.9</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left;\">ja-to-ko</td>\n      <td style=\"text-align: left;\"><strong>6.23</strong></td>\n      <td style=\"text-align: left;\">7.92</td>\n      <td style=\"text-align: left;\">21.5</td>\n    </tr>\n  </tbody>\n</table>\n\n</details>\n\n\n### Setting for Evaluation\n\n*   **Decoding Strategy**: For the Qwen3-Omni series across all evaluation benchmarks, `Instruct` models use greedy decoding during generation without sampling. For `Thinking` models, the decoding parameters should be taken from the `generation_config.json` file in the checkpoint.\n*   **Benchmark-Specific Formatting**: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to `fps=2` during evaluation.\n*   **Default Prompts**: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:\n\n| Task Type | Prompt |\n| :--- | :--- |\n| Auto Speech Recognition (ASR) for Chinese | \u8bf7\u5c06\u8fd9\u6bb5\u4e2d\u6587\u8bed\u97f3\u8f6c\u6362\u4e3a\u7eaf\u6587\u672c\u3002 |\n| Auto Speech Recognition (ASR) for Other languages | Transcribe the <language> audio into text. |\n| Speech-to-Text Translation (S2TT) | Listen to the provided <source_language> speech and produce a translation in <target_language> text. |\n| Song Lyrics Recognition | Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations. |\n\n*   **System Prompt**: No `system prompt` should be set for any evaluation benchmark.\n*   **Input Sequence**: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come **after** multimodal data in the sequence. For example:\n\n```python\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"audio\", \"audio\": \"/path/to/audio.wav\"},\n            {\"type\": \"image\", \"image\": \"/path/to/image.png\"},\n            {\"type\": \"video\", \"video\": \"/path/to/video.mp4\"},\n            {\"type\": \"text\", \"text\": \"Describe the audio, image and video.\"},\n        ],\n    },\n]\n```\n\n\n<!-- ## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n```BibTeX\n@article{Qwen3-Omni,\n  title={Qwen3-Omni Technical Report},\n  author={Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin},\n  journal={arXiv preprint arXiv},\n  year={2025}\n}\n``` -->\n\n<br>\n",
      "card_hash": "0e44065c4c4a27071f7239afd5b5a33af5bc2e437dd7ea9950e51aafabfde3df",
      "token_count": 28055,
      "success": true
    }
  ],
  "failures": [],
  "stats": {
    "total": 50,
    "successful": 50,
    "failed": 0
  }
}